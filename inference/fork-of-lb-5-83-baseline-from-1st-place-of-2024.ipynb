{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b086fc",
   "metadata": {
    "papermill": {
     "duration": 0.007287,
     "end_time": "2025-11-13T05:06:15.273410",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.266123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This Notebook is forked from 暗黑AGI's \"baseline from 1st place of 2024\"\n",
    "Original Notebook: [baseline-from-1st-place-of-2024](https://www.kaggle.com/code/boristown/baseline-from-1st-place-of-2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c1284",
   "metadata": {
    "papermill": {
     "duration": 0.005987,
     "end_time": "2025-11-13T05:06:15.285752",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.279765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d538f",
   "metadata": {
    "papermill": {
     "duration": 0.005929,
     "end_time": "2025-11-13T05:06:15.297812",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.291883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modifications\n",
    "\n",
    "- **Active Layer Control**: The num_active_layers variable has been added to common_stuff.py, allowing for control over how many initial layers of the Mistral-NeMo-Minitron model will be trained, while the rest are frozen.\n",
    "\n",
    "- **Tuned Hyperparameter**: LoRA and training settings have been tuned to maximize performance within the 12-hour runtime limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d7ecc",
   "metadata": {
    "papermill": {
     "duration": 0.005875,
     "end_time": "2025-11-13T05:06:15.309822",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.303947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook scores about 115 on the public ARC-AGI-2 evaluation set. When run five times on the semi-private dataset, it scores [4.17, 5, 5, 5.42, 5.83]. The reason for this significant discrepancy remains unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa992f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.323719Z",
     "iopub.status.busy": "2025-11-13T05:06:15.323028Z",
     "iopub.status.idle": "2025-11-13T05:06:15.326966Z",
     "shell.execute_reply": "2025-11-13T05:06:15.326371Z"
    },
    "papermill": {
     "duration": 0.012419,
     "end_time": "2025-11-13T05:06:15.328373",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.315954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Daniel Franzen and Jan Disselhoff\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1d4a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.341684Z",
     "iopub.status.busy": "2025-11-13T05:06:15.341168Z",
     "iopub.status.idle": "2025-11-13T05:06:15.343993Z",
     "shell.execute_reply": "2025-11-13T05:06:15.343439Z"
    },
    "papermill": {
     "duration": 0.010908,
     "end_time": "2025-11-13T05:06:15.345383",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.334475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook contains our winning submission to the ARC Prize 2024 Kaggle competition,\n",
    "# scoring 53.5 points on the private evaluation set.\n",
    "# the ARChitects (Daniel Franzen and Jan Disselhoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547c696b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.359397Z",
     "iopub.status.busy": "2025-11-13T05:06:15.359169Z",
     "iopub.status.idle": "2025-11-13T05:06:15.384120Z",
     "shell.execute_reply": "2025-11-13T05:06:15.383563Z"
    },
    "papermill": {
     "duration": 0.03402,
     "end_time": "2025-11-13T05:06:15.385538",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.351518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_runner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_runner.py\n",
    "import json\n",
    "import os, sys\n",
    "import bz2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def indices_required_for_merges(keep_indices, vocab, merges):\n",
    "    merges_lookup = {}\n",
    "    for m in merges:\n",
    "        a, b = m.split(' ') if isinstance(m, str) else m\n",
    "        key = vocab[f'{a}{b}']\n",
    "        if key not in merges_lookup: merges_lookup[key] = set()\n",
    "        merges_lookup[key].add(vocab[a])\n",
    "        merges_lookup[key].add(vocab[b])\n",
    "    to_process = list(keep_indices)\n",
    "    while len(to_process):\n",
    "        for w in merges_lookup.get(to_process.pop(), []):\n",
    "            if w not in keep_indices:\n",
    "                keep_indices[w] = None\n",
    "                to_process.append(w)\n",
    "    return keep_indices\n",
    "\n",
    "def remove_unused_merges(merges, vocab):\n",
    "    return [f'{a} {b}' for a, b in [m.split(' ') if isinstance(m, str) else m for m in merges] if all(w in vocab for w in [a, b, a + b])]\n",
    "\n",
    "def map_special_tokens(data, mapping=None):\n",
    "    tokens = set()\n",
    "    if isinstance(data, dict):\n",
    "        special = data.get('special_tokens')\n",
    "        if special is not None:\n",
    "            for v in special.values():\n",
    "                tokens.update(v['ids'])\n",
    "                if mapping is not None:\n",
    "                    v['ids'] = [mapping.get(i) for i in v['ids'] if i in mapping]\n",
    "    for v in (data.values() if isinstance(data, dict) else data if isinstance(data, list) else []):\n",
    "        tokens.update(map_special_tokens(v, mapping))\n",
    "    return tokens\n",
    "\n",
    "def remove_tokenizer_normalizer(tokenizer):\n",
    "    from tokenizers import Tokenizer\n",
    "    assert tokenizer.is_fast\n",
    "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "    if tokenizer_json.get('normalizer') is not None:\n",
    "        tokenizer_json['normalizer'] = None\n",
    "        tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n",
    "\n",
    "def shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order):\n",
    "    from tokenizers import Tokenizer\n",
    "    assert tokenizer.is_fast\n",
    "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "    assert tokenizer_json['model']['type'] == \"BPE\"\n",
    "    if keep_special_tokens:\n",
    "        keep_indices.update({k: None for k in tokenizer.all_special_ids})\n",
    "        keep_indices.update({k: None for k in map_special_tokens(tokenizer_json.get('post_processor'))})\n",
    "    keep_indices = indices_required_for_merges(keep_indices, tokenizer_json['model']['vocab'], tokenizer_json['model']['merges'])\n",
    "    if keep_token_order: keep_indices = sorted(keep_indices)\n",
    "    mapping = {old: new for new, old in enumerate(keep_indices)}\n",
    "    tokenizer_json['model']['vocab'] = {k: mapping[v] for k, v in tokenizer_json['model']['vocab'].items() if v in mapping}\n",
    "    tokenizer_json['model']['merges'] = remove_unused_merges(tokenizer_json['model']['merges'], tokenizer_json['model']['vocab'])\n",
    "    special_tokens_order = [t['id'] for t in tokenizer_json['added_tokens']]\n",
    "    assert special_tokens_order==sorted(special_tokens_order)\n",
    "    tokenizer_json['added_tokens'] = sorted([{**t, 'id': mapping[t['id']]} for t in tokenizer_json['added_tokens'] if t['id'] in mapping], key=lambda t: t['id'])\n",
    "    map_special_tokens(tokenizer_json.get('post_processor'), mapping)\n",
    "    tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n",
    "    return mapping, keep_indices\n",
    "\n",
    "def shrink_model_embeddings(model, keep_indices, mapping):\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        row_select = torch.tensor(list(keep_indices))\n",
    "        new_embed_t = torch.index_select(model.get_input_embeddings().weight.data, 0, row_select.to(model.get_input_embeddings().weight.data.device))\n",
    "        new_lm_head = torch.index_select(model.get_output_embeddings().weight.data, 0, row_select.to(model.get_output_embeddings().weight.data.device))\n",
    "        model.resize_token_embeddings(len(keep_indices))\n",
    "        model.get_input_embeddings().weight.data[:] = new_embed_t\n",
    "        model.get_output_embeddings().weight.data[:] = new_lm_head\n",
    "        for config in [model.config, model.generation_config]:\n",
    "            for k, v in list(config.to_dict().items()):\n",
    "                if k.endswith('token_id'):\n",
    "                    setattr(config, k, [mapping.get(t) for t in v] if isinstance(v, list) else mapping.get(v))\n",
    "\n",
    "def shrink_embeddings(model, tokenizer, corpus=None, keep_token_ids=[], keep_tokens=[], remove_token_ids=[], keep_model_tokens=True, keep_special_tokens=True, keep_normalizer=False, keep_token_order=True):\n",
    "    if not keep_normalizer: remove_tokenizer_normalizer(tokenizer)\n",
    "    from collections import OrderedDict  # use as OrderedSet\n",
    "    keep_indices = OrderedDict()\n",
    "    keep_indices.update({k: None for k in keep_token_ids})\n",
    "    keep_indices.update({tokenizer.vocab[t]: None for t in keep_tokens})\n",
    "    if corpus is not None: keep_indices.update({k: None for k in tokenizer(corpus)['input_ids']})\n",
    "    if keep_model_tokens:\n",
    "        for config in [model.config, model.generation_config]:\n",
    "            for k, v in config.to_dict().items():\n",
    "                if k.endswith('token_id'):\n",
    "                    keep_indices.update({k: None for k in (v if isinstance(v, list) else [v])})\n",
    "    keep_indices.pop(None, None)\n",
    "    for idx in remove_token_ids: keep_indices.pop(idx, None)\n",
    "    mapping, keep_indices = shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order)\n",
    "    shrink_model_embeddings(model, keep_indices, mapping=mapping)\n",
    "    return mapping\n",
    "\n",
    "def fix_dtypes(model, fix_weights=True, fix_quant_states=True):\n",
    "    import torch\n",
    "    for module in model.modules():\n",
    "        weight = getattr(module, 'weight', None)\n",
    "        if weight is not None:\n",
    "            if torch.is_floating_point(weight):\n",
    "                if fix_weights and weight.dtype!=model.dtype:\n",
    "                    module.to(model.dtype)\n",
    "            else:\n",
    "                qs = getattr(weight, 'quant_state', None)\n",
    "                if qs is not None:\n",
    "                    if fix_quant_states and qs.dtype!=model.dtype:\n",
    "                        qs.dtype = model.dtype\n",
    "    return model\n",
    "\n",
    "def merge_peft_into_base(model):\n",
    "    print('*** Merge peft model into base model...')\n",
    "    assert is_peft_model(model)\n",
    "    return fix_dtypes(model.merge_and_unload())\n",
    "\n",
    "def save_model(store_path, model=None, tokenizer=None, merge=False):\n",
    "    if merge: model = merge_peft_into_base(model)\n",
    "    if store_path is not None:\n",
    "        assert model is not None or tokenizer is not None\n",
    "        print(f\"*** Saving{' merged' if merge else ''} model/tokenizer to '{store_path}'...\")\n",
    "        if model is not None: model.save_pretrained(store_path)\n",
    "        if tokenizer is not None:\n",
    "            tokenizer.save_pretrained(store_path)\n",
    "            to_delete = os.path.join(store_path, 'tokenizer.model')\n",
    "            if os.path.isfile(to_delete): os.remove(to_delete)\n",
    "    return model\n",
    "\n",
    "def is_unsloth_model(model):\n",
    "    return model.model_tags is not None and 'unsloth' in model.model_tags\n",
    "\n",
    "def is_peft_model(model):\n",
    "    return hasattr(model, 'peft_type')\n",
    "\n",
    "def download_model(repo_id, store_path, get_name=lambda n: os.path.join(n.replace('/', '--'), 'transformers', 'default', '1')):\n",
    "    import os\n",
    "    if os.path.exists(repo_id): return repo_id\n",
    "    model_path = os.path.join(store_path, get_name(repo_id))\n",
    "    if not os.path.exists(model_path):\n",
    "        from huggingface_hub import snapshot_download\n",
    "        download_path = snapshot_download(repo_id=repo_id)\n",
    "        os.makedirs(os.path.split(model_path)[0], exist_ok=True)\n",
    "        os.symlink(download_path, model_path, target_is_directory=True)\n",
    "    return model_path\n",
    "\n",
    "def get_and_fix_peft_weights(store):\n",
    "    print(f\"*** Load peft state_dict from '{store}'...\")\n",
    "    from peft import load_peft_weights\n",
    "    state_dict = load_peft_weights(store)\n",
    "    for k in list(state_dict.keys()):\n",
    "        if 'modules_to_save' in k:\n",
    "            del state_dict[k]\n",
    "            original_module_key = k.replace('.modules_to_save.', '.original_module.')\n",
    "            if original_module_key in state_dict: del state_dict[original_module_key]\n",
    "            assert k.replace('.modules_to_save.', '.') in state_dict\n",
    "    return state_dict\n",
    "\n",
    "def set_peft_weights(model, state_dict):\n",
    "    print(f\"*** Set model state_dict...\")\n",
    "    from peft import set_peft_model_state_dict\n",
    "    res = set_peft_model_state_dict(model, state_dict)\n",
    "    assert not res.unexpected_keys\n",
    "\n",
    "def load_peft_state(model, store):\n",
    "    set_peft_weights(model, get_and_fix_peft_weights(store))\n",
    "\n",
    "def prepare_model(model, mode, tokenizer=None, formatter=None, shrink_embedding=False, dequantize=False, peft=[], local_files_only=False, add_special_tokens={}, set_pad_token=None, keep_tokens=[], keep_normalizer=None, peft_trainable=True, device_map=None, tf_grad_cp=True, tf_use_fa2=True, num_active_layers=None, **kwargs):\n",
    "    if isinstance(model, str):\n",
    "        assert tokenizer is None\n",
    "        print(f\"*** Load base model and tokenizer from '{model}'...\")\n",
    "        if mode=='unsloth_4bit':\n",
    "            assert device_map is None, 'unsupported'\n",
    "            from unsloth import FastLanguageModel\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(model_name=model, dtype=None, load_in_4bit=True, local_files_only=local_files_only, **kwargs)\n",
    "        elif mode in ['transformers', 'transformers_bf16', 'transformers_4bit', 'transformers_bf16_4bit', 'tokenizer_only']:\n",
    "            import torch\n",
    "            model_load_args = {}\n",
    "            if device_map is not None: model_load_args['device_map'] = device_map\n",
    "            if tf_use_fa2: model_load_args['attn_implementation'] = 'flash_attention_2'\n",
    "            if mode in ['transformers_bf16', 'transformers_bf16_4bit']: model_load_args['torch_dtype'] = torch.bfloat16\n",
    "            elif mode in ['transformers_4bit', 'transformers_bf16_4bit']:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "                model_load_args['quantization_config'] = nf4_config\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=local_files_only, **kwargs)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model, **model_load_args) if mode!='tokenizer_only' else None\n",
    "            if tf_grad_cp and model is not None: model.gradient_checkpointing_enable()\n",
    "        else: raise NotImplementedError('Unknown mode.')\n",
    "    if add_special_tokens: tokenizer.add_special_tokens(add_special_tokens)\n",
    "    if set_pad_token is not None: tokenizer.pad_token = set_pad_token\n",
    "    if formatter is not None and not hasattr(formatter, 'corpus'):\n",
    "        formatter = formatter(tokenizer=tokenizer)\n",
    "    if (shrink_embedding<len(tokenizer.vocab) if type(shrink_embedding)==int else shrink_embedding) or keep_normalizer is False:\n",
    "        print('*** Shrink embedding...')\n",
    "        embedding_size_before_shrink = len(tokenizer.vocab)\n",
    "        mapping = shrink_embeddings(model, tokenizer, formatter.get_corpus(), keep_tokens=keep_tokens, keep_normalizer=keep_normalizer)\n",
    "        print(f'*** -> Reduced embedding size from {embedding_size_before_shrink} to {len(mapping)} words.')\n",
    "    if dequantize:\n",
    "        print(f'*** Dequantize model...')\n",
    "        model = model.dequantize()\n",
    "    if len(peft):\n",
    "        peft_trained = True if is_peft_model(model) else None\n",
    "        for i, m in enumerate(peft):\n",
    "            if peft_trained is True: model, peft_trained = merge_peft_into_base(model), None\n",
    "            if isinstance(m, str):\n",
    "                if peft_trained is False:\n",
    "                    _, peft_trained = load_peft_state(model, m), True\n",
    "                else:\n",
    "                    print(f\"*** Load peft model from '{m}'...\")\n",
    "                    from peft import PeftModel\n",
    "                    model, peft_trained = PeftModel.from_pretrained(model, m, trainable=peft_trainable), True\n",
    "            else:\n",
    "                assert peft_trained is None\n",
    "                if isinstance(m, dict):\n",
    "                    print('*** Create new peft model...')\n",
    "                    if is_unsloth_model(model):\n",
    "                        from unsloth import FastLanguageModel\n",
    "                        my_get_peft_model = FastLanguageModel.get_peft_model\n",
    "                    else:\n",
    "                        from peft import LoraConfig, get_peft_model\n",
    "                        my_get_peft_model = lambda model, **kwargs: get_peft_model(model, LoraConfig(**kwargs))\n",
    "                    model, peft_trained = my_get_peft_model(model, **m), False\n",
    "                else: assert m is None\n",
    "    layers_to_freeze = None\n",
    "    if hasattr(model, 'model'):\n",
    "        if hasattr(model.model, 'model') and hasattr(model.model.model, 'layers'):\n",
    "            layers_to_freeze = model.model.model.layers\n",
    "        elif hasattr(model.model, 'layers'):\n",
    "            layers_to_freeze = model.model.layers\n",
    "\n",
    "    if num_active_layers is not None and is_peft_model(model) and layers_to_freeze is not None:\n",
    "        print(f\"*** Activating only the first {num_active_layers} layers and freezing the rest...\")\n",
    "        total_layers = len(layers_to_freeze)\n",
    "        if num_active_layers > total_layers:\n",
    "            print(f\"*** WARNING: num_active_layers ({num_active_layers}) is greater than total layers ({total_layers}). All layers will be active.\")\n",
    "        else:\n",
    "            for i, layer in enumerate(layers_to_freeze):\n",
    "                if i >= num_active_layers:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            all_param = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"*** -> Layers from {num_active_layers} to {total_layers - 1} are frozen.\")\n",
    "            print(f\"*** -> Trainable params after freezing: {trainable_params:,} ({100 * trainable_params / all_param:.2f}%)\")\n",
    "    return model, tokenizer, formatter\n",
    "\n",
    "def training_run(model, formatter, dataset, train_args, max_seq_length, merge=False, store=None, packing=False, grad_acc_fix=False, optimizers=None):\n",
    "    assert merge is False, \"merge after training does not seen to work (at least with unsloth, saved merged model will cointain the untrained weights!)\"\n",
    "    import torch\n",
    "    from datasets import Dataset\n",
    "    add_train_args = {}\n",
    "    if is_unsloth_model(model):\n",
    "        from unsloth import FastLanguageModel\n",
    "        from unsloth import UnslothTrainer as Trainer\n",
    "        from unsloth import UnslothTrainingArguments as TrainingArguments\n",
    "        from unsloth import is_bfloat16_supported\n",
    "        FastLanguageModel.for_training(model)\n",
    "        add_train_args.update(fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported())\n",
    "    else:\n",
    "        from trl import SFTConfig as TrainingArguments\n",
    "        from trl import SFTTrainer as Trainer\n",
    "        model.train()\n",
    "        add_train_args.update(bf16=True)\n",
    "\n",
    "    formatter.tokenizer.padding_side = 'right'\n",
    "    if is_unsloth_model(model):\n",
    "        for convert_to_float in [model.get_input_embeddings(), model.get_output_embeddings()]:\n",
    "            if convert_to_float.weight.dtype!=torch.float32: convert_to_float.to(torch.float32)\n",
    "\n",
    "    add_args = {}\n",
    "    if optimizers is not None: add_args['optimizers'] = optimizers\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=formatter.tokenizer,\n",
    "        data_collator=formatter.get_data_collator(),\n",
    "        train_dataset=Dataset.from_list(dataset.as_list(formatter)),\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=None,\n",
    "        packing=packing,  # Can make training 5x faster for short sequences.\n",
    "        **add_args,\n",
    "        args=TrainingArguments(\n",
    "            **add_train_args,\n",
    "            **train_args\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print('*** Start training run...')\n",
    "    if grad_acc_fix and is_unsloth_model(model):\n",
    "        from unsloth import unsloth_train\n",
    "        trainer_stats = unsloth_train(trainer)\n",
    "    else:\n",
    "        if is_unsloth_model(model) and train_args['gradient_accumulation_steps']>1: print('*** WARNING: using faulty unsloth gradient accumulation')\n",
    "        trainer_stats = trainer.train()\n",
    "    try: print(f'*** -> Training took {trainer_stats.metrics[\"train_runtime\"]} seconds.')\n",
    "    except: pass\n",
    "    if store is not None: save_model(store, model, formatter.tokenizer, merge=merge)\n",
    "    return model, trainer_stats\n",
    "\n",
    "def inference_load(store, keys=True, result_dict=None, always_read_from_file=False):\n",
    "    if result_dict is None: result_dict = {}\n",
    "    if store is not None:\n",
    "        if keys is True: keys = os.listdir(store)\n",
    "        for key in keys:\n",
    "            if always_read_from_file or key not in result_dict:\n",
    "                try:\n",
    "                    with bz2.BZ2File(os.path.join(store, key)) as f: result_dict[key] = pickle.load(f)\n",
    "                except: continue\n",
    "    return result_dict\n",
    "\n",
    "def inference_save(store, key, outputs):\n",
    "    if store is not None:\n",
    "        os.makedirs(store, exist_ok=True)\n",
    "        with bz2.BZ2File(os.path.join(store, key), 'w') as f: pickle.dump(outputs, f)\n",
    "\n",
    "class Decoder(object):\n",
    "    def __init__(self, formatter, dataset, n_guesses, max_outputs=None, frac_score=False, quiet=False, name='', additional_decoders=None, prob_baseline=None):\n",
    "        self.formatter = formatter\n",
    "        self.dataset = dataset\n",
    "        self.n_guesses = n_guesses\n",
    "        self.decoded_results = {}\n",
    "        self.correct_solutions = {}\n",
    "        self.keys_lim = set()\n",
    "        self.keys_all = set()\n",
    "        self.mult_cnt = {}\n",
    "        self.keys_cnt = {}\n",
    "        self.frac_score = frac_score\n",
    "        self.max_outputs = max_outputs\n",
    "        self.quiet = quiet\n",
    "        self.input_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='input') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n",
    "        self.reply_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='reply') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n",
    "        self.additional_decoders = additional_decoders\n",
    "        self.name = name\n",
    "        self.prob_tracker = {}\n",
    "        self.prob_tracker_best = {}\n",
    "        self.prob_baseline = prob_baseline\n",
    "\n",
    "    def score(self, *to_score):\n",
    "        scores = [(sum(1/self.mult_cnt[k.split('_')[0]] for k in s) if self.frac_score else len(s)) for s in to_score]\n",
    "        score_cnt = len(self.mult_cnt if self.frac_score else self.keys_cnt)\n",
    "        return scores, score_cnt\n",
    "\n",
    "    def from_store(self, store, **kwargs):\n",
    "        for key, outputs in inference_load(store).items():\n",
    "            self.process(key, outputs, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def score_fmt(self, v):\n",
    "        return f'{v:5.1f}' if self.frac_score else f'{v:3}'\n",
    "\n",
    "    def process_single_output(self, key, output_len, decoded, print_func=print, len_info=None, device_info=None):\n",
    "        import numpy as np\n",
    "        inv_mod = {k: v if k.endswith('val') else self.dataset.invert_mod(v, key, inv_perm=(k.startswith('output') or k.startswith('score_all'))) for k, v in decoded.items()}\n",
    "        base_key = key.split('.')[0]\n",
    "        self.decoded_results[base_key] = self.decoded_results.get(base_key, {})\n",
    "        self.decoded_results[base_key][key] = inv_mod\n",
    "        output = inv_mod.get('output')\n",
    "        score = inv_mod.get('score')\n",
    "\n",
    "        # quick scoring\n",
    "        self.keys_cnt[base_key] = self.keys_cnt.get(base_key, 0) + 1\n",
    "        mult_key, mult_sub = (base_key.split('_') + ['0'])[:2]\n",
    "        self.mult_cnt[mult_key] = max(self.mult_cnt.get(mult_key, 0), int(mult_sub) + 1)\n",
    "        if len(self.dataset.replies):\n",
    "            correct_solution = self.dataset.replies.get(base_key)\n",
    "            if correct_solution is not None:\n",
    "                correct_solution = correct_solution[0]\n",
    "                self.correct_solutions[base_key] = correct_solution\n",
    "                is_correct = correct_solution is not None and np.array_equal(correct_solution, output)\n",
    "                if is_correct:\n",
    "                    self.keys_all.add(base_key)\n",
    "                    if self.keys_cnt[base_key] <= self.n_guesses: self.keys_lim.add(base_key)\n",
    "            corr_str = 'cant_decode' if output is None else 'sol_unknown' if correct_solution is None else 'ALL_CORRECT' if is_correct else 'bad_xy_size' if np.shape(correct_solution)!=np.shape(output) else 'bad_content'\n",
    "            (score_lim, score_all), score_cnt = self.score(self.keys_lim, self.keys_all)\n",
    "\n",
    "            tp_arr = (key.count('transpose') + key.count('rot90')) % 2\n",
    "            msc = None if score is None else np.sum(score)\n",
    "            fsc = inv_mod.get('score_val')\n",
    "            if output is not None and fsc is not None:\n",
    "                pt = self.prob_tracker[base_key] = self.prob_tracker.get(base_key, {})\n",
    "                hash = tuple(map(tuple, output))\n",
    "                prob = pt[hash] = pt.get(hash, 0) + (np.exp(fsc) if self.prob_baseline is None else fsc - np.log(self.prob_baseline))\n",
    "                current_best = self.prob_tracker_best.get(base_key)\n",
    "                if current_best is None or current_best[0]<prob:\n",
    "                    self.prob_tracker_best[base_key] = (prob, output)\n",
    "            fmt_name = f'{self.name}: ' if self.name else ''\n",
    "            msc_print = f'{min(-msc, 9.99999):7.5f}' if msc is not None else 'unknown'\n",
    "            fsc_print = f'{min(-fsc, 9.99999):7.5f}' if fsc is not None else 'unknown'\n",
    "            if not self.quiet: print_func(f\" {fmt_name}acc: {self.score_fmt(score_lim)}/{score_cnt:3}={min(score_lim/score_cnt, 0.999):5.1%} (2-guess), {self.score_fmt(score_all)}/{score_cnt:3}={min(score_all/score_cnt, 0.999):5.1%} (any);{f' {device_info}' if device_info else ''} tok:{self.input_len[tp_arr].get(base_key, '?'):>4}+{self.reply_len[tp_arr].get(base_key, '?'):>3}>{'n/a' if output_len is None else output_len:>3} {corr_str}:{msc_print}|{fsc_print} [{key}]\")\n",
    "\n",
    "    def get_current_best(self, base_key):\n",
    "        current_best = self.prob_tracker_best.get(base_key)\n",
    "        return None if current_best is None else current_best[1]\n",
    "\n",
    "    def process_single_decode(self, key, de_tokenized, print_func=print, **kwargs):\n",
    "        if len(de_tokenized)==3 and not isinstance(de_tokenized[1], float):  # for backwards compatibility\n",
    "            output_len, *data = de_tokenized\n",
    "            score_val = None\n",
    "        else: output_len, score_val, *data = de_tokenized\n",
    "        if self.formatter is None:\n",
    "            assert len(data) == 1\n",
    "            decoded = [data[0]]\n",
    "        else: decoded = self.formatter.decode_to_array(*data)\n",
    "        #if len(decoded)==2:\n",
    "        #    same = np.array_equal(decoded[0].get('output'), decoded[1].get('output'))\n",
    "        #    print_func(f\"is_identical: {same}\")\n",
    "        #    if not same: for i in range(2): print_func(str(decoded[i].get('output')))\n",
    "        for d in decoded: d['score_val'] = score_val\n",
    "        for i, dec in enumerate(decoded):\n",
    "            if i==0: self.process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n",
    "            elif self.additional_decoders:\n",
    "                if i-1<len(self.additional_decoders): self.additional_decoders[i-1].process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n",
    "                else: print_func(f'{key} no decoder available for output #{i}')\n",
    "            else: self.process_single_output(f'{key}.fix{i}', output_len, dec, print_func=print_func, **kwargs)\n",
    "\n",
    "    def process(self, key, de_tokenized, **kwargs):\n",
    "        for i, d in enumerate(de_tokenized):\n",
    "            if self.max_outputs is None or i<=self.max_outputs:\n",
    "                self.process_single_decode(f'{key}.out{i}', d, **kwargs)\n",
    "\n",
    "    def get_unsolved_keys(self):\n",
    "        unsolved = []\n",
    "        for base_key, reply in self.dataset.replies.items():\n",
    "            if not any(np.array_equal(reply[0], s.get('output')) for s in self.decoded_results.get(base_key, {}).values()):\n",
    "                unsolved.append(base_key)\n",
    "        return unsolved\n",
    "\n",
    "    def run_selection_algo(self, selection_algorithm):\n",
    "        return {bk: (selection_algorithm({k: g for k, g in v.items() if g.get('output') is not None}) if any(g.get('output') is not None for g in v.values()) else []) for bk, v in self.decoded_results.items()}\n",
    "\n",
    "    def benchmark_selection_algos(self, selection_algorithms, skip_failed=True):\n",
    "        import numpy as np\n",
    "        results = {}\n",
    "        print('*** Benchmark selection algorithms...')\n",
    "        for selection_algorithm in selection_algorithms:\n",
    "            name = selection_algorithm.__name__\n",
    "            try:\n",
    "                selected = self.run_selection_algo(selection_algorithm)\n",
    "                if self.formatter is not None:\n",
    "                    for sols in selected.values():\n",
    "                        for s in sols:\n",
    "                            assert self.formatter.is_valid_solution(s), f'found invalid solutions {s}'\n",
    "                correct_keys = {k for k, v in selected.items() if self.correct_solutions.get(k) is not None and any(np.array_equal(guess, self.correct_solutions[k]) for guess in v[:self.n_guesses])}\n",
    "                (score,), score_cnt = self.score(correct_keys)\n",
    "                results[name] = score\n",
    "                print(f\" acc: {score:5.1f}/{score_cnt:3}={score/score_cnt:6.2%} ('{name}')\")\n",
    "            except:\n",
    "                print(f\" {'execution failed':>21} ('{name}')\")\n",
    "                if not skip_failed: raise\n",
    "        return results\n",
    "\n",
    "    def calc_augmented_scores(self, model, base_keys=None, store=None, seed=0, max_len=None, make_unique=True, quiet=False, **kwargs):\n",
    "        if base_keys is None: base_keys = list(self.decoded_results.keys())\n",
    "        if store is not None: store = f'{store}_new'  # new format is not backwards compatible, so use new folder\n",
    "        for bk in (base_keys if quiet else tqdm(base_keys, desc='calculate augmented scores', file=sys.stdout)):\n",
    "            res = self.decoded_results.get(bk, {})\n",
    "            known_scores = {}\n",
    "            for k, v in sorted(res.items()):\n",
    "                if 'output' in v:\n",
    "                    k_store = None if store is None else os.path.join(store, k)\n",
    "                    id = tuple(map(tuple, v['output']))\n",
    "                    if not (make_unique and id in known_scores):\n",
    "                        try:\n",
    "                            assert k_store is not None\n",
    "                            with bz2.BZ2File(k_store) as f: known_scores[id] = pickle.load(f)\n",
    "                            if isinstance(known_scores[id], list): known_scores[id] = dict(score_multi=known_scores[id])  # for backwards compatibility\n",
    "                            k_store = None\n",
    "                        except:\n",
    "                            temp_dataset = self.dataset.__class__(\n",
    "                                keys=[bk],\n",
    "                                queries={bk: self.dataset.queries.get(bk)},\n",
    "                                replies={bk: [v['output'].tolist()]},\n",
    "                            )\n",
    "                            temp_decoder = self.__class__(self.formatter, temp_dataset, n_guesses=self.n_guesses, quiet=True)\n",
    "                            temp_dataset = temp_dataset.augment(**kwargs, seed=(seed+hash(k)+hash(id)) % 1024**2, quiet=True)\n",
    "                            if max_len is not None: temp_dataset = temp_dataset.cut_to_len(formatter=self.formatter, name='input', max_len=max_len, quiet=True)\n",
    "                            for x in temp_dataset.as_list(self.formatter): calc_score(**x, formatter=self.formatter, model=model, decoder=temp_decoder)\n",
    "                            known_scores[id] = dict(\n",
    "                                score_multi=[np.sum(x['score']) for x in temp_decoder.decoded_results[bk].values()],\n",
    "                                score_multi_nl=[x['score_val'] for x in temp_decoder.decoded_results[bk].values()],\n",
    "                                score_multi_array=np.array([x['score'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                                score_multi_array_cum=np.array([x['score_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                                score_multi_array_all=np.array([x['score_all'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                                score_multi_array_all_cum=np.array([x['score_all_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                            )\n",
    "                            if k_store is not None:\n",
    "                                os.makedirs(store, exist_ok=True)\n",
    "                                with bz2.BZ2File(k_store, 'w') as f: pickle.dump(known_scores[id], f)\n",
    "                    v.update(known_scores[id])\n",
    "\n",
    "def turbo_dfs(model, logits, path, eos_token_id, max_new_tokens, max_score, max_score_greedy, temperature, suppress_tokens, torch, score=0.0, pos=0, cache=None):\n",
    "    logits, next_logits = logits[0], (logits[1:] if len(logits)>1 else None)\n",
    "    nll = -(logits / temperature).detach().float().log_softmax(-1).cpu().numpy()\n",
    "    greedy_index = nll.argmin(-1).item()\n",
    "    nll = list(enumerate(nll))\n",
    "    if path: nll[0], nll[path[0]], path = nll[path[0]], nll[0], path[1:]  # follow precomputed path first\n",
    "    suffixes = []\n",
    "    for i, s in nll:\n",
    "        next_score = score + s\n",
    "        allowed_max_score = max_score_greedy if i==greedy_index else max_score\n",
    "        if next_score < allowed_max_score:\n",
    "            if i==eos_token_id: next_suffixes = [(next_score, [], [])]\n",
    "            elif max_new_tokens>1:\n",
    "                if next_logits is None:\n",
    "                    if pos<cache[0][0][0].shape[2]: cache[0] = tuple(tuple(c[:, :, :pos] for c in l) for l in cache[0])\n",
    "                    next_logits, cache[0] = model(\n",
    "                        input_ids= torch.full((1,1), i, device=model.device),\n",
    "                        position_ids=torch.full((1,1), pos, device=model.device),\n",
    "                        past_key_values=cache[0],\n",
    "                    )[:2]\n",
    "                    next_logits = next_logits[0]  # unbatch\n",
    "                next_suffixes = turbo_dfs(model, logits=next_logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens-1, max_score=max_score, max_score_greedy=allowed_max_score, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=next_score, pos=pos+1, cache=cache)\n",
    "            else: next_suffixes = []\n",
    "            for suffix in next_suffixes:\n",
    "                suffix[1].append(i)\n",
    "                suffix[2].append(logits)\n",
    "            suffixes.extend(next_suffixes)\n",
    "        next_logits = None\n",
    "    return suffixes\n",
    "\n",
    "def inference_turbo_dfs(model, input_ids, eos_token_id, max_new_tokens, min_prob, min_prob_greedy=1, temperature=1.0, suppress_tokens=[], path=[], attention_mask=None):\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        assert attention_mask is None or attention_mask.all(), 'not implemented'\n",
    "        input_ids = torch.as_tensor(input_ids, device=model.device, dtype=int)\n",
    "        if input_ids.ndim==2: input_ids = input_ids.squeeze(0)\n",
    "        assert input_ids.ndim==1, 'batching not supported'\n",
    "        max_score = -np.log(min_prob)\n",
    "        max_score_greedy = (-np.log(min_prob_greedy)) if min_prob_greedy>0 else float('inf')  # avoid throwing numpy error\n",
    "        max_score_greedy = max(max_score, max_score_greedy)\n",
    "        if path is None: path = []\n",
    "        if len(path) and path[-1]==eos_token_id: path = path[:-1]\n",
    "        with torch.no_grad():\n",
    "            full_path = input_ids\n",
    "            if len(path): full_path = torch.cat([full_path, torch.as_tensor(path, device=model.device)])\n",
    "            logits, cache = model(input_ids=full_path[np.newaxis])[:2]\n",
    "            logits = logits[0, len(input_ids)-1:]\n",
    "        result = turbo_dfs(model, logits=logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens, max_score=max_score, max_score_greedy=max_score_greedy, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=0.0, pos=len(input_ids), cache=[cache])\n",
    "        return sorted([(score_val, np.array(suffix[::-1]), torch.stack(score_arr[::-1]).float().cpu().numpy()) for score_val, suffix, score_arr in result], key=lambda x:x[0])\n",
    "\n",
    "def inference_step(tokenized, model, remove_token_type_ids=True, num_beams=1, formatter=None, min_prob=None, current_best=None, **kwargs):\n",
    "    import torch\n",
    "    if remove_token_type_ids: tokenized.pop('token_type_ids', None)\n",
    "    if min_prob is not None:\n",
    "        assert num_beams==1\n",
    "        gen = inference_turbo_dfs(model, **tokenized.to(model.device), path=current_best, min_prob=min_prob, eos_token_id=formatter.tokenizer.eos_token_id, **kwargs)\n",
    "        tokens_out = [[g[1] for g in gen]]\n",
    "        scores_out = [[g[2] for g in gen]]\n",
    "    elif is_unsloth_model(model) and num_beams > 1:\n",
    "        assert False, 'unsloth does not support beam search'\n",
    "    else:\n",
    "        gen = model.generate(**tokenized.to(model.device), return_dict_in_generate=True, output_logits=True, use_cache=True, **kwargs)\n",
    "        tokens_out = gen['sequences'][:, torch.newaxis, tokenized['input_ids'].shape[-1]:].cpu().numpy().copy()\n",
    "        scores_out = torch.stack(gen['logits'], axis=-2)[:, torch.newaxis].float().cpu().numpy().copy()\n",
    "    return tokens_out, scores_out\n",
    "\n",
    "def process_inference_output(key, outputs, formatter, store=None, decoder=None, decoder_args={}):\n",
    "    de_tokenized = [formatter.de_tokenize(*output) for output in zip(*outputs)]\n",
    "    inference_save(store, key, de_tokenized)\n",
    "    if decoder is not None: decoder.process(key, de_tokenized, **decoder_args)\n",
    "    return de_tokenized\n",
    "\n",
    "def inference_run_v2(model, formatter, dataset, decoder=None, max_new_tokens=None, max_batch_size=1, store=None, result_dict=None, rerun_empty=False, retrain=None, use_turbo=False, group_multi_output=True, **kwargs):\n",
    "    import torch\n",
    "    assert max_batch_size==1, 'unsupported'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('*** Load stored data...')\n",
    "        if result_dict is None: result_dict = {}\n",
    "        result_dict = inference_load(store, dataset.keys, result_dict)\n",
    "        by_base_key = {}\n",
    "        needs_rerun = {}\n",
    "        base_key_list = []\n",
    "        for key in dataset.keys:\n",
    "            base_key = key.split('.')[0]\n",
    "            if group_multi_output: base_key = base_key.split('_')[0]\n",
    "            if base_key not in by_base_key: base_key_list.append(base_key)\n",
    "            bk_list = by_base_key[base_key] = by_base_key.get(base_key, [])\n",
    "            bk_list.append(key)\n",
    "        for base_key, keys in by_base_key.items():\n",
    "            for key in keys:\n",
    "                de_tokenized = result_dict.get(key)\n",
    "                if de_tokenized is None or (rerun_empty and not de_tokenized):\n",
    "                    bk_list = needs_rerun[base_key] = needs_rerun.get(base_key, [])\n",
    "                    bk_list.append(key)\n",
    "                elif decoder is not None: decoder.process(key, de_tokenized)\n",
    "\n",
    "        formatter.tokenizer.padding_side = 'left'\n",
    "        if max_new_tokens is None: max_new_tokens = formatter.max_new_tokens()\n",
    "        if is_unsloth_model(model):\n",
    "            from unsloth import FastLanguageModel\n",
    "            FastLanguageModel.for_inference(model)\n",
    "        else: model.eval()\n",
    "\n",
    "        print('*** Start inference run...')\n",
    "    try:\n",
    "        with tqdm(base_key_list, file=sys.stdout) as pbar:\n",
    "            for base_key in pbar:\n",
    "                run_keys = needs_rerun.get(base_key)\n",
    "                if run_keys:\n",
    "                    if retrain is not None:\n",
    "                        retrain_dataset = dataset.keep_key_startswith(base_key)\n",
    "                        print(f\"retraining model for key '{base_key}' (retrain_dataset_size={len(retrain_dataset.keys)})\")\n",
    "                        retrain(model, retrain_dataset)\n",
    "                        if is_unsloth_model(model): FastLanguageModel.for_inference(model)\n",
    "                    with torch.no_grad():\n",
    "                        for key in run_keys:\n",
    "                            input_text = dataset.get(key, formatter)['input']\n",
    "                            batch = formatter.tokenizer([input_text], return_tensors='pt')\n",
    "                            current_best = decoder.get_current_best(key.split('.')[0]) if use_turbo else None\n",
    "                            if current_best is not None:\n",
    "                                current_best = dataset.forward_mod(current_best, key)\n",
    "                                current_best = formatter.fmt_reply([current_best])\n",
    "                                current_best = formatter.tokenizer(input_text+current_best)['input_ids'][batch['input_ids'].shape[-1]:]\n",
    "                            batch_out = inference_step(batch, model, formatter=formatter, max_new_tokens=max_new_tokens, current_best=current_best, **kwargs)\n",
    "                            outputs = [x[0] for x in batch_out]\n",
    "                            result_dict[key] = process_inference_output(key, outputs, formatter, store=store, decoder=decoder, decoder_args=dict(print_func=pbar.write))\n",
    "        print('*** Completed inference run.')\n",
    "    except KeyboardInterrupt: print('*** Ctrl+C pressed, stopping inference run.')\n",
    "    return result_dict\n",
    "\n",
    "class Retrainer(object):\n",
    "    def __init__(self, n, aug_opts, reload_state_dict=None, **kwargs):\n",
    "        self.n = n\n",
    "        self.aug_opts = aug_opts\n",
    "        self.reload_state_dict = reload_state_dict\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def preprocess(self, dataset):\n",
    "        ds = [dataset.augment(quiet=True, shfl_keys=True, **self.aug_opts) for _ in range((self.n-1)//dataset.length()+1)]\n",
    "        ds = ds[0] if len(ds)==1 else ds[0].append(*ds[1:])\n",
    "        ds, _ = ds.split_at_pos(self.n)\n",
    "        return ds\n",
    "\n",
    "    def __call__(self, model, dataset):\n",
    "        if self.reload_state_dict is not None: set_peft_weights(model, self.reload_state_dict)\n",
    "        assert is_unsloth_model(model), 'not implemented'\n",
    "        if is_unsloth_model(model):\n",
    "            from unsloth import FastLanguageModel\n",
    "            FastLanguageModel.for_training(model)\n",
    "        else: model.train()\n",
    "        training_run(model, dataset=self.preprocess(dataset), **self.kwargs)\n",
    "\n",
    "def calc_score(key, input, reply, formatter, model, store=None, decoder=None, **_):\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        input_len = len(formatter.tokenizer(input)['input_ids'])\n",
    "        tokenized = formatter.tokenizer([input+reply], return_tensors='pt')\n",
    "        reply_tok = tokenized['input_ids'][0][input_len:].cpu().numpy().copy()\n",
    "        reply_log = model.forward(**tokenized.to(model.device))['logits'][0, input_len-1: -1].float().cpu().numpy().copy()\n",
    "        process_inference_output(key, (reply_tok[torch.newaxis], reply_log[torch.newaxis]), formatter, store=store, decoder=decoder)\n",
    "\n",
    "def mem_info(gpu_id=0):\n",
    "    import torch\n",
    "    try:\n",
    "        gpu_stats = torch.cuda.get_device_properties(gpu_id)\n",
    "        usage = torch.cuda.max_memory_reserved() / 1024**3\n",
    "        avail = gpu_stats.total_memory / 1024**3\n",
    "        print(f\"*** GPU: {gpu_stats.name}, used {usage:.3} / {avail:.3} GB.\")\n",
    "    except: print('*** Exception occured when getting memory stats.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df7ab26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.399404Z",
     "iopub.status.busy": "2025-11-13T05:06:15.399178Z",
     "iopub.status.idle": "2025-11-13T05:06:15.417754Z",
     "shell.execute_reply": "2025-11-13T05:06:15.417199Z"
    },
    "papermill": {
     "duration": 0.027433,
     "end_time": "2025-11-13T05:06:15.419140",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.391707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing arc_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile arc_loader.py\n",
    "import json\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def cut_at_token(output, token_id):\n",
    "    eos_positions = (output==token_id).nonzero()[0]\n",
    "    return output[:eos_positions[0]] if len(eos_positions) else output\n",
    "\n",
    "def shuffled(data_list):\n",
    "    return np.random.permutation(data_list).tolist()\n",
    "\n",
    "def permute_mod(a, descriptor, invert=False):\n",
    "    permutation = [int(i) for i in descriptor if str(i).isdigit()]\n",
    "    assert sorted(permutation)==list(range(10))\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim==3:\n",
    "        if not invert: permutation = np.argsort(permutation)\n",
    "        a = a[..., permutation]\n",
    "    else:\n",
    "        assert a.ndim==2\n",
    "        if invert: permutation = np.argsort(permutation)\n",
    "        a = np.asarray(permutation)[a]\n",
    "    return a\n",
    "\n",
    "def permute_rnd_col_(query):\n",
    "    permutation = [0]+(1+np.random.permutation(9)).tolist()\n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "def permute_rnd_all_(query):\n",
    "    permutation = np.random.permutation(10).tolist()\n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "def permute_cnt_col_(query):\n",
    "    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n",
    "    permutation = [0]+sorted(np.random.permutation(9)+1, key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "def permute_cnt_all_(query):\n",
    "    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n",
    "    permutation = sorted(np.random.permutation(10), key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "permute_rnd_col = (permute_mod, permute_rnd_col_)\n",
    "permute_rnd_all = (permute_mod, permute_rnd_all_)\n",
    "permute_cnt_col = (permute_mod, permute_cnt_col_)\n",
    "permute_cnt_all = (permute_mod, permute_cnt_all_)\n",
    "permute_None = (np.copy, None)\n",
    "\n",
    "class ArcDataset(object):\n",
    "    @staticmethod\n",
    "    def forward_mod(a, key, use_perm=True, is_output=True):\n",
    "        if a is None: return a\n",
    "        for op in key.split('.')[1:]:\n",
    "            if op.startswith('I'):\n",
    "                if is_output: continue\n",
    "                op = op[1:]\n",
    "            if   op=='rot90':              a = np.rot90(a)\n",
    "            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n",
    "            elif op.startswith('permute'): a = permute_mod(a, op, invert=False) if use_perm else a\n",
    "            elif op.startswith('copy'):    a = np.copy(a)\n",
    "            elif op.startswith('out'):     a = a\n",
    "            elif op.startswith('ex'):      a = a\n",
    "            elif op.startswith('fix'):     a = a\n",
    "            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n",
    "            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n",
    "        return a\n",
    "\n",
    "    @staticmethod\n",
    "    def invert_mod(a, key, inv_perm=True, is_output=True):\n",
    "        if a is None: return a\n",
    "        for op in key.split('.')[1:][::-1]:\n",
    "            if op.startswith('I'):\n",
    "                if is_output: continue\n",
    "                op = op[1:]\n",
    "            if   op=='rot90':              a = np.rot90(np.rot90(np.rot90(a)))\n",
    "            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n",
    "            elif op.startswith('permute'): a = permute_mod(a, op, invert=True) if inv_perm else a\n",
    "            elif op.startswith('copy'):    a = np.copy(a)\n",
    "            elif op.startswith('out'):     a = a\n",
    "            elif op.startswith('ex'):      a = a\n",
    "            elif op.startswith('fix'):     a = a\n",
    "            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n",
    "            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n",
    "        return a\n",
    "\n",
    "    def __init__(self, queries, replies={}, keys=None, is_orig=False, is_fake=False):\n",
    "        if keys is not None: keys = [k for k in keys if k is not None]\n",
    "        self.queries = queries if keys is None else {k: queries[k] for k in keys}\n",
    "        self.replies = replies if keys is None else {k: replies[k] for k in keys if k in replies}\n",
    "        self.is_orig = is_orig\n",
    "        self.is_fake = is_fake\n",
    "        self.keys = sorted(queries.keys()) if keys is None else keys\n",
    "        self.faulty = {}\n",
    "        self.transposed_dataset = None\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls):\n",
    "        return cls(queries={}, replies={}, keys=[])\n",
    "\n",
    "    def change_keys(self, keys, keep_flags=False):\n",
    "        flags = dict(is_fake=self.is_fake, is_orig=self.is_orig) if keep_flags else {}\n",
    "        return self.__class__(queries=self.queries, replies=self.replies, keys=keys, **flags)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, queries_file):\n",
    "        print(f\"*** Load challanges from '{queries_file}'...\")\n",
    "        with open(queries_file) as f: queries = f.read()\n",
    "        import os\n",
    "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'): #Real submit\n",
    "            is_fake = False\n",
    "        else: #Fake run\n",
    "            is_fake = True\n",
    "        #is_fake = hashlib.md5(queries.encode('utf-8')).hexdigest().lower()=='a6b7dac3cab03abf2eb333e16610d6dc'\n",
    "        if is_fake: print(\"*** -> Fake test set detected, setting flag 'is_fake' to True.\")\n",
    "        return cls(\n",
    "            queries=json.loads(queries),\n",
    "            is_fake=is_fake,\n",
    "            is_orig=True,\n",
    "        )\n",
    "\n",
    "    def load_replies(self, replies_file):\n",
    "        print(f\"*** Load solutions from '{replies_file}'...\")\n",
    "        with open(replies_file) as f: replies = f.read()\n",
    "        replies_parsed = json.loads(replies)\n",
    "        self.replies = {k: replies_parsed[k] for k in self.keys}\n",
    "        return self\n",
    "\n",
    "    def split_multi_replies(self):\n",
    "        key_indices = [(k, i) for k in self.keys for i in range(len(self.queries[k]['test']))]\n",
    "        return self.__class__(\n",
    "            keys=[f'{k}_{i}' for k, i in key_indices],\n",
    "            queries={f'{k}_{i}': {'train': self.queries[k]['train'], 'test': [self.queries[k]['test'][i]]} for k, i in key_indices},\n",
    "            replies={f'{k}_{i}': [self.replies[k][i]] for k, i in key_indices if k in self.replies},\n",
    "        )\n",
    "\n",
    "    def move_test_to_train(self):\n",
    "        new_queries = {k: {'train': self.queries[k]['train'] + [{**t, 'output': self.replies[k][i]} for i, t in enumerate(self.queries[k]['test'])], 'test': []} for k in self.keys}\n",
    "        return self.__class__(queries=new_queries, keys=[k for k in self.keys])\n",
    "\n",
    "    def last_train_ex_for_test(self):\n",
    "        assert not self.replies\n",
    "        new_queries = {k: {'train': self.queries[k]['train'][:-1], 'test': [{'input': self.queries[k]['train'][-1]['input']}]} for k in self.keys}\n",
    "        new_replies = {k: [self.queries[k]['train'][-1]['output']] for k in self.keys}\n",
    "        return self.__class__(queries=new_queries, replies=new_replies, keys=[k for k in self.keys])\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def shuffled(self, seed=None):\n",
    "        if seed is not None: np.random.seed(seed)\n",
    "        return self.__class__(queries=self.queries, replies=self.replies, keys=shuffled(self.keys))\n",
    "\n",
    "    def sorted(self, **kwargs):\n",
    "        return self.__class__(queries=self.queries, replies=self.replies, keys=sorted(self.keys, **kwargs))\n",
    "\n",
    "    def append(*datasets):\n",
    "        return datasets[0].__class__(\n",
    "            queries={k: v for d in datasets for k, v in d.queries.items()},\n",
    "            replies={k: v for d in datasets for k, v in d.replies.items()},\n",
    "            keys   =[k    for d in datasets for k    in d.keys           ],\n",
    "        )\n",
    "\n",
    "    def sort_ex_by_input_size(self, seed=42, reverse=False):\n",
    "        np.random.seed(seed)\n",
    "        sort_key = lambda ex: np.prod(np.shape(ex['input']))\n",
    "        new_queries = {k2: {k: (sorted(np.random.permutation(np.array(v, dtype=object)), key=sort_key, reverse=reverse) if k=='train' else v) for k, v in v2.items()} for k2, v2 in self.queries.items()}\n",
    "        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n",
    "\n",
    "    def interleave(self, block_size, num_gpus=None):\n",
    "        keys = np.reshape(self.keys, (-1, block_size)).T\n",
    "        if num_gpus is None: return self.change_keys(keys.ravel().tolist())\n",
    "        ret, num_gpus = (None, num_gpus) if isinstance(num_gpus, int) else num_gpus\n",
    "        keys = np.concatenate([keys, np.full((-keys.shape[0]%num_gpus, keys.shape[1]), None)])\n",
    "        keys = np.reshape(keys, (keys.shape[0]//num_gpus, num_gpus, -1)).swapaxes(0, 1).reshape(num_gpus, -1)\n",
    "        new_datasets = [self.change_keys(gpu_keys.tolist()) for gpu_keys in keys]\n",
    "        return new_datasets if ret is None else new_datasets[ret]\n",
    "\n",
    "    def remove(self, *datasets):\n",
    "        remove_keys = {k for d in datasets for k in d.keys}\n",
    "        new_keys = [k for k in self.keys if k not in remove_keys]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def keep_key_startswith(self, key_start):\n",
    "        new_keys = [k for k in self.keys if k.startswith(key_start)]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def mod_single(self, mod_func, descriptor, i, keep_key, inputs_only):\n",
    "        queries = {}\n",
    "        replies = {}\n",
    "        keys    = []\n",
    "        for k0 in self.keys:\n",
    "            desc = (('copy{i}' if mod_func is np.copy else mod_func.__name__) if descriptor is None else descriptor if isinstance(descriptor, str) else descriptor(self.queries[k0])).format(i=i)\n",
    "            func = lambda a, d: np.asarray(mod_func(a) if descriptor is None else mod_func(a, d)).tolist()\n",
    "            k1 = k0 if keep_key else f\"{k0}.{'I' if inputs_only else ''}{desc}\"\n",
    "            keys.append(k1)\n",
    "            queries[k1] = {m: [{t: (func(a, desc) if t=='input' or not inputs_only else a) for t, a in x.items()} for x in e] for m, e in self.queries[k0].items()}\n",
    "            if k0 in self.replies:\n",
    "                replies[k1] = [func(a, desc) for a in self.replies[k0]]\n",
    "        ret = self.__class__(queries=queries, replies=replies, keys=keys)\n",
    "        return ret\n",
    "\n",
    "    def mod(self, mod_func, descriptor=None, n=1, stack=None, keep=False, keep_key=False, shuffle=False, join=True, inputs_only=False):\n",
    "        assert not (keep and keep_key)\n",
    "        cur = self\n",
    "        ret = [cur.shuffled() if shuffle else cur] if keep else []\n",
    "        if stack is None: stack = mod_func.__name__.startswith('rot')\n",
    "        for i in range(n):\n",
    "            cur = (cur if stack else self).mod_single(mod_func, descriptor, i=i, keep_key=keep_key, inputs_only=inputs_only)\n",
    "            ret.append(cur.shuffled() if shuffle else cur)\n",
    "        return self.__class__.append(*ret) if join else ret\n",
    "\n",
    "    def get(self, key, formatter):\n",
    "        assert formatter.out2_token is None or key in self.replies\n",
    "        train = formatter.fmt_train(self.queries[key]['train'])\n",
    "        query = formatter.fmt_query(self.queries[key]['test'], i=len(self.queries[key]['train']))\n",
    "        reply = formatter.fmt_reply(self.replies[key], self.faulty.get(key)) if key in self.replies else ''\n",
    "        text = train+query+reply if reply else formatter.fmt_train(self.queries[key]['train'], last_is_challenge=True)\n",
    "        return dict(key=key, train=train, query=query, reply=reply, input=train+query, text=text)\n",
    "\n",
    "    def as_list(self, formatter):\n",
    "        return [self.get(key, formatter) for key in self.keys]\n",
    "\n",
    "    def as_dataset(self):\n",
    "        from datasets import Dataset\n",
    "        return Dataset.from_list([{'key': k, 'query': self.queries[k], 'reply': self.replies[k]} for k in self.keys])\n",
    "\n",
    "    def get_length(self, key, formatter, name, max_of_transposed=False):\n",
    "        if formatter is None:\n",
    "            if   name=='input': return sum(np.prod(np.shape(v)) for v3 in self.queries[key].values() for v2 in v3 for v in v2.values())\n",
    "            elif name=='reply': return sum(np.prod(np.shape(v)) for v in self.replies[key])\n",
    "            else: assert False\n",
    "        else:\n",
    "            datasets = [self]\n",
    "            if max_of_transposed:\n",
    "                if self.transposed_dataset is None: self.transposed_dataset = self.mod(np.transpose, keep=False, keep_key=True)\n",
    "                datasets.append(self.transposed_dataset)\n",
    "            return max(len(formatter.tokenizer(ds.get(key, formatter=formatter)[name])['input_ids']) for ds in datasets)\n",
    "\n",
    "    def get_lengths(self, formatter, name, max_of_transposed=False):\n",
    "        return {key: self.get_length(key, formatter=formatter, name=name, max_of_transposed=max_of_transposed) for key in self.keys}\n",
    "\n",
    "    def sorted_by_len(self, reverse=False, **kwargs):\n",
    "        new_keys = [key for _, key in sorted([(v, k) for k, v in self.get_lengths(**kwargs).items()], reverse=reverse)]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def filter_by_len(self, min_len=0, max_len=float('inf'), **kwargs):\n",
    "        new_keys = [k for k, v in self.get_lengths(**kwargs).items() if min_len<=v<=max_len]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def cut_to_query_count(self, max_count, from_end=False):\n",
    "        new_queries = {}\n",
    "        for k in self.keys:\n",
    "            new_queries[k] = q = self.queries[k]\n",
    "            while len(q['train'])>max_count: q['train'] = q['train'][:-1] if from_end else q['train'][1:]\n",
    "        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n",
    "\n",
    "    def cut_to_len(self, formatter, name, max_len, max_new_tokens='auto', from_end=False, quiet=False, **kwargs):\n",
    "        if max_new_tokens:\n",
    "            if max_new_tokens=='auto': max_new_tokens = formatter.max_new_tokens()\n",
    "            max_len_old, max_len = max_len, max_len - max_new_tokens\n",
    "            if not quiet: print(f'*** Reducing task size to max. {max_len_old} tokens ({max_len} input + {max_new_tokens} generated)...')\n",
    "        elif not quiet: print(f'*** Reducing task size to max. {max_len} tokens...')\n",
    "        temp_ds = self.change_keys(self.keys)\n",
    "        new_keys = []\n",
    "        new_queries = {}\n",
    "        new_replies = {}\n",
    "        for key in (self.keys if quiet else tqdm(self.keys, file=sys.stdout)):\n",
    "            reply = temp_ds.replies.get(key)\n",
    "            while max_len<temp_ds.get_length(key, formatter=formatter, name=name, **kwargs):\n",
    "                query = temp_ds.queries[key]\n",
    "                if not key.split('.')[-1].startswith('ex'): key = f\"{key}.ex{''.join(map(str, range(len(query['train']))))}\"\n",
    "                key_split = key.split('.')\n",
    "                assert key_split[-1].startswith('ex')\n",
    "                key = '.'.join(key_split[:-1] + [f'ex{key_split[-1][2:-1] if from_end else key_split[-1][3:]}'])\n",
    "                temp_ds.queries[key] = {k: ((v[:-1] if from_end else v[1:]) if k=='train' else v) for k, v in query.items()}\n",
    "                if reply is not None: temp_ds.replies[key] = reply\n",
    "            new_keys.append(key)\n",
    "            new_queries[key] = temp_ds.queries[key]\n",
    "            if reply is not None: new_replies[key] = reply\n",
    "        return self.__class__(keys=new_keys, queries=new_queries, replies=new_replies)\n",
    "\n",
    "    def shuffle_ex(self, perm=None, keep_max=None):\n",
    "        new_keys = []\n",
    "        new_queries = {}\n",
    "        new_replies = {}\n",
    "        for key in self.keys:\n",
    "            n = len(self.queries[key]['train'])\n",
    "            p = np.random.permutation(n) if perm is None else perm\n",
    "            if keep_max is not None: p = p[:keep_max]\n",
    "            new_key = f'{key}.ex' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n",
    "            new_keys.append(new_key)\n",
    "            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='train' else v) for k, v in self.queries[key].items()}\n",
    "            if key in self.replies: new_replies[new_key] = self.replies[key]\n",
    "        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n",
    "\n",
    "    def shuffle_rp(self, keep_max=None):\n",
    "        new_keys = []\n",
    "        new_queries = {}\n",
    "        new_replies = {}\n",
    "        for key in self.keys:\n",
    "            n = len(self.queries[key]['test'])\n",
    "            p = np.random.permutation(n)\n",
    "            if keep_max is not None: p = p[:keep_max]\n",
    "            new_key = f'{key}.rp' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n",
    "            new_keys.append(new_key)\n",
    "            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='test' else v) for k, v in self.queries[key].items()}\n",
    "            if key in self.replies: new_replies[new_key] = np.array(self.replies[key], dtype=object)[p].tolist()\n",
    "        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n",
    "\n",
    "    def append_to_keys(self, test):\n",
    "        return self.change_keys([f'{k}{text}' for k in self.keys])\n",
    "\n",
    "    def random_select(self, n):\n",
    "        keys = np.array(self.keys).reshape(n, -1).T\n",
    "        choice = np.random.randint(0, n, size=[len(keys)])\n",
    "        return self.change_keys(keys[np.arange(len(keys)), choice])\n",
    "\n",
    "    def augment(self, tp=False, rot=False, n=1, perm=None, perm_append=False, shfl_keys=False, shfl_ex=False, seed=None, quiet=False, inputs_only=False):\n",
    "        if not quiet: print(f\"*** Augment dataset{' (inputs only)' if inputs_only else ''}...\")\n",
    "        np.random.seed(seed)\n",
    "        d = self\n",
    "        if tp: d = d.mod(np.transpose, keep=True, inputs_only=inputs_only)\n",
    "        if tp=='rand': d = d.random_select(n=2)\n",
    "        if rot: d = d.mod(np.rot90, n=3, keep=True, inputs_only=inputs_only)\n",
    "        if rot=='rand': d = d.random_select(n=4)\n",
    "        if perm is None and n<=1: d = d.shuffled() if shfl_keys else d\n",
    "        else: d = d.mod(*([np.copy] if perm is None else globals()[f\"permute_{perm}\"]), n=n, shuffle=shfl_keys, keep=perm_append, inputs_only=inputs_only)\n",
    "        np.random.seed(seed)\n",
    "        if shfl_ex: d = d.shuffle_ex()\n",
    "        return d\n",
    "\n",
    "    def remove_replies(self):\n",
    "        return self.__class__(queries=self.queries, replies={}, keys=[k for k in self.keys])\n",
    "\n",
    "    def split_at_pos(self, pos, random_seed=None):\n",
    "        keys = self.keys\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "            keys = np.random.permutation(keys)\n",
    "        if isinstance(pos, float): pos = int(pos * len(self.keys) + 0.5)\n",
    "        keys_split = [keys[:pos], keys[pos:]]\n",
    "        return tuple(self.change_keys(new_keys, keep_flags=True) for new_keys in keys_split)\n",
    "\n",
    "    def get_submission(self, results=None):\n",
    "        assert self.is_orig==True, 'Must be run on original dataset.'\n",
    "        submission = {k: [{f'attempt_{i+1}': [[0]] for i in range(2)} for _ in range(len(self.queries[k]['test']))] for k in self.keys}\n",
    "        if results is not None: self.fill_submission(results, submission)\n",
    "        return submission\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_submission(results, submission):\n",
    "        print(f'*** Generating submission for {len(results)} outputs...')\n",
    "        for k, v in results.items():\n",
    "            base_id, base_nr = k.split('_')\n",
    "            target_dict = submission[base_id][int(base_nr)]\n",
    "            for i, g in enumerate(v[:len(target_dict)]):\n",
    "                target_dict[f'attempt_{i+1}'] = g.tolist()\n",
    "\n",
    "    def validate_submission(self, submission):\n",
    "        assert self.is_orig==True, 'Must be run on original dataset.'\n",
    "        score = 0\n",
    "        for k, v in self.replies.items():\n",
    "            for i, r in enumerate(v):\n",
    "                for attempt in ['attempt_1', 'attempt_2']:\n",
    "                    if np.array_equal(r, submission[k][i][attempt]):\n",
    "                        score += 1 / len(v)\n",
    "                        break\n",
    "        return score\n",
    "def get_class_MyDataCollator(cache=[]):\n",
    "    if not cache:\n",
    "        from trl import DataCollatorForCompletionOnlyLM\n",
    "        class MyDataCollator(DataCollatorForCompletionOnlyLM):\n",
    "            def setup(self, out2_token_id=None, fault_token_id=None, fault_freq=0, sample_tries=8, mask_first_output=False):\n",
    "                self.out2_token_id = out2_token_id\n",
    "                self.fault_token_id = fault_token_id\n",
    "                self.fault_freq = fault_freq\n",
    "                self.sample_tries = sample_tries\n",
    "                self.mask_first_output = mask_first_output\n",
    "                return self\n",
    "\n",
    "            def torch_call(self, examples):\n",
    "                batch = super().torch_call(examples)\n",
    "                if self.out2_token_id is not None:\n",
    "                    assert not self.fault_freq\n",
    "                    for i in range(len(batch['input_ids'])):\n",
    "                        end_pos = ((batch['labels'][i] != -100              ).nonzero().max()).item() + 1\n",
    "                        mid_pos = ((batch['labels'][i] == self.out2_token_id).nonzero().max()).item() + 1\n",
    "                        beg_pos = mid_pos - (end_pos - mid_pos)\n",
    "                        batch['labels'][i][beg_pos:mid_pos] = batch['labels'][i][mid_pos:end_pos]\n",
    "                elif self.fault_freq:\n",
    "                    for i in range(len(batch['input_ids'])):\n",
    "                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n",
    "                        if not isinstance(self.fault_freq, float):\n",
    "                            eos_token_id = batch['labels'][i][end_pos - 1]\n",
    "                            num_examples = (batch['labels'][i] == eos_token_id).sum().item() - 1\n",
    "                            fault_freq = self.fault_freq[num_examples]\n",
    "                        else: fault_freq = self.fault_freq\n",
    "                        if random.random() < fault_freq:\n",
    "                            beg_pos = ((batch['labels'][i][:end_pos]==-100).nonzero().max()).item() + 1\n",
    "                            fault_pos = random.randint(beg_pos, end_pos-2)\n",
    "                            fault_tok = batch['labels'][i][fault_pos].item()\n",
    "                            for t in range(self.sample_tries):\n",
    "                                new_tok = batch['labels'][i][random.randint(beg_pos, end_pos-2)].item()\n",
    "                                if fault_tok!=new_tok:\n",
    "                                    batch['input_ids'][i][fault_pos] = new_tok\n",
    "                                    batch['labels'][i][fault_pos+1:end_pos] = self.fault_token_id\n",
    "                                    break\n",
    "                for i in range(len(batch['labels'])):\n",
    "                    for _ in range(self.mask_first_output):\n",
    "                        beg_pos = ((batch['labels'][i] != -100).nonzero().min()).item()\n",
    "                        mid_pos = ((batch['labels'][i][beg_pos:] == -100).nonzero().min()).item() + beg_pos\n",
    "                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n",
    "                        if mid_pos<end_pos: batch['labels'][i][beg_pos:mid_pos] = -100\n",
    "                return batch\n",
    "        cache.append(MyDataCollator)\n",
    "    return cache[0]\n",
    "\n",
    "class ArcFormatter(object):\n",
    "    def __init__(self, inp_prefix, out_prefix, arr_sep, out2_use=False, out2_token=None, arr_beg='', arr_end='', pretext='', pre_out=None, exa_sep='', exa_end='', qry_prefix=None, rpl_prefix=None, rpl_sep=None, dec_sep=None, min_wid=0, min_pad='', pretext_corpus_split='', masking=0, tokenizer=None, collator_kwargs={}, repeat_input_aug=None, repeat_input_pre=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inp_prefix = inp_prefix\n",
    "        self.out_prefix = out_prefix\n",
    "        self.out2_token = out2_token\n",
    "        self.out2_use = out2_use\n",
    "        assert not out2_use or out2_token is not None\n",
    "        assert not out2_use or masking in [1, 2]\n",
    "        assert masking!=2 or out2_use or rpl_prefix is not None\n",
    "        self.qry_prefix = qry_prefix if qry_prefix is not None else inp_prefix\n",
    "        self.rpl_prefix = rpl_prefix if rpl_prefix is not None else out_prefix\n",
    "        self.rpl_sep = rpl_sep if rpl_sep is not None else self.rpl_prefix\n",
    "        self.arr_sep = arr_sep\n",
    "        self.arr_beg = arr_beg\n",
    "        self.arr_end = arr_end\n",
    "        self.pretext = pretext\n",
    "        self.pre_out = pre_out\n",
    "        self.pre_out_empty = ['']*99\n",
    "        self.pretext_corpus_split = pretext_corpus_split\n",
    "        self.exa_sep = exa_sep\n",
    "        self.exa_end = exa_end\n",
    "        self.dec_sep = arr_sep if dec_sep is None else dec_sep\n",
    "        self.min_wid = min_wid\n",
    "        self.min_pad = min_pad\n",
    "        self.masking = masking\n",
    "        self.collator_kwargs = collator_kwargs\n",
    "        self.repeat_input_aug = repeat_input_aug\n",
    "        self.repeat_input_pre = repeat_input_pre\n",
    "\n",
    "    def fmt_array(self, array):\n",
    "        return self.arr_beg + self.arr_sep.join(str(row).replace(' ', '').replace(',', '').replace('[', '').replace(']', '')+self.min_pad*max(0, self.min_wid-len(row)) for row in array) + self.arr_end\n",
    "\n",
    "    def get_pre_out(self, pretext_split):\n",
    "        if self.pre_out is None: return self.pre_out_empty\n",
    "        if pretext_split: return [self.pretext_corpus_split.join(list(p) + ['']) for p in self.pre_out]\n",
    "        return self.pre_out\n",
    "\n",
    "    def fmt_train(self, train, last_is_challenge=False, pretext_split=False):\n",
    "        po = self.get_pre_out(pretext_split=pretext_split)\n",
    "        ex = [(f\"{self.fmt_query([x], i, pretext_split=pretext_split)}{self.fmt_reply([x['output']])}\" if last_is_challenge and i+1==len(train) else\n",
    "               f\"{self.inp_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.out_prefix}{self.fmt_array(x['output'])}\") for i, x in enumerate(train)]\n",
    "        pre = self.pretext_corpus_split.join(list(self.pretext)+['']) if pretext_split else self.pretext\n",
    "        end = '' if last_is_challenge else (self.exa_end + self.tokenizer.eos_token)\n",
    "        return pre + (self.exa_end + self.tokenizer.eos_token + self.exa_sep).join(ex) + end\n",
    "\n",
    "    def fmt_query(self, query, i, pretext_split=False):\n",
    "        po = self.get_pre_out(pretext_split=pretext_split)\n",
    "        return ''.join(f\"{self.qry_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.rpl_prefix}\" for x in query[:1])\n",
    "\n",
    "    def repeat_input(self, x, no_aug=False):\n",
    "        if self.repeat_input_aug is None: return ''\n",
    "        return f\"{self.repeat_input_pre}{self.fmt_array(((lambda x: x) if no_aug else self.repeat_input_aug)(x['input']))}\"\n",
    "\n",
    "    def fmt_reply(self, reply, fault=None):\n",
    "        ids = self.fmt_array(reply[0]) + self.exa_end + self.tokenizer.eos_token\n",
    "        if self.out2_use:\n",
    "            if fault is None: fault = reply\n",
    "            ids = self.fmt_array(fault[0]) + self.exa_end + self.out2_token + ids\n",
    "        return ids\n",
    "\n",
    "    def quick_test(self, decoded, done):\n",
    "        sp = decoded.split(self.tokenizer.eos_token)[0].split(self.dec_sep)\n",
    "        sl = len(sp[0])\n",
    "        is_prefix = sl>0 and len(sp[-1])<=sl and (len(sp)==1 or len(sp[-2])==sl) and all(x.isdigit() for x in sp[-1])\n",
    "        return is_prefix and (not done or len(sp[-1])==0 or len(sp[-1])==sl)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_solution(guess):\n",
    "        return isinstance(guess, np.ndarray) and guess.ndim == 2 and all(0 < x <= 30 for x in guess.shape)\n",
    "\n",
    "    def max_new_tokens(self, safety_margin=1):\n",
    "        max_sized_reply = np.zeros([30, 30], dtype=int)\n",
    "        tokenized = self.tokenizer(self.fmt_reply([max_sized_reply]))['input_ids']\n",
    "        max_new_tokens = len(tokenized)\n",
    "        if tokenized[0]==self.tokenizer.bos_token_id: max_new_tokens -= 1\n",
    "        return max_new_tokens + safety_margin\n",
    "\n",
    "    def de_tokenize(self, tokens, scores=None):\n",
    "        import torch\n",
    "        tokens_cut = cut_at_token(tokens, self.tokenizer.eos_token_id)\n",
    "        de_tokenized = self.tokenizer.batch_decode([tokens_cut])[0]\n",
    "        score_val = None\n",
    "        if scores is not None:\n",
    "            tokens_with_eos = tokens[:len(tokens_cut)+1]\n",
    "            score_val = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1).numpy().copy()[np.arange(len(tokens_with_eos)), tokens_with_eos].sum()\n",
    "            number_token_ids = [self.tokenizer.vocab[k] for k in map(str, range(10))]\n",
    "            fault_token_id = self.collator_kwargs.get('fault_token_id')\n",
    "            if fault_token_id is not None: number_token_ids.append(fault_token_id)\n",
    "            number_token_ids = np.array(number_token_ids)\n",
    "            number_positions = (tokens_cut[..., np.newaxis] == number_token_ids).any(-1)\n",
    "            scores = scores[:len(tokens_cut), number_token_ids][number_positions]\n",
    "            scores = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1)[:, :10].numpy().copy()\n",
    "        return max(len(tokens)+1, len(tokens_cut)), score_val, de_tokenized, scores\n",
    "\n",
    "    def decode_to_array_single(self, text, score=None, limit_rows=30):\n",
    "        try:\n",
    "            by_rows = [row for row in [[int(x) for x in line if x.isdigit()] for line in text.split(self.dec_sep)] if len(row)]\n",
    "            if limit_rows and len(by_rows) > limit_rows:\n",
    "                by_rows = by_rows[:limit_rows]\n",
    "                limited = True\n",
    "            else: limited = False\n",
    "            decoded = np.array(by_rows, dtype=int)\n",
    "            if self.is_valid_solution(decoded):\n",
    "                try:\n",
    "                    assert score is not None\n",
    "                    decoded_flat = decoded.ravel()\n",
    "                    if limited: score = score[:len(decoded_flat)]\n",
    "                    score_all = score.reshape(decoded.shape + score.shape[1:])\n",
    "                    score_result = score[range(len(decoded_flat)), decoded_flat]\n",
    "                    score_reshaped = score_result.reshape(decoded.shape)\n",
    "                    score_cum_reshaped = score_result.cumsum().reshape(score_reshaped.shape)\n",
    "                    score_all_cum = score_cum_reshaped[..., np.newaxis] - score_reshaped[..., np.newaxis] + score_all\n",
    "                except: score_reshaped = score_cum_reshaped = np.full(decoded.shape, -float('inf'))\n",
    "                return {'output': decoded, 'score': score_reshaped, 'score_cum': score_cum_reshaped, 'score_all': score_all, 'score_all_cum': score_all_cum}\n",
    "        except: pass\n",
    "        return {}\n",
    "\n",
    "    def decode_to_array(self, text, score=None, limit_rows=30):\n",
    "        if not self.out2_use: text, score = [text], [score]\n",
    "        else:\n",
    "            text = text.split(self.out2_token)\n",
    "            if score is None: score = [None]*len(text)\n",
    "            else:\n",
    "                lengths = np.cumsum([len(list(filter(str.isdigit, t))) for t in text])\n",
    "                score = [score[s:e] for s, e in zip([0]+lengths[:-1].tolist(), lengths)]\n",
    "        return [self.decode_to_array_single(t, s) for t, s in zip(text, score)]\n",
    "\n",
    "    def get_corpus(self):\n",
    "        try:\n",
    "            old_min_wid, self.min_wid = self.min_wid, min(self.min_wid, 2)\n",
    "            return self.fmt_train([{'input': [[i] for i in range(10)], 'output': [[i] for i in range(10)]}]*3, last_is_challenge=True, pretext_split=True)\n",
    "        finally: self.min_wid = old_min_wid\n",
    "\n",
    "    def get_data_collator(self):\n",
    "        if not self.masking: return None\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        collator_params = dict(tokenizer=self.tokenizer, mlm=False)\n",
    "        pass_out2_token = self.tokenizer.vocab[self.out2_token] if self.out2_use and self.masking==1 else None\n",
    "        if self.masking:\n",
    "            assert not self.collator_kwargs.get('mask_first_output') or self.masking==1\n",
    "            data_collator = get_class_MyDataCollator()(\n",
    "                **collator_params,\n",
    "                instruction_template=[self.inp_prefix, self.tokenizer.bos_token][self.masking - 1],\n",
    "                response_template=[self.out_prefix, (self.out2_token if self.out2_use else self.rpl_sep)][self.masking - 1],\n",
    "            ).setup(out2_token_id=pass_out2_token, **self.collator_kwargs)\n",
    "        else:\n",
    "            assert not self.collator_kwargs, 'only supported with masking on'\n",
    "            data_collator = DataCollatorForLanguageModeling(**collator_params)\n",
    "        return data_collator\n",
    "\n",
    "    def get_output_token_ids(self):\n",
    "        assert not self.out2_use\n",
    "        num_tokens = [self.tokenizer.vocab[str(i)] for i in range(10)]\n",
    "        sep_tokens = [tok for txt in [self.arr_beg, self.arr_sep, self.arr_end, self.exa_sep] if txt for tok in self.tokenizer(txt)['input_ids'][1:]]\n",
    "        sep_tokens.append(self.tokenizer.eos_token_id)\n",
    "        return num_tokens + sorted(set(sep_tokens))\n",
    "\n",
    "ArcFormatter_pretext2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pretext_corpus_split='\\n', **kwargs)\n",
    "ArcFormatter_pretext3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pretext_corpus_split='\\n', **kwargs)\n",
    "ArcFormatter_premix_2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\n",
    "ArcFormatter_premix_3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\n",
    "\n",
    "available_formatters = dict(\n",
    "    ArcFormatter_pretext2=ArcFormatter_pretext2,\n",
    "    ArcFormatter_pretext3=ArcFormatter_pretext3,\n",
    "    ArcFormatter_premix_2=ArcFormatter_premix_2,\n",
    "    ArcFormatter_premix_3=ArcFormatter_premix_3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c9b2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.432514Z",
     "iopub.status.busy": "2025-11-13T05:06:15.432268Z",
     "iopub.status.idle": "2025-11-13T05:06:15.437024Z",
     "shell.execute_reply": "2025-11-13T05:06:15.436428Z"
    },
    "papermill": {
     "duration": 0.01312,
     "end_time": "2025-11-13T05:06:15.438428",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.425308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing selection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile selection.py\n",
    "import numpy as np\n",
    "\n",
    "def hashable(guess):\n",
    "    return tuple(map(tuple, guess))\n",
    "\n",
    "def make_unique(guess_list, indices=None):\n",
    "    used = set()\n",
    "    out = []\n",
    "    out_ind = []\n",
    "    for i, g in enumerate(guess_list):\n",
    "        h = hashable(g)\n",
    "        if h not in used:\n",
    "            used.add(h)\n",
    "            out.append(np.array(g))\n",
    "            if indices is not None: out_ind.append(indices[i])\n",
    "    return out if indices is None else (out, out_ind)\n",
    "\n",
    "def first_only(guesses):\n",
    "    return [g['output'] for g in guesses.values()][:1]\n",
    "\n",
    "def keep_order(guesses):\n",
    "    return [g['output'] for g in guesses.values()]\n",
    "\n",
    "def keep_order_unique(guesses):\n",
    "    return make_unique(keep_order(guesses))\n",
    "\n",
    "def get_best_shape_by_score(guess_list, getter, once_per_result=True):\n",
    "    seen_outputs = set()\n",
    "    shape_scores = {}\n",
    "    for i, g in enumerate(guess_list):\n",
    "        shape = tuple(g['output'].shape)\n",
    "        scores = shape_scores[shape] = shape_scores.get(shape, [[], []])\n",
    "        scores[1].append(i)\n",
    "        h = hashable(g['output'])\n",
    "        if h in seen_outputs: continue\n",
    "        if once_per_result: seen_outputs.add(h)\n",
    "        scores[0].append(g)\n",
    "    shape_scores = [(getter(scores), shape, indices) for shape, (scores, indices) in shape_scores.items()]\n",
    "    shape_scores = sorted(shape_scores, key=(lambda x: x[0]), reverse=True)\n",
    "    return shape_scores[0]\n",
    "\n",
    "def score_sum(guesses, getter, shape_getter=None, prefer_common_shape=True):\n",
    "    if shape_getter is None: shape_getter = getter\n",
    "    guess_list = list(guesses.values())\n",
    "    common_shape_indices = set(get_best_shape_by_score(guess_list, shape_getter)[2]) if prefer_common_shape else []\n",
    "    scores = {}\n",
    "    for i, g in enumerate(guess_list):\n",
    "        h = hashable(g['output'])\n",
    "        x = scores[h] = scores.get(h, [i in common_shape_indices, [], g['output']])\n",
    "        x[1].append(g)\n",
    "    scores = [(cs, getter(sc), o) for cs, sc, o in scores.values()]\n",
    "    scores = sorted(scores, key=(lambda x: x[:2]), reverse=True)\n",
    "    ordered_outputs = [x[-1] for x in scores]\n",
    "    return ordered_outputs\n",
    "\n",
    "getter_all_probsum = lambda guesses: sum(np.exp(g['score_val']) for g in guesses)\n",
    "def score_all_probsum(guesses): return score_sum(guesses, getter_all_probsum)\n",
    "\n",
    "def getter_full_probmul(p):\n",
    "    def _getter(guesses, baseline=p):\n",
    "        inf_score = sum([g['score_val']+baseline for g in guesses])\n",
    "        aug_score = np.mean([sum(s+baseline for s in g['score_multi_nl']) for g in guesses])\n",
    "        return inf_score + aug_score\n",
    "    return _getter\n",
    "\n",
    "def score_full_probmul_3(guesses): return score_sum(guesses, getter_full_probmul(3), prefer_common_shape=False)\n",
    "\n",
    "selection_algorithms = [\n",
    "    first_only,\n",
    "    keep_order,\n",
    "    keep_order_unique,\n",
    "    score_all_probsum,\n",
    "    score_full_probmul_3,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363dd77f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.451908Z",
     "iopub.status.busy": "2025-11-13T05:06:15.451517Z",
     "iopub.status.idle": "2025-11-13T05:06:15.455540Z",
     "shell.execute_reply": "2025-11-13T05:06:15.454994Z"
    },
    "papermill": {
     "duration": 0.012277,
     "end_time": "2025-11-13T05:06:15.457003",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.444726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing async_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile async_tools.py\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "async def stream_reader(stream, id, to):\n",
    "    id = '' if id is None else f'{id}. '\n",
    "    data = b''\n",
    "    while True:\n",
    "        read = await stream.read(n=4096)\n",
    "        if not read: break\n",
    "        if to is not None:\n",
    "            *complete_lines, data = (data + read + b'X').splitlines()\n",
    "            data = data[:-1]\n",
    "            for line in complete_lines:\n",
    "                line = line.rstrip()\n",
    "                if line: print(f\"{id}{line.decode('utf-8')}\", file=to, end='\\n', flush=True)\n",
    "\n",
    "async def wait_for_subprocess(subprocess, print_output=False, id=None):\n",
    "    await asyncio.gather(\n",
    "            stream_reader(subprocess.stdout, id, (sys.stdout if print_output else None)),\n",
    "            stream_reader(subprocess.stderr, id, (sys.stderr if print_output else None)),\n",
    "        )\n",
    "    return await subprocess.wait()\n",
    "\n",
    "async def wait_for_subprocesses(*processes, print_output=False):\n",
    "    return await asyncio.gather(*[wait_for_subprocess(p, print_output=print_output, id=i if len(processes)>1 else None) for i, p in enumerate(processes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6546860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.470637Z",
     "iopub.status.busy": "2025-11-13T05:06:15.470409Z",
     "iopub.status.idle": "2025-11-13T05:06:15.478095Z",
     "shell.execute_reply": "2025-11-13T05:06:15.477535Z"
    },
    "papermill": {
     "duration": 0.016256,
     "end_time": "2025-11-13T05:06:15.479545",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.463289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing common_stuff.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile common_stuff.py\n",
    "# common configuration for training and evaluation\n",
    "from arc_loader import *\n",
    "from model_runner import *\n",
    "from selection import *\n",
    "from async_tools import *\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "\n",
    "def set_all_seeds(seed=GLOBAL_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "set_all_seeds()\n",
    "\n",
    "# paths\n",
    "tmp_dir = '/kaggle/temp'\n",
    "arc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'\n",
    "arc_solutions_file = '/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json'\n",
    "model_temp_storage = os.path.join(tmp_dir, 'finetuned_model')\n",
    "infer_temp_storage = os.path.join(tmp_dir, 'inference_outputs')\n",
    "score_temp_storage = os.path.join(tmp_dir, 'inference_scoring')\n",
    "\n",
    "# load datasets\n",
    "arc_test_set = ArcDataset.from_file(arc_challenge_file)\n",
    "# if arc_test_set.is_fake: arc_test_set.load_replies(arc_solutions_file)\n",
    "arc_test_set.is_fake = False  # force full run\n",
    "# arc_train_set = ArcDataset.from_file('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "\n",
    "# models\n",
    "MyFormatter, perm_aug, max_seq_length_train, mask_first = ArcFormatter_premix_3, 'rnd_all', 4224, 1\n",
    "\n",
    "# training & inference\n",
    "train_epochs = 4\n",
    "multi_gpu_train = True\n",
    "multi_gpu_random_split = False\n",
    "max_seq_length_infer = 8192\n",
    "prime_on_single_task = True\n",
    "num_active_layers = 32\n",
    "infer_params = dict(min_prob=0.5, store=infer_temp_storage, use_turbo=True)\n",
    "\n",
    "# scoring\n",
    "use_aug_score = True\n",
    "aug_score_params = dict(tp=True, rot=True, perm=perm_aug, shfl_ex=True, make_unique=True, max_len=max_seq_length_infer)\n",
    "submission_select_algo = score_full_probmul_3 if use_aug_score else score_all_probsum\n",
    "\n",
    "\n",
    "def prepare_run(model_path, load_lora=None, train=False, gpu=None, **kwargs):\n",
    "    seed = GLOBAL_SEED + (0 if gpu is None else gpu)\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    if gpu is not None:\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "    model_kwargs = dict(max_seq_length=max_seq_length_train)\n",
    "    model_kwargs.update(kwargs)\n",
    "\n",
    "    model, tokenizer, formatter = prepare_model(\n",
    "        model=model_path,\n",
    "        local_files_only=True,\n",
    "        mode='unsloth_4bit',\n",
    "        formatter=MyFormatter,\n",
    "        peft=([dict(\n",
    "            r=32,\n",
    "            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'embed_tokens', 'lm_head'],\n",
    "            lora_alpha=128,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=True,\n",
    "            random_state=42,\n",
    "            use_rslora=True,\n",
    "            loftq_config=None,\n",
    "        )] if train or load_lora else []) + ([load_lora] if load_lora else []),\n",
    "        num_active_layers=(num_active_layers if train else None),\n",
    "        **model_kwargs\n",
    "    )\n",
    "\n",
    "    if train and mask_first: formatter.collator_kwargs.update(mask_first_output=mask_first)\n",
    "\n",
    "    return model, formatter\n",
    "\n",
    "\n",
    "def prepare_dataset(formatter, train, gpu=None):\n",
    "    seed = GLOBAL_SEED + (0 if gpu is None else gpu)\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    ds = arc_test_set\n",
    "    if multi_gpu_train and gpu is not None:\n",
    "        if multi_gpu_random_split:\n",
    "            all_keys_shuffled = ds.shuffled(seed=123).keys\n",
    "            num_total_keys = len(all_keys_shuffled)\n",
    "            base_quarter_size = num_total_keys // 4\n",
    "            start_index = gpu * base_quarter_size\n",
    "            if gpu < 3:\n",
    "                end_index = (gpu + 1) * base_quarter_size\n",
    "            else:\n",
    "                end_index = num_total_keys\n",
    "            gpu_specific_keys = all_keys_shuffled[start_index:end_index]\n",
    "            ds = ds.change_keys(gpu_specific_keys, keep_flags=True)\n",
    "        else:\n",
    "            ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n",
    "            # 4-GPU rotation pattern instead of 2-GPU\n",
    "            assignment = ([0, 1, 2, 3] * ds.length())[:ds.length()][::-1]\n",
    "            ds = ds.change_keys((np.array(ds.keys)[np.array(assignment) == gpu]).tolist())\n",
    "\n",
    "    if arc_test_set.is_fake: ds.keys = ds.keys[:1]\n",
    "    # Rest of the function remains the same\n",
    "    if train:\n",
    "        ds = ds.remove_replies()\n",
    "        ds = ds.augment(tp=True, rot=True, perm=perm_aug, n=(2 if arc_test_set.is_fake else train_epochs), shfl_ex=True, shfl_keys=True)\n",
    "        ds = ds.cut_to_len(formatter=formatter, name='text', max_len=max_seq_length_train, max_new_tokens=0, quiet=True)\n",
    "        if arc_test_set.is_fake: ds = ds.sorted_by_len(formatter=formatter, name='text', reverse=True)\n",
    "        print(len(ds.keys))\n",
    "    else:\n",
    "        ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n",
    "        ds = ds.split_multi_replies()\n",
    "        ds = ds.augment(tp=True, rot=True, n=2, seed=42, perm=perm_aug, shfl_ex=True).interleave(ds.length())\n",
    "        ds = ds.cut_to_len(formatter=formatter, name='input', max_len=max_seq_length_infer, quiet=True)\n",
    "        print(len(ds.keys))\n",
    "        grouped_keys = {}\n",
    "        for key in ds.keys:\n",
    "            base_key = key.split('.')[0]\n",
    "            if base_key not in grouped_keys:\n",
    "                grouped_keys[base_key] = []\n",
    "            grouped_keys[base_key].append(key)\n",
    "        final_keys = []\n",
    "        for base_key in sorted(grouped_keys.keys()):\n",
    "            group = grouped_keys[base_key]\n",
    "            permuted_group = np.random.permutation(group).tolist()\n",
    "            final_keys.extend(permuted_group[:5])\n",
    "        ds = ds.change_keys(final_keys)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def start_training(gpu):\n",
    "    seed = GLOBAL_SEED + gpu\n",
    "    set_all_seeds(seed)\n",
    "    base_model = '/kaggle/input/mistral-hybrid/transformers/default/1/namannn/mistral-hybrid'\n",
    "\n",
    "    try:\n",
    "        storage_path = f'{model_temp_storage}_gpu{gpu}'\n",
    "        if gpu == 0 or multi_gpu_train:\n",
    "            with RemapCudaOOM():\n",
    "                model, formatter = prepare_run(base_model, train=True, gpu=gpu)\n",
    "                dataset = prepare_dataset(formatter, train=True, gpu=gpu if multi_gpu_train else None)\n",
    "                model, trainer_stats = training_run(\n",
    "                    model, formatter, dataset, store=storage_path,\n",
    "                    max_seq_length=max_seq_length_train,\n",
    "                    grad_acc_fix=False,\n",
    "                    train_args=dict(\n",
    "                        per_device_train_batch_size=8,\n",
    "                        gradient_accumulation_steps=1,\n",
    "                        warmup_steps=48,\n",
    "                        num_train_epochs=1,\n",
    "                        max_steps=5 if arc_test_set.is_fake else 240,\n",
    "                        learning_rate=1e-4,\n",
    "                        embedding_learning_rate=1e-5,\n",
    "                        logging_steps=10,\n",
    "                        optim=\"adamw_8bit\",\n",
    "                        weight_decay=0.01,\n",
    "                        lr_scheduler_type='cosine',  # \"linear\", \"cosine\",\n",
    "                        seed=42,\n",
    "                        output_dir=os.path.join(tmp_dir, 'checkpoints'),\n",
    "                        save_strategy=\"no\",\n",
    "                        report_to='none',\n",
    "                    ),\n",
    "                )\n",
    "                mem_info()\n",
    "    finally:\n",
    "        os.makedirs(f'{storage_path}_done', exist_ok=True)\n",
    "\n",
    "\n",
    "def start_inference(gpu):\n",
    "    seed = GLOBAL_SEED + gpu + 100\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    storage_path = f'{model_temp_storage}_gpu{gpu if multi_gpu_train else 0}'\n",
    "    while not os.path.exists(f'{storage_path}_done'): time.sleep(15)\n",
    "    with RemapCudaOOM():\n",
    "        model, formatter = prepare_run(storage_path, gpu=gpu)\n",
    "        dataset = prepare_dataset(formatter, train=False, gpu=gpu)\n",
    "        retrainer = None if not prime_on_single_task else Retrainer(\n",
    "            n=128,\n",
    "            aug_opts=dict(tp=True, rot=True, perm=perm_aug, shfl_ex=True),\n",
    "            reload_state_dict=get_and_fix_peft_weights(storage_path),\n",
    "            formatter=formatter,\n",
    "            max_seq_length=max_seq_length_infer,\n",
    "            grad_acc_fix=False,\n",
    "            train_args=dict(\n",
    "                per_device_train_batch_size=8,\n",
    "                gradient_accumulation_steps=1,\n",
    "                warmup_steps=4,\n",
    "                num_train_epochs=1,\n",
    "                learning_rate=5e-5,\n",
    "                embedding_learning_rate=0,\n",
    "                logging_steps=8,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.01,\n",
    "                lr_scheduler_type='constant',  # \"linear\", \"cosine\",\n",
    "                seed=42,\n",
    "                output_dir='tmp_output',\n",
    "                save_strategy='no',\n",
    "                report_to='none',\n",
    "            ),\n",
    "        )\n",
    "        decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, prob_baseline=0.05)\n",
    "        inference_run_v2(model, formatter, dataset, decoder, retrain=retrainer, **infer_params)\n",
    "        if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
    "        mem_info()\n",
    "\n",
    "\n",
    "class RemapCudaOOM:\n",
    "    def __enter__(self): pass\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        oom_errors = [\"CUDA out of memory\", \"Make sure you have enough GPU RAM\", \"does not fit any GPU's remaining memory\"]\n",
    "        if exc_value and any(x in str(exc_value) for x in oom_errors):\n",
    "            with open('submission.json', 'w') as f: f.write('cause submission scoring error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7b986e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:06:15.493584Z",
     "iopub.status.busy": "2025-11-13T05:06:15.493222Z",
     "iopub.status.idle": "2025-11-13T05:08:05.699050Z",
     "shell.execute_reply": "2025-11-13T05:08:05.698201Z"
    },
    "papermill": {
     "duration": 110.21455,
     "end_time": "2025-11-13T05:08:05.700738",
     "exception": false,
     "start_time": "2025-11-13T05:06:15.486188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcommon_stuff\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_DISABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/capstone video/git/inference/common_stuff.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# common configuration for training and evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01marc_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel_runner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/capstone video/git/inference/arc_loader.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from common_stuff import *\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "if not os.path.exists(os.path.join(tmp_dir, 'unsloth_installed')):  # unsloth offline install - https://stackoverflow.com/a/51646354\n",
    "    !pip uninstall --yes torch accelerate\n",
    "    !pip install --no-index --find-links=/kaggle/input/unsloth-2024-9-post4/wheelhouse unsloth\n",
    "    #!pip uninstall --yes accelerate fastai torch torchaudio transformers\n",
    "    #!pip install --no-index --find-links=/kaggle/input/unsloth-2024-10-7/wheelhouse unsloth  # do not use grad_acc_fix - trains very slow\n",
    "    #!sed -i 's/if ((post_check - pre_check) >= 1).sum() > 1:/if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\n",
    "    # fix delay bug in get_statistics()\n",
    "    !sed -i 's/^def get_statistics():/def get_statistics():\\n if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py\n",
    "    # fix faulty unsloth multi-gpu detection\n",
    "    !sed -i \"s/raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')/pass/g\" /opt/conda/lib/python3.10/site-packages/unsloth/tokenizer_utils.py /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py /opt/conda/lib/python3.10/site-packages/unsloth/models/vision.py\n",
    "    os.makedirs(os.path.join(tmp_dir, 'unsloth_installed'), exist_ok=True)\n",
    "    print('Unsloth installed & patched.')\n",
    "\n",
    "for gpu in [0, 1]: \n",
    "    signal_path = f'{model_temp_storage}_gpu{gpu}_done'\n",
    "    if os.path.exists(signal_path): os.rmdir(signal_path)\n",
    "\n",
    "if arc_test_set.is_fake:  # cleanup? (for debugging)\n",
    "    #!rm -R /kaggle/temp/finetuned_model*\n",
    "    #!rm -R /kaggle/temp/inference_outputs\n",
    "    #!rm -R /kaggle/temp/inference_scoring\n",
    "    #!ls /kaggle/temp\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a85d59f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:05.720009Z",
     "iopub.status.busy": "2025-11-13T05:08:05.719076Z",
     "iopub.status.idle": "2025-11-13T05:08:11.044305Z",
     "shell.execute_reply": "2025-11-13T05:08:11.043662Z"
    },
    "papermill": {
     "duration": 5.336407,
     "end_time": "2025-11-13T05:08:11.045886",
     "exception": false,
     "start_time": "2025-11-13T05:08:05.709479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Simplified ARC data visualization script (English version)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01marc_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colors\n",
      "File \u001b[0;32m~/Documents/capstone video/git/inference/arc_loader.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# Simplified ARC data visualization script (English version)\n",
    "from arc_loader import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create ARC color map\n",
    "cmap = colors.ListedColormap(\n",
    "    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "norm = colors.Normalize(vmin=0, vmax=9)\n",
    "\n",
    "# Load data directly from file\n",
    "arc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n",
    "\n",
    "# Load original data\n",
    "with open(arc_challenge_file, 'r') as f:\n",
    "    arc_data = json.load(f)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def visualize_arc_example(train_data, test_data, task_id):\n",
    "    \"\"\"Visualize training and test data for an ARC task\"\"\"\n",
    "    # Get number of training and test examples\n",
    "    n_train = len(train_data)\n",
    "    n_test = len(test_data)\n",
    "    \n",
    "    # Create figure large enough for all examples\n",
    "    fig, axes = plt.subplots(2, max(n_train, n_test), figsize=(4*max(n_train, n_test), 8))\n",
    "    fig.suptitle(f\"Task ID: {task_id}\", fontsize=16)\n",
    "    \n",
    "    # Visualize training data\n",
    "    for i in range(n_train):\n",
    "        # Input\n",
    "        axes[0, i].imshow(train_data[i]['input'], cmap=cmap, norm=norm)\n",
    "        axes[0, i].grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        axes[0, i].set_title(f\"Training #{i+1} - Input\")\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "        \n",
    "        # Output\n",
    "        axes[1, i].imshow(train_data[i]['output'], cmap=cmap, norm=norm)\n",
    "        axes[1, i].grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        axes[1, i].set_title(f\"Training #{i+1} - Output\")\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "    \n",
    "    # Handle test data visualization\n",
    "    for i in range(n_test):\n",
    "        if i < n_train:\n",
    "            # Already have training data in this column\n",
    "            pass\n",
    "        else:\n",
    "            # Hide unused training cells\n",
    "            if i >= n_train:\n",
    "                axes[0, i].axis('off')\n",
    "                axes[1, i].axis('off')\n",
    "    \n",
    "    # Show first test input\n",
    "    if n_test > 0:\n",
    "        # Create separate figure for test input\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(test_data[0]['input'], cmap=cmap, norm=norm)\n",
    "        plt.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        plt.title(f\"Test Input - {task_id}\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Simulate 4 GPU data splitting\n",
    "task_ids = list(arc_data.keys())\n",
    "random.shuffle(task_ids)  # Shuffle task order\n",
    "\n",
    "# Assign tasks to each GPU\n",
    "gpu_tasks = {}\n",
    "for gpu_id in range(4):\n",
    "    # Simple equal division - each GPU gets 1/4 of tasks\n",
    "    start_idx = gpu_id * len(task_ids) // 4\n",
    "    end_idx = (gpu_id + 1) * len(task_ids) // 4\n",
    "    gpu_tasks[gpu_id] = task_ids[start_idx:end_idx]\n",
    "\n",
    "# Display training data samples for each GPU\n",
    "for gpu_id in range(4):\n",
    "    assigned_tasks = gpu_tasks[gpu_id]\n",
    "    print(f\"\\n{'='*40}\\nGPU {gpu_id} Training Data Samples\\n{'='*40}\")\n",
    "    print(f\"GPU {gpu_id} assigned {len(assigned_tasks)} training tasks\")\n",
    "    \n",
    "    # Show only first 3 examples\n",
    "    samples = assigned_tasks[:3]\n",
    "    \n",
    "    for task_id in samples:\n",
    "        print(f\"\\nTask: {task_id}\")\n",
    "        \n",
    "        # Get training and test data for this task\n",
    "        train_data = arc_data[task_id]['train']\n",
    "        test_data = arc_data[task_id]['test']\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_arc_example(train_data, test_data, task_id)\n",
    "        \n",
    "        # Print data matrices\n",
    "        print(\"Training Input (first example):\")\n",
    "        print(np.array(train_data[0]['input']))\n",
    "        print(\"\\nTraining Output (first example):\")\n",
    "        print(np.array(train_data[0]['output']))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ec4c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.083158Z",
     "iopub.status.busy": "2025-11-13T05:08:11.082862Z",
     "iopub.status.idle": "2025-11-13T05:08:11.107147Z",
     "shell.execute_reply": "2025-11-13T05:08:11.106532Z"
    },
    "papermill": {
     "duration": 0.044781,
     "end_time": "2025-11-13T05:08:11.108660",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.063879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc0\n",
    "from common_stuff import *\n",
    "start_training(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c23f890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.144727Z",
     "iopub.status.busy": "2025-11-13T05:08:11.144071Z",
     "iopub.status.idle": "2025-11-13T05:08:11.149509Z",
     "shell.execute_reply": "2025-11-13T05:08:11.148928Z"
    },
    "papermill": {
     "duration": 0.024844,
     "end_time": "2025-11-13T05:08:11.150906",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.126062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc1\n",
    "from common_stuff import *\n",
    "start_training(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26c5647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.186681Z",
     "iopub.status.busy": "2025-11-13T05:08:11.186401Z",
     "iopub.status.idle": "2025-11-13T05:08:11.191741Z",
     "shell.execute_reply": "2025-11-13T05:08:11.191153Z"
    },
    "papermill": {
     "duration": 0.024969,
     "end_time": "2025-11-13T05:08:11.193237",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.168268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc2\n",
    "from common_stuff import *\n",
    "start_training(gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e201eea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.275495Z",
     "iopub.status.busy": "2025-11-13T05:08:11.274354Z",
     "iopub.status.idle": "2025-11-13T05:08:11.290104Z",
     "shell.execute_reply": "2025-11-13T05:08:11.288181Z"
    },
    "papermill": {
     "duration": 0.064647,
     "end_time": "2025-11-13T05:08:11.294346",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.229699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc3\n",
    "from common_stuff import *\n",
    "start_training(gpu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "121bea0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.419260Z",
     "iopub.status.busy": "2025-11-13T05:08:11.418939Z",
     "iopub.status.idle": "2025-11-13T05:08:11.425488Z",
     "shell.execute_reply": "2025-11-13T05:08:11.424839Z"
    },
    "papermill": {
     "duration": 0.054955,
     "end_time": "2025-11-13T05:08:11.427139",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.372184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc0\n",
    "from common_stuff import *\n",
    "start_inference(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10d7c051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.486186Z",
     "iopub.status.busy": "2025-11-13T05:08:11.485594Z",
     "iopub.status.idle": "2025-11-13T05:08:11.500494Z",
     "shell.execute_reply": "2025-11-13T05:08:11.497239Z"
    },
    "papermill": {
     "duration": 0.060625,
     "end_time": "2025-11-13T05:08:11.505609",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.444984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc1\n",
    "from common_stuff import *\n",
    "start_inference(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1264acb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.578947Z",
     "iopub.status.busy": "2025-11-13T05:08:11.578114Z",
     "iopub.status.idle": "2025-11-13T05:08:11.586145Z",
     "shell.execute_reply": "2025-11-13T05:08:11.585035Z"
    },
    "papermill": {
     "duration": 0.043006,
     "end_time": "2025-11-13T05:08:11.589060",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.546054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc2\n",
    "from common_stuff import *\n",
    "start_inference(gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a936e446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.678879Z",
     "iopub.status.busy": "2025-11-13T05:08:11.678292Z",
     "iopub.status.idle": "2025-11-13T05:08:11.706038Z",
     "shell.execute_reply": "2025-11-13T05:08:11.703702Z"
    },
    "papermill": {
     "duration": 0.078293,
     "end_time": "2025-11-13T05:08:11.710519",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.632226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc3\n",
    "from common_stuff import *\n",
    "start_inference(gpu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df2cced6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:08:11.809367Z",
     "iopub.status.busy": "2025-11-13T05:08:11.808751Z",
     "iopub.status.idle": "2025-11-13T11:57:08.267013Z",
     "shell.execute_reply": "2025-11-13T11:57:08.266280Z"
    },
    "papermill": {
     "duration": 24536.480203,
     "end_time": "2025-11-13T11:57:08.268573",
     "exception": false,
     "start_time": "2025-11-13T05:08:11.788370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "2. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "1. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "0. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "2. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "3. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "1. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "0. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "2. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "2. *** Load base model and tokenizer from '/kaggle/input/mistral-hybrid/transformers/default/1/namannn/mistral-hybrid'...\n",
      "2. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "2.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "2. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "2. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "2.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "2. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "1. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "1. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "1. *** Load base model and tokenizer from '/kaggle/input/mistral-hybrid/transformers/default/1/namannn/mistral-hybrid'...\n",
      "1. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "1. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "1.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "1. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "1. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "1.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "1. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "0. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "0. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "0. *** Load base model and tokenizer from '/kaggle/input/mistral-hybrid/transformers/default/1/namannn/mistral-hybrid'...\n",
      "0. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "0. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "0.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "0. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "0. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "0.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "0. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "3. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "3. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "3. *** Load base model and tokenizer from '/kaggle/input/mistral-hybrid/transformers/default/1/namannn/mistral-hybrid'...\n",
      "3. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "3. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "3.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "3. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "3. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "3.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "3. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "2. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "3. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "0. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "1. Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.15s/it]\n",
      "2. Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.93s/it]\n",
      "0. Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.74s/it]\n",
      "3. Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.70s/it]\n",
      "1. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.95s/it]\n",
      "1. Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.70s/it]\n",
      "1. Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.79s/it]\n",
      "2. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.28s/it]\n",
      "2. Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  8.86s/it]\n",
      "2. Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.04s/it]\n",
      "0. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.18s/it]\n",
      "3. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.18s/it]\n",
      "3. Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.80s/it]\n",
      "3. Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.95s/it]\n",
      "0. Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.80s/it]\n",
      "0. Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.95s/it]\n",
      "1. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n",
      "2. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n",
      "0. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n",
      "3. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. *** Create new peft model...\n",
      "1. Unsloth: Casting embed_tokens to float32\n",
      "1. Unsloth: Casting lm_head to float32\n",
      "1. *** Activating only the first 32 layers and freezing the rest...\n",
      "1. *** -> Layers from 32 to 39 are frozen.\n",
      "1. *** -> Trainable params after freezing: 75,907,072 (2.02%)\n",
      "1. *** Augment dataset...\n",
      "1. 960\n",
      "3. *** Create new peft model...\n",
      "3. Unsloth: Casting embed_tokens to float32\n",
      "3. Unsloth: Casting lm_head to float32\n",
      "3. *** Activating only the first 32 layers and freezing the rest...\n",
      "3. *** -> Layers from 32 to 39 are frozen.\n",
      "3. *** -> Trainable params after freezing: 75,907,072 (2.02%)\n",
      "3. *** Augment dataset...\n",
      "3. 960\n",
      "2. *** Create new peft model...\n",
      "2. Unsloth: Casting embed_tokens to float32\n",
      "2. Unsloth: Casting lm_head to float32\n",
      "2. *** Activating only the first 32 layers and freezing the rest...\n",
      "2. *** -> Layers from 32 to 39 are frozen.\n",
      "2. *** -> Trainable params after freezing: 75,907,072 (2.02%)\n",
      "2. *** Augment dataset...\n",
      "2. 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. Map:   0%|          | 0/960 [00:00<?, ? examples/s]\n",
      "1. Map: 100%|██████████| 960/960 [00:01<00:00, 631.94 examples/s]\n",
      "1. Map: 100%|██████████| 960/960 [00:01<00:00, 626.91 examples/s]\n",
      "1. max_steps is given, it will override any value given in num_train_epochs\n",
      "3. Map:   0%|          | 0/960 [00:00<?, ? examples/s]\n",
      "3. Map: 100%|██████████| 960/960 [00:01<00:00, 833.53 examples/s]\n",
      "3. Map: 100%|██████████| 960/960 [00:01<00:00, 826.77 examples/s]\n",
      "2. Map:   0%|          | 0/960 [00:00<?, ? examples/s]\n",
      "3. max_steps is given, it will override any value given in num_train_epochs\n",
      "2. Map: 100%|██████████| 960/960 [00:01<00:00, 826.56 examples/s]\n",
      "2. Map: 100%|██████████| 960/960 [00:01<00:00, 819.92 examples/s]\n",
      "2. max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. *** Create new peft model...\n",
      "0. Unsloth: Casting embed_tokens to float32\n",
      "0. Unsloth: Casting lm_head to float32\n",
      "0. *** Activating only the first 32 layers and freezing the rest...\n",
      "0. *** -> Layers from 32 to 39 are frozen.\n",
      "0. *** -> Trainable params after freezing: 75,907,072 (2.02%)\n",
      "0. *** Augment dataset...\n",
      "0. 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0. Map:   0%|          | 0/960 [00:00<?, ? examples/s]\n",
      "0. Map: 100%|██████████| 960/960 [00:01<00:00, 860.89 examples/s]\n",
      "0. Map: 100%|██████████| 960/960 [00:01<00:00, 853.29 examples/s]\n",
      "0. max_steps is given, it will override any value given in num_train_epochs\n",
      "1. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "1.    \\\\   /|    Num examples = 960 | Num Epochs = 2\n",
      "1. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "1. \\        /    Total batch size = 8 | Total steps = 240\n",
      "1.  \"-____-\"     Number of trainable parameters = 75,907,072\n",
      "3. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "3.    \\\\   /|    Num examples = 960 | Num Epochs = 2\n",
      "3. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "3. \\        /    Total batch size = 8 | Total steps = 240\n",
      "3.  \"-____-\"     Number of trainable parameters = 75,907,072\n",
      "2. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "2.    \\\\   /|    Num examples = 960 | Num Epochs = 2\n",
      "2. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "2. \\        /    Total batch size = 8 | Total steps = 240\n",
      "2.  \"-____-\"     Number of trainable parameters = 75,907,072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. *** Start training run...\n",
      "1. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "1. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "0.    \\\\   /|    Num examples = 960 | Num Epochs = 2\n",
      "0. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "0. \\        /    Total batch size = 8 | Total steps = 240\n",
      "0.  \"-____-\"     Number of trainable parameters = 75,907,072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. *** Start training run...\n",
      "3. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "3. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for lm_head.\n",
      "2. *** Start training run...\n",
      "2. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "2. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for lm_head.\n",
      "0. *** Start training run...\n",
      "0. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for embed_tokens.\n",
      "0. Unsloth: Setting lr = 1.00e-05 instead of 1.00e-04 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.   0%|          | 0/240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "3.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "0.   0%|          | 0/240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "0.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "2.   0%|          | 0/240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "2.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "1.   0%|          | 0/240 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "1.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "3. /opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "3.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "0. /opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "0.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "2. /opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "2.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "3.   0%|          | 1/240 [00:37<2:27:43, 37.08s/it]\n",
      "0.   0%|          | 1/240 [00:38<2:31:46, 38.10s/it]\n",
      "2.   0%|          | 1/240 [00:38<2:32:49, 38.37s/it]\n",
      "2.   1%|          | 2/240 [01:07<2:11:25, 33.13s/it]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "2.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "2. /opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "2.   with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "1.   0%|          | 1/240 [00:39<2:37:45, 39.60s/it]\n",
      "3.   1%|          | 2/240 [00:56<1:46:17, 26.79s/it]\n",
      "0.   1%|          | 2/240 [01:06<2:07:51, 32.23s/it]\n",
      "1.   1%|          | 2/240 [01:20<2:39:52, 40.31s/it]\n",
      "3.   1%|▏         | 3/240 [01:33<2:03:40, 31.31s/it]\n",
      "2.   1%|▏         | 3/240 [01:42<2:13:23, 33.77s/it]\n",
      "0.   1%|▏         | 3/240 [01:38<2:08:15, 32.47s/it]\n",
      "3.   2%|▏         | 4/240 [02:07<2:07:04, 32.31s/it]\n",
      "1.   1%|▏         | 3/240 [02:03<2:43:46, 41.46s/it]\n",
      "0.   2%|▏         | 4/240 [02:16<2:15:12, 34.38s/it]\n",
      "2.   2%|▏         | 4/240 [02:16<2:13:53, 34.04s/it]\n",
      "3.   2%|▏         | 5/240 [02:25<1:46:31, 27.20s/it]\n",
      "2.   2%|▏         | 5/240 [02:54<2:18:31, 35.37s/it]\n",
      "0.   2%|▏         | 5/240 [02:53<2:18:16, 35.30s/it]\n",
      "3.   2%|▎         | 6/240 [02:54<1:48:50, 27.91s/it]\n",
      "1.   2%|▏         | 4/240 [02:45<2:43:40, 41.61s/it]\n",
      "0.   2%|▎         | 6/240 [03:22<2:09:20, 33.17s/it]\n",
      "2.   2%|▎         | 6/240 [03:17<2:00:57, 31.02s/it]\n",
      "1.   2%|▏         | 5/240 [03:27<2:44:13, 41.93s/it]\n",
      "3.   3%|▎         | 7/240 [03:24<1:51:19, 28.67s/it]\n",
      "0.   3%|▎         | 7/240 [03:46<1:57:00, 30.13s/it]\n",
      "2.   3%|▎         | 7/240 [03:54<2:08:59, 33.22s/it]\n",
      "3.   3%|▎         | 8/240 [03:57<1:56:19, 30.08s/it]\n",
      "1.   2%|▎         | 6/240 [03:56<2:25:43, 37.37s/it]\n",
      "0.   3%|▎         | 8/240 [04:23<2:04:54, 32.30s/it]\n",
      "2.   3%|▎         | 8/240 [04:25<2:05:07, 32.36s/it]\n",
      "1.   3%|▎         | 7/240 [04:38<2:31:15, 38.95s/it]\n",
      "3.   4%|▍         | 9/240 [04:34<2:04:02, 32.22s/it]\n",
      "3.   4%|▍         | 10/240 [05:07<2:04:29, 32.48s/it]\n",
      "2.   4%|▍         | 9/240 [04:57<2:04:14, 32.27s/it]\n",
      "2.   4%|▍         | 10/240 [05:29<2:03:03, 32.10s/it]\n",
      "0.   4%|▍         | 9/240 [04:56<2:05:08, 32.51s/it]\n",
      "0.   4%|▍         | 10/240 [05:28<2:04:53, 32.58s/it]\n",
      "1.   3%|▎         | 8/240 [05:01<2:11:15, 33.95s/it]\n",
      "3.   4%|▍         | 10/240 [05:07<2:04:29, 32.48s/it]\n",
      "2.   4%|▍         | 10/240 [05:29<2:03:03, 32.10s/it]\n",
      "1.   4%|▍         | 9/240 [05:36<2:12:10, 34.33s/it]\n",
      "1.   4%|▍         | 10/240 [05:58<1:56:57, 30.51s/it]\n",
      "0.   4%|▍         | 10/240 [05:28<2:04:53, 32.58s/it]\n",
      "3.   5%|▍         | 11/240 [05:37<2:00:56, 31.69s/it]\n",
      "2.   5%|▍         | 11/240 [05:50<1:49:28, 28.68s/it]\n",
      "1.   4%|▍         | 10/240 [05:58<1:56:57, 30.51s/it]\n",
      "0.   5%|▍         | 11/240 [06:06<2:10:21, 34.16s/it]\n",
      "3.   5%|▌         | 12/240 [06:12<2:03:54, 32.61s/it]\n",
      "2.   5%|▌         | 12/240 [06:20<1:51:02, 29.22s/it]\n",
      "1.   5%|▍         | 11/240 [06:31<1:58:56, 31.16s/it]\n",
      "0.   5%|▌         | 12/240 [06:43<2:13:06, 35.03s/it]\n",
      "3.   5%|▌         | 13/240 [06:49<2:08:27, 33.95s/it]\n",
      "2.   5%|▌         | 13/240 [06:50<1:50:55, 29.32s/it]\n",
      "1.   5%|▌         | 12/240 [07:00<1:56:02, 30.54s/it]\n",
      "0.   5%|▌         | 13/240 [07:11<2:04:07, 32.81s/it]\n",
      "3.   6%|▌         | 14/240 [07:26<2:11:11, 34.83s/it]\n",
      "2.   6%|▌         | 14/240 [07:28<2:00:21, 31.95s/it]\n",
      "1.   5%|▌         | 13/240 [07:35<2:00:38, 31.89s/it]\n",
      "0.   6%|▌         | 14/240 [07:48<2:08:07, 34.02s/it]\n",
      "3.   6%|▋         | 15/240 [07:56<2:04:42, 33.26s/it]\n",
      "2.   6%|▋         | 15/240 [07:58<1:58:27, 31.59s/it]\n",
      "0.   6%|▋         | 15/240 [08:20<2:06:08, 33.64s/it]\n",
      "3.   7%|▋         | 16/240 [08:25<1:59:41, 32.06s/it]\n",
      "1.   6%|▌         | 14/240 [08:17<2:11:52, 35.01s/it]\n",
      "2.   7%|▋         | 16/240 [08:36<2:04:11, 33.27s/it]\n",
      "3.   7%|▋         | 17/240 [08:55<1:56:49, 31.43s/it]\n",
      "0.   7%|▋         | 16/240 [08:53<2:04:20, 33.31s/it]\n",
      "1.   6%|▋         | 15/240 [08:59<2:19:32, 37.21s/it]\n",
      "2.   7%|▋         | 17/240 [09:10<2:04:48, 33.58s/it]\n",
      "3.   8%|▊         | 18/240 [09:23<1:52:54, 30.51s/it]\n",
      "0.   7%|▋         | 17/240 [09:26<2:03:16, 33.17s/it]\n",
      "1.   7%|▋         | 16/240 [09:34<2:16:17, 36.51s/it]\n",
      "2.   8%|▊         | 18/240 [09:48<2:08:45, 34.80s/it]\n",
      "3.   8%|▊         | 19/240 [09:49<1:47:22, 29.15s/it]\n",
      "3.   8%|▊         | 20/240 [10:19<1:47:40, 29.37s/it]\n",
      "0.   8%|▊         | 18/240 [09:59<2:02:24, 33.08s/it]\n",
      "1.   7%|▋         | 17/240 [10:03<2:07:00, 34.17s/it]\n",
      "2.   8%|▊         | 19/240 [10:18<2:03:28, 33.52s/it]\n",
      "2.   8%|▊         | 20/240 [10:56<2:07:17, 34.72s/it]\n",
      "3.   8%|▊         | 20/240 [10:19<1:47:40, 29.37s/it]\n",
      "0.   8%|▊         | 19/240 [10:26<1:55:09, 31.26s/it]\n",
      "0.   8%|▊         | 20/240 [11:03<2:00:57, 32.99s/it]\n",
      "1.   8%|▊         | 18/240 [10:38<2:07:15, 34.39s/it]\n",
      "0.   8%|▊         | 20/240 [11:03<2:00:57, 32.99s/it]\n",
      "3.   9%|▉         | 21/240 [10:56<1:55:39, 31.69s/it]\n",
      "2.   8%|▊         | 20/240 [10:56<2:07:17, 34.72s/it]\n",
      "1.   8%|▊         | 19/240 [11:21<2:15:42, 36.85s/it]\n",
      "1.   8%|▊         | 20/240 [12:00<2:18:16, 37.71s/it]\n",
      "0.   9%|▉         | 21/240 [11:29<1:53:01, 30.97s/it]\n",
      "3.   9%|▉         | 22/240 [11:33<2:00:40, 33.21s/it]\n",
      "2.   9%|▉         | 21/240 [11:33<2:09:58, 35.61s/it]\n",
      "0.   9%|▉         | 22/240 [12:06<1:58:59, 32.75s/it]\n",
      "3.  10%|▉         | 23/240 [12:10<2:04:07, 34.32s/it]\n",
      "1.   8%|▊         | 20/240 [12:00<2:18:16, 37.71s/it]\n",
      "2.   9%|▉         | 22/240 [12:11<2:11:36, 36.22s/it]\n",
      "3.  10%|█         | 24/240 [12:40<1:58:41, 32.97s/it]\n",
      "0.  10%|▉         | 23/240 [12:39<1:58:25, 32.75s/it]\n",
      "2.  10%|▉         | 23/240 [12:49<2:12:54, 36.75s/it]\n",
      "1.   9%|▉         | 21/240 [12:43<2:23:14, 39.25s/it]\n",
      "2.  10%|█         | 24/240 [13:18<2:04:33, 34.60s/it]\n",
      "3.  10%|█         | 25/240 [13:14<1:59:48, 33.43s/it]\n",
      "0.  10%|█         | 24/240 [13:16<2:02:56, 34.15s/it]\n",
      "1.   9%|▉         | 22/240 [13:26<2:26:04, 40.20s/it]\n",
      "3.  11%|█         | 26/240 [13:44<1:55:48, 32.47s/it]\n",
      "2.  10%|█         | 25/240 [13:39<1:48:52, 30.39s/it]\n",
      "0.  10%|█         | 25/240 [13:53<2:05:17, 34.97s/it]\n",
      "1.  10%|▉         | 23/240 [14:01<2:19:52, 38.68s/it]\n",
      "3.  11%|█▏        | 27/240 [14:14<1:51:58, 31.54s/it]\n",
      "2.  11%|█         | 26/240 [14:17<1:56:09, 32.57s/it]\n",
      "0.  11%|█         | 26/240 [14:22<1:58:53, 33.34s/it]\n",
      "1.  10%|█         | 24/240 [14:33<2:12:40, 36.85s/it]\n",
      "3.  12%|█▏        | 28/240 [14:47<1:52:49, 31.93s/it]\n",
      "2.  11%|█▏        | 27/240 [14:54<2:00:53, 34.05s/it]\n",
      "0.  11%|█▏        | 27/240 [14:59<2:02:08, 34.40s/it]\n",
      "1.  10%|█         | 25/240 [15:03<2:04:02, 34.61s/it]\n",
      "3.  12%|█▏        | 29/240 [15:23<1:57:31, 33.42s/it]\n",
      "3.  12%|█▎        | 30/240 [16:00<2:00:36, 34.46s/it]\n",
      "2.  12%|█▏        | 28/240 [15:25<1:56:38, 33.01s/it]\n",
      "1.  11%|█         | 26/240 [15:42<2:08:29, 36.03s/it]\n",
      "0.  12%|█▏        | 28/240 [15:29<1:56:32, 32.98s/it]\n",
      "0.  12%|█▏        | 29/240 [16:06<2:00:48, 34.35s/it]\n",
      "0.  12%|█▎        | 30/240 [16:33<1:52:23, 32.11s/it]\n",
      "1.  11%|█▏        | 27/240 [16:06<1:55:14, 32.46s/it]\n",
      "3.  12%|█▎        | 30/240 [16:00<2:00:36, 34.46s/it]\n",
      "2.  12%|█▏        | 29/240 [16:03<2:01:06, 34.44s/it]\n",
      "2.  12%|█▎        | 30/240 [16:41<2:04:26, 35.55s/it]\n",
      "3.  13%|█▎        | 31/240 [16:37<2:02:39, 35.22s/it]\n",
      "0.  12%|█▎        | 30/240 [16:33<1:52:23, 32.11s/it]\n",
      "2.  12%|█▎        | 30/240 [16:41<2:04:26, 35.55s/it]\n",
      "1.  12%|█▏        | 28/240 [16:36<1:51:39, 31.60s/it]\n",
      "3.  13%|█▎        | 32/240 [17:07<1:55:47, 33.40s/it]\n",
      "0.  13%|█▎        | 31/240 [17:10<1:56:55, 33.57s/it]\n",
      "1.  12%|█▏        | 29/240 [17:18<2:02:43, 34.90s/it]\n",
      "1.  12%|█▎        | 30/240 [17:53<2:02:06, 34.89s/it]\n",
      "2.  13%|█▎        | 31/240 [17:18<2:05:39, 36.07s/it]\n",
      "3.  14%|█▍        | 33/240 [17:39<1:54:18, 33.13s/it]\n",
      "0.  13%|█▎        | 32/240 [17:38<1:50:19, 31.82s/it]\n",
      "1.  12%|█▎        | 30/240 [17:53<2:02:06, 34.89s/it]\n",
      "2.  13%|█▎        | 32/240 [17:55<2:06:25, 36.47s/it]\n",
      "3.  14%|█▍        | 34/240 [18:00<1:40:57, 29.40s/it]\n",
      "0.  14%|█▍        | 33/240 [18:05<1:44:47, 30.37s/it]\n",
      "3.  15%|█▍        | 35/240 [18:28<1:39:01, 28.98s/it]\n",
      "1.  13%|█▎        | 31/240 [18:19<1:51:55, 32.13s/it]\n",
      "2.  14%|█▍        | 33/240 [18:26<1:59:45, 34.71s/it]\n",
      "3.  15%|█▌        | 36/240 [18:47<1:29:00, 26.18s/it]\n",
      "0.  14%|█▍        | 34/240 [18:42<1:51:04, 32.35s/it]\n",
      "2.  14%|█▍        | 34/240 [19:04<2:02:15, 35.61s/it]\n",
      "3.  15%|█▌        | 37/240 [19:18<1:32:37, 27.38s/it]\n",
      "1.  13%|█▎        | 32/240 [19:01<2:01:37, 35.09s/it]\n",
      "0.  15%|█▍        | 35/240 [19:19<1:54:59, 33.66s/it]\n",
      "2.  15%|█▍        | 35/240 [19:28<1:50:19, 32.29s/it]\n",
      "1.  14%|█▍        | 33/240 [19:40<2:05:27, 36.37s/it]\n",
      "3.  16%|█▌        | 38/240 [19:38<1:24:51, 25.21s/it]\n",
      "0.  15%|█▌        | 36/240 [19:48<1:50:11, 32.41s/it]\n",
      "2.  15%|█▌        | 36/240 [19:59<1:47:58, 31.76s/it]\n",
      "0.  15%|█▌        | 37/240 [20:15<1:43:40, 30.64s/it]\n",
      "1.  14%|█▍        | 34/240 [20:04<1:52:18, 32.71s/it]\n",
      "3.  16%|█▋        | 39/240 [20:15<1:36:14, 28.73s/it]\n",
      "3.  17%|█▋        | 40/240 [20:48<1:40:01, 30.01s/it]\n",
      "2.  15%|█▌        | 37/240 [20:36<1:53:28, 33.54s/it]\n",
      "3.  17%|█▋        | 40/240 [20:48<1:40:01, 30.01s/it]\n",
      "0.  16%|█▌        | 38/240 [20:42<1:39:27, 29.54s/it]\n",
      "1.  15%|█▍        | 35/240 [20:44<1:58:47, 34.77s/it]\n",
      "3.  17%|█▋        | 41/240 [21:08<1:30:14, 27.21s/it]\n",
      "2.  16%|█▌        | 38/240 [21:07<1:49:49, 32.62s/it]\n",
      "0.  16%|█▋        | 39/240 [21:11<1:38:49, 29.50s/it]\n",
      "0.  17%|█▋        | 40/240 [21:48<1:45:30, 31.65s/it]\n",
      "1.  15%|█▌        | 36/240 [21:14<1:53:01, 33.24s/it]\n",
      "3.  18%|█▊        | 42/240 [21:38<1:32:41, 28.09s/it]\n",
      "2.  16%|█▋        | 39/240 [21:41<1:50:28, 32.98s/it]\n",
      "2.  17%|█▋        | 40/240 [22:15<1:51:01, 33.31s/it]\n",
      "0.  17%|█▋        | 40/240 [21:48<1:45:30, 31.65s/it]\n",
      "1.  15%|█▌        | 37/240 [21:56<2:01:47, 36.00s/it]\n",
      "3.  18%|█▊        | 43/240 [22:07<1:32:37, 28.21s/it]\n",
      "2.  17%|█▋        | 40/240 [22:15<1:51:01, 33.31s/it]\n",
      "0.  17%|█▋        | 41/240 [22:25<1:50:46, 33.40s/it]\n",
      "1.  16%|█▌        | 38/240 [22:31<2:00:07, 35.68s/it]\n",
      "3.  18%|█▊        | 44/240 [22:41<1:38:20, 30.10s/it]\n",
      "2.  17%|█▋        | 41/240 [22:49<1:51:02, 33.48s/it]\n",
      "0.  18%|█▊        | 42/240 [22:49<1:40:47, 30.55s/it]\n",
      "1.  16%|█▋        | 39/240 [22:57<1:49:33, 32.71s/it]\n",
      "1.  17%|█▋        | 40/240 [23:32<1:51:54, 33.57s/it]\n",
      "3.  19%|█▉        | 45/240 [23:16<1:42:01, 31.39s/it]\n",
      "2.  18%|█▊        | 42/240 [23:26<1:54:22, 34.66s/it]\n",
      "0.  18%|█▊        | 43/240 [23:26<1:46:35, 32.47s/it]\n",
      "1.  17%|█▋        | 40/240 [23:32<1:51:54, 33.57s/it]\n",
      "3.  19%|█▉        | 46/240 [23:51<1:44:42, 32.38s/it]\n",
      "0.  18%|█▊        | 44/240 [23:59<1:46:15, 32.53s/it]\n",
      "2.  18%|█▊        | 43/240 [23:58<1:50:57, 33.79s/it]\n",
      "1.  17%|█▋        | 41/240 [24:07<1:52:50, 34.02s/it]\n",
      "0.  19%|█▉        | 45/240 [24:25<1:40:03, 30.79s/it]\n",
      "3.  20%|█▉        | 47/240 [24:25<1:46:01, 32.96s/it]\n",
      "2.  18%|█▊        | 44/240 [24:35<1:53:57, 34.89s/it]\n",
      "0.  19%|█▉        | 46/240 [24:53<1:35:56, 29.67s/it]\n",
      "1.  18%|█▊        | 42/240 [24:47<1:57:35, 35.63s/it]\n",
      "3.  20%|██        | 48/240 [25:02<1:49:19, 34.17s/it]\n",
      "1.  18%|█▊        | 43/240 [25:19<1:54:02, 34.73s/it]\n",
      "2.  19%|█▉        | 45/240 [25:13<1:56:09, 35.74s/it]\n",
      "0.  20%|█▉        | 47/240 [25:14<1:27:19, 27.15s/it]\n",
      "3.  20%|██        | 49/240 [25:23<1:35:53, 30.12s/it]\n",
      "3.  21%|██        | 50/240 [26:00<1:41:58, 32.20s/it]\n",
      "2.  19%|█▉        | 46/240 [25:51<1:57:26, 36.32s/it]\n",
      "1.  18%|█▊        | 44/240 [25:42<1:41:12, 30.98s/it]\n",
      "3.  21%|██        | 50/240 [26:00<1:41:58, 32.20s/it]\n",
      "0.  20%|██        | 48/240 [25:51<1:36:50, 30.26s/it]\n",
      "2.  20%|█▉        | 47/240 [26:16<1:45:46, 32.88s/it]\n",
      "1.  19%|█▉        | 45/240 [26:21<1:49:13, 33.61s/it]\n",
      "0.  20%|██        | 49/240 [26:28<1:42:52, 32.32s/it]\n",
      "0.  21%|██        | 50/240 [26:56<1:37:32, 30.80s/it]\n",
      "3.  21%|██▏       | 51/240 [26:28<1:37:40, 31.01s/it]\n",
      "1.  19%|█▉        | 46/240 [26:50<1:43:57, 32.15s/it]\n",
      "2.  20%|██        | 48/240 [26:48<1:44:20, 32.61s/it]\n",
      "0.  21%|██        | 50/240 [26:56<1:37:32, 30.80s/it]\n",
      "3.  22%|██▏       | 52/240 [27:05<1:42:59, 32.87s/it]\n",
      "2.  20%|██        | 49/240 [27:25<1:48:21, 34.04s/it]\n",
      "2.  21%|██        | 50/240 [27:50<1:39:02, 31.28s/it]\n",
      "1.  20%|█▉        | 47/240 [27:16<1:37:18, 30.25s/it]\n",
      "3.  22%|██▏       | 53/240 [27:42<1:46:18, 34.11s/it]\n",
      "0.  21%|██▏       | 51/240 [27:33<1:42:55, 32.67s/it]\n",
      "2.  21%|██        | 50/240 [27:50<1:39:02, 31.28s/it]\n",
      "1.  20%|██        | 48/240 [27:59<1:48:56, 34.05s/it]\n",
      "3.  22%|██▎       | 54/240 [28:10<1:39:54, 32.23s/it]\n",
      "0.  22%|██▏       | 52/240 [28:09<1:46:11, 33.89s/it]\n",
      "2.  21%|██▏       | 51/240 [28:19<1:36:57, 30.78s/it]\n",
      "1.  20%|██        | 49/240 [28:38<1:53:28, 35.65s/it]\n",
      "1.  21%|██        | 50/240 [29:04<1:43:17, 32.62s/it]\n",
      "0.  22%|██▏       | 53/240 [28:47<1:48:39, 34.86s/it]\n",
      "3.  23%|██▎       | 55/240 [28:44<1:41:27, 32.91s/it]\n",
      "2.  22%|██▏       | 52/240 [28:57<1:42:43, 32.78s/it]\n",
      "1.  21%|██        | 50/240 [29:04<1:43:17, 32.62s/it]\n",
      "0.  22%|██▎       | 54/240 [29:15<1:41:37, 32.78s/it]\n",
      "3.  23%|██▎       | 56/240 [29:19<1:42:25, 33.40s/it]\n",
      "2.  22%|██▏       | 53/240 [29:34<1:46:17, 34.10s/it]\n",
      "1.  21%|██▏       | 51/240 [29:44<1:49:28, 34.75s/it]\n",
      "3.  24%|██▍       | 57/240 [29:49<1:38:39, 32.35s/it]\n",
      "0.  23%|██▎       | 55/240 [29:47<1:41:12, 32.82s/it]\n",
      "2.  22%|██▎       | 54/240 [30:08<1:45:58, 34.18s/it]\n",
      "1.  22%|██▏       | 52/240 [30:19<1:49:20, 34.90s/it]\n",
      "3.  24%|██▍       | 58/240 [30:26<1:42:29, 33.79s/it]\n",
      "0.  23%|██▎       | 56/240 [30:25<1:44:59, 34.23s/it]\n",
      "2.  23%|██▎       | 55/240 [30:43<1:45:51, 34.33s/it]\n",
      "3.  25%|██▍       | 59/240 [30:55<1:38:02, 32.50s/it]\n",
      "3.  25%|██▌       | 60/240 [31:25<1:35:18, 31.77s/it]\n",
      "1.  22%|██▏       | 53/240 [30:54<1:49:27, 35.12s/it]\n",
      "0.  24%|██▍       | 57/240 [31:02<1:47:10, 35.14s/it]\n",
      "2.  23%|██▎       | 56/240 [31:21<1:48:51, 35.50s/it]\n",
      "1.  22%|██▎       | 54/240 [31:29<1:48:37, 35.04s/it]\n",
      "3.  25%|██▌       | 60/240 [31:25<1:35:18, 31.77s/it]\n",
      "3.  25%|██▌       | 61/240 [31:55<1:32:27, 30.99s/it]\n",
      "0.  24%|██▍       | 58/240 [31:39<1:47:57, 35.59s/it]\n",
      "2.  24%|██▍       | 57/240 [31:44<1:36:11, 31.54s/it]\n",
      "1.  23%|██▎       | 55/240 [31:54<1:38:08, 31.83s/it]\n",
      "0.  25%|██▍       | 59/240 [32:16<1:48:43, 36.04s/it]\n",
      "0.  25%|██▌       | 60/240 [32:44<1:40:37, 33.54s/it]\n",
      "2.  24%|██▍       | 58/240 [32:21<1:41:05, 33.32s/it]\n",
      "3.  26%|██▌       | 62/240 [32:16<1:23:27, 28.13s/it]\n",
      "1.  23%|██▎       | 56/240 [32:33<1:44:38, 34.12s/it]\n",
      "0.  25%|██▌       | 60/240 [32:44<1:40:37, 33.54s/it]\n",
      "2.  25%|██▍       | 59/240 [32:46<1:32:46, 30.76s/it]\n",
      "2.  25%|██▌       | 60/240 [33:20<1:35:31, 31.84s/it]\n",
      "3.  26%|██▋       | 63/240 [32:49<1:27:03, 29.51s/it]\n",
      "1.  24%|██▍       | 57/240 [32:57<1:34:46, 31.07s/it]\n",
      "0.  25%|██▌       | 61/240 [33:12<1:34:59, 31.84s/it]\n",
      "2.  25%|██▌       | 60/240 [33:20<1:35:31, 31.84s/it]\n",
      "3.  27%|██▋       | 64/240 [33:24<1:31:10, 31.08s/it]\n",
      "1.  24%|██▍       | 58/240 [33:36<1:41:54, 33.59s/it]\n",
      "0.  26%|██▌       | 62/240 [33:49<1:39:19, 33.48s/it]\n",
      "2.  25%|██▌       | 61/240 [33:51<1:33:43, 31.42s/it]\n",
      "3.  27%|██▋       | 65/240 [33:56<1:32:06, 31.58s/it]\n",
      "1.  25%|██▍       | 59/240 [34:09<1:40:02, 33.16s/it]\n",
      "1.  25%|██▌       | 60/240 [34:52<1:48:36, 36.20s/it]\n",
      "2.  26%|██▌       | 62/240 [34:23<1:34:01, 31.69s/it]\n",
      "0.  26%|██▋       | 63/240 [34:17<1:33:38, 31.74s/it]\n",
      "3.  28%|██▊       | 66/240 [34:33<1:36:15, 33.19s/it]\n",
      "0.  27%|██▋       | 64/240 [34:54<1:37:50, 33.35s/it]\n",
      "1.  25%|██▌       | 60/240 [34:52<1:48:36, 36.20s/it]\n",
      "2.  26%|██▋       | 63/240 [34:53<1:32:14, 31.27s/it]\n",
      "3.  28%|██▊       | 67/240 [35:08<1:36:44, 33.55s/it]\n",
      "0.  27%|██▋       | 65/240 [35:23<1:33:42, 32.13s/it]\n",
      "1.  25%|██▌       | 61/240 [35:24<1:44:30, 35.03s/it]\n",
      "2.  27%|██▋       | 64/240 [35:31<1:37:06, 33.10s/it]\n",
      "3.  28%|██▊       | 68/240 [35:41<1:35:43, 33.39s/it]\n",
      "0.  28%|██▊       | 66/240 [35:55<1:33:28, 32.23s/it]\n",
      "2.  27%|██▋       | 65/240 [36:05<1:37:44, 33.51s/it]\n",
      "1.  26%|██▌       | 62/240 [36:03<1:47:39, 36.29s/it]\n",
      "3.  29%|██▉       | 69/240 [36:17<1:37:56, 34.37s/it]\n",
      "3.  29%|██▉       | 70/240 [36:50<1:35:52, 33.84s/it]\n",
      "0.  28%|██▊       | 67/240 [36:32<1:36:52, 33.60s/it]\n",
      "2.  28%|██▊       | 66/240 [36:42<1:40:32, 34.67s/it]\n",
      "3.  29%|██▉       | 70/240 [36:50<1:35:52, 33.84s/it]\n",
      "1.  26%|██▋       | 63/240 [36:43<1:49:57, 37.28s/it]\n",
      "0.  28%|██▊       | 68/240 [36:59<1:30:27, 31.56s/it]\n",
      "2.  28%|██▊       | 67/240 [37:20<1:42:32, 35.56s/it]\n",
      "3.  30%|██▉       | 71/240 [37:25<1:36:16, 34.18s/it]\n",
      "1.  27%|██▋       | 64/240 [37:25<1:53:42, 38.77s/it]\n",
      "0.  29%|██▉       | 69/240 [37:36<1:34:32, 33.17s/it]\n",
      "0.  29%|██▉       | 70/240 [38:13<1:37:03, 34.25s/it]\n",
      "2.  28%|██▊       | 68/240 [37:51<1:37:29, 34.01s/it]\n",
      "3.  30%|███       | 72/240 [37:55<1:32:15, 32.95s/it]\n",
      "0.  29%|██▉       | 70/240 [38:13<1:37:03, 34.25s/it]\n",
      "1.  27%|██▋       | 65/240 [38:05<1:53:58, 39.08s/it]\n",
      "3.  30%|███       | 73/240 [38:32<1:35:12, 34.21s/it]\n",
      "2.  29%|██▉       | 69/240 [38:28<1:39:56, 35.07s/it]\n",
      "2.  29%|██▉       | 70/240 [39:03<1:38:51, 34.89s/it]\n",
      "1.  28%|██▊       | 66/240 [38:47<1:56:11, 40.06s/it]\n",
      "0.  30%|██▉       | 71/240 [38:42<1:32:21, 32.79s/it]\n",
      "3.  31%|███       | 74/240 [39:02<1:31:11, 32.96s/it]\n",
      "2.  29%|██▉       | 70/240 [39:03<1:38:51, 34.89s/it]\n",
      "1.  28%|██▊       | 67/240 [39:08<1:38:20, 34.11s/it]\n",
      "0.  30%|███       | 72/240 [39:10<1:27:40, 31.31s/it]\n",
      "3.  31%|███▏      | 75/240 [39:37<1:32:01, 33.47s/it]\n",
      "1.  28%|██▊       | 68/240 [39:43<1:38:34, 34.39s/it]\n",
      "0.  30%|███       | 73/240 [39:47<1:32:06, 33.09s/it]\n",
      "2.  30%|██▉       | 71/240 [39:40<1:40:29, 35.68s/it]\n",
      "0.  31%|███       | 74/240 [40:17<1:28:34, 32.02s/it]\n",
      "2.  30%|███       | 72/240 [40:18<1:41:34, 36.28s/it]\n",
      "3.  32%|███▏      | 76/240 [40:11<1:32:24, 33.81s/it]\n",
      "1.  29%|██▉       | 69/240 [40:18<1:38:36, 34.60s/it]\n",
      "1.  29%|██▉       | 70/240 [40:57<1:42:14, 36.08s/it]\n",
      "0.  31%|███▏      | 75/240 [40:46<1:25:46, 31.19s/it]\n",
      "3.  32%|███▏      | 77/240 [40:49<1:34:35, 34.82s/it]\n",
      "2.  30%|███       | 73/240 [40:48<1:36:19, 34.61s/it]\n",
      "1.  29%|██▉       | 70/240 [40:57<1:42:14, 36.08s/it]\n",
      "0.  32%|███▏      | 76/240 [41:15<1:23:11, 30.43s/it]\n",
      "3.  32%|███▎      | 78/240 [41:19<1:30:11, 33.40s/it]\n",
      "1.  30%|██▉       | 71/240 [41:30<1:38:29, 34.97s/it]\n",
      "2.  31%|███       | 74/240 [41:26<1:38:17, 35.53s/it]\n",
      "3.  33%|███▎      | 79/240 [41:52<1:29:11, 33.24s/it]\n",
      "3.  33%|███▎      | 80/240 [42:25<1:28:27, 33.17s/it]\n",
      "0.  32%|███▏      | 77/240 [41:47<1:24:30, 31.11s/it]\n",
      "1.  30%|███       | 72/240 [41:59<1:32:47, 33.14s/it]\n",
      "2.  31%|███▏      | 75/240 [42:04<1:39:51, 36.31s/it]\n",
      "3.  33%|███▎      | 80/240 [42:25<1:28:27, 33.17s/it]\n",
      "0.  32%|███▎      | 78/240 [42:24<1:28:51, 32.91s/it]\n",
      "2.  32%|███▏      | 76/240 [42:42<1:40:23, 36.73s/it]\n",
      "3.  34%|███▍      | 81/240 [42:46<1:18:33, 29.64s/it]\n",
      "1.  30%|███       | 73/240 [42:41<1:39:58, 35.92s/it]\n",
      "0.  33%|███▎      | 79/240 [43:02<1:31:40, 34.16s/it]\n",
      "0.  33%|███▎      | 80/240 [43:29<1:25:32, 32.08s/it]\n",
      "2.  32%|███▏      | 77/240 [43:12<1:34:29, 34.78s/it]\n",
      "1.  31%|███       | 74/240 [43:21<1:42:28, 37.04s/it]\n",
      "3.  34%|███▍      | 82/240 [43:19<1:20:33, 30.59s/it]\n",
      "0.  33%|███▎      | 80/240 [43:29<1:25:32, 32.08s/it]\n",
      "2.  32%|███▎      | 78/240 [43:43<1:30:28, 33.51s/it]\n",
      "3.  35%|███▍      | 83/240 [43:56<1:24:59, 32.48s/it]\n",
      "1.  31%|███▏      | 75/240 [43:56<1:40:08, 36.41s/it]\n",
      "0.  34%|███▍      | 81/240 [44:06<1:28:55, 33.55s/it]\n",
      "2.  33%|███▎      | 79/240 [44:13<1:26:57, 32.40s/it]\n",
      "2.  33%|███▎      | 80/240 [44:45<1:26:06, 32.29s/it]\n",
      "3.  35%|███▌      | 84/240 [44:16<1:15:08, 28.90s/it]\n",
      "1.  32%|███▏      | 76/240 [44:38<1:44:27, 38.22s/it]\n",
      "0.  34%|███▍      | 82/240 [44:39<1:27:48, 33.35s/it]\n",
      "2.  33%|███▎      | 80/240 [44:45<1:26:06, 32.29s/it]\n",
      "3.  35%|███▌      | 85/240 [44:53<1:20:59, 31.35s/it]\n",
      "0.  35%|███▍      | 83/240 [45:06<1:22:57, 31.70s/it]\n",
      "1.  32%|███▏      | 77/240 [45:08<1:36:45, 35.62s/it]\n",
      "2.  34%|███▍      | 81/240 [45:19<1:27:04, 32.86s/it]\n",
      "3.  36%|███▌      | 86/240 [45:21<1:17:58, 30.38s/it]\n",
      "0.  35%|███▌      | 84/240 [45:44<1:26:42, 33.35s/it]\n",
      "1.  32%|███▎      | 78/240 [45:51<1:42:07, 37.82s/it]\n",
      "2.  34%|███▍      | 82/240 [45:57<1:30:37, 34.42s/it]\n",
      "3.  36%|███▋      | 87/240 [45:58<1:22:25, 32.32s/it]\n",
      "0.  35%|███▌      | 85/240 [46:17<1:26:00, 33.30s/it]\n",
      "1.  33%|███▎      | 79/240 [46:20<1:34:59, 35.40s/it]\n",
      "1.  33%|███▎      | 80/240 [46:53<1:32:06, 34.54s/it]\n",
      "2.  35%|███▍      | 83/240 [46:27<1:26:47, 33.17s/it]\n",
      "3.  37%|███▋      | 88/240 [46:31<1:22:10, 32.43s/it]\n",
      "0.  36%|███▌      | 86/240 [46:46<1:22:27, 32.13s/it]\n",
      "3.  37%|███▋      | 89/240 [47:00<1:19:11, 31.47s/it]\n",
      "3.  38%|███▊      | 90/240 [47:27<1:15:00, 30.00s/it]\n",
      "1.  33%|███▎      | 80/240 [46:53<1:32:06, 34.54s/it]\n",
      "2.  35%|███▌      | 84/240 [46:57<1:23:42, 32.20s/it]\n",
      "3.  38%|███▊      | 90/240 [47:27<1:15:00, 30.00s/it]\n",
      "0.  36%|███▋      | 87/240 [47:19<1:22:19, 32.28s/it]\n",
      "2.  35%|███▌      | 85/240 [47:35<1:27:37, 33.92s/it]\n",
      "1.  34%|███▍      | 81/240 [47:28<1:31:52, 34.67s/it]\n",
      "0.  37%|███▋      | 88/240 [47:49<1:19:49, 31.51s/it]\n",
      "3.  38%|███▊      | 91/240 [47:47<1:07:29, 27.18s/it]\n",
      "2.  36%|███▌      | 86/240 [47:52<1:13:51, 28.78s/it]\n",
      "1.  34%|███▍      | 82/240 [47:52<1:23:14, 31.61s/it]\n",
      "0.  37%|███▋      | 89/240 [48:10<1:11:35, 28.45s/it]\n",
      "0.  38%|███▊      | 90/240 [48:39<1:11:20, 28.54s/it]\n",
      "2.  36%|███▋      | 87/240 [48:26<1:17:40, 30.46s/it]\n",
      "3.  38%|███▊      | 92/240 [48:14<1:06:33, 26.99s/it]\n",
      "1.  35%|███▍      | 83/240 [48:28<1:25:39, 32.74s/it]\n",
      "0.  38%|███▊      | 90/240 [48:39<1:11:20, 28.54s/it]\n",
      "3.  39%|███▉      | 93/240 [48:43<1:07:55, 27.72s/it]\n",
      "2.  37%|███▋      | 88/240 [48:42<1:06:18, 26.17s/it]\n",
      "1.  35%|███▌      | 84/240 [48:57<1:22:27, 31.72s/it]\n",
      "0.  38%|███▊      | 91/240 [49:08<1:11:31, 28.80s/it]\n",
      "3.  39%|███▉      | 94/240 [49:13<1:09:08, 28.41s/it]\n",
      "2.  37%|███▋      | 89/240 [49:20<1:14:34, 29.63s/it]\n",
      "2.  38%|███▊      | 90/240 [49:51<1:14:45, 29.90s/it]\n",
      "1.  35%|███▌      | 85/240 [49:39<1:30:01, 34.85s/it]\n",
      "0.  38%|███▊      | 92/240 [49:45<1:16:50, 31.16s/it]\n",
      "3.  40%|███▉      | 95/240 [49:50<1:14:47, 30.95s/it]\n",
      "2.  38%|███▊      | 90/240 [49:51<1:14:45, 29.90s/it]\n",
      "0.  39%|███▉      | 93/240 [50:13<1:14:34, 30.44s/it]\n",
      "1.  36%|███▌      | 86/240 [50:12<1:27:44, 34.18s/it]\n",
      "3.  40%|████      | 96/240 [50:27<1:18:34, 32.74s/it]\n",
      "2.  38%|███▊      | 91/240 [50:28<1:19:59, 32.21s/it]\n",
      "0.  39%|███▉      | 94/240 [50:42<1:12:42, 29.88s/it]\n",
      "3.  40%|████      | 97/240 [50:55<1:14:47, 31.38s/it]\n",
      "1.  36%|███▋      | 87/240 [50:54<1:33:34, 36.70s/it]\n",
      "2.  38%|███▊      | 92/240 [50:59<1:18:20, 31.76s/it]\n",
      "0.  40%|███▉      | 95/240 [51:08<1:09:18, 28.68s/it]\n",
      "3.  41%|████      | 98/240 [51:16<1:06:25, 28.07s/it]\n",
      "1.  37%|███▋      | 88/240 [51:20<1:24:45, 33.46s/it]\n",
      "0.  40%|████      | 96/240 [51:29<1:03:39, 26.52s/it]\n",
      "2.  39%|███▉      | 93/240 [51:29<1:16:19, 31.15s/it]\n",
      "3.  41%|████▏     | 99/240 [51:44<1:05:55, 28.05s/it]\n",
      "3.  42%|████▏     | 100/240 [52:18<1:09:54, 29.96s/it]\n",
      "0.  40%|████      | 97/240 [51:56<1:03:29, 26.64s/it]\n",
      "1.  37%|███▋      | 89/240 [51:55<1:25:18, 33.89s/it]\n",
      "1.  38%|███▊      | 90/240 [52:35<1:28:57, 35.58s/it]\n",
      "2.  39%|███▉      | 94/240 [52:06<1:20:22, 33.03s/it]\n",
      "0.  41%|████      | 98/240 [52:26<1:05:19, 27.61s/it]\n",
      "3.  42%|████▏     | 100/240 [52:18<1:09:54, 29.96s/it]\n",
      "1.  38%|███▊      | 90/240 [52:35<1:28:57, 35.58s/it]\n",
      "2.  40%|███▉      | 95/240 [52:44<1:23:12, 34.43s/it]\n",
      "0.  41%|████▏     | 99/240 [52:52<1:03:31, 27.03s/it]\n",
      "0.  42%|████▏     | 100/240 [53:20<1:03:37, 27.27s/it]\n",
      "3.  42%|████▏     | 101/240 [52:55<1:14:23, 32.11s/it]\n",
      "1.  38%|███▊      | 91/240 [53:10<1:28:00, 35.44s/it]\n",
      "0.  42%|████▏     | 100/240 [53:20<1:03:37, 27.27s/it]\n",
      "2.  40%|████      | 96/240 [53:17<1:22:03, 34.19s/it]\n",
      "1.  38%|███▊      | 92/240 [53:34<1:19:12, 32.11s/it]\n",
      "3.  42%|████▎     | 102/240 [53:33<1:17:24, 33.65s/it]\n",
      "0.  42%|████▏     | 101/240 [53:49<1:04:50, 27.99s/it]\n",
      "2.  40%|████      | 97/240 [53:55<1:23:56, 35.22s/it]\n",
      "3.  43%|████▎     | 103/240 [54:07<1:17:28, 33.93s/it]\n",
      "0.  42%|████▎     | 102/240 [54:11<59:53, 26.04s/it]\n",
      "1.  39%|███▉      | 93/240 [54:06<1:18:50, 32.18s/it]\n",
      "2.  41%|████      | 98/240 [54:27<1:21:16, 34.34s/it]\n",
      "3.  43%|████▎     | 104/240 [54:35<1:13:04, 32.24s/it]\n",
      "0.  43%|████▎     | 103/240 [54:35<57:57, 25.38s/it]\n",
      "2.  41%|████▏     | 99/240 [54:58<1:18:07, 33.24s/it]\n",
      "2.  42%|████▏     | 100/240 [55:19<1:08:59, 29.57s/it]\n",
      "1.  39%|███▉      | 94/240 [54:46<1:23:31, 34.33s/it]\n",
      "3.  44%|████▍     | 105/240 [55:10<1:14:00, 32.89s/it]\n",
      "1.  40%|███▉      | 95/240 [55:21<1:23:28, 34.54s/it]\n",
      "0.  43%|████▎     | 104/240 [55:12<1:05:19, 28.82s/it]\n",
      "2.  42%|████▏     | 100/240 [55:19<1:08:59, 29.57s/it]\n",
      "3.  44%|████▍     | 106/240 [55:38<1:10:26, 31.54s/it]\n",
      "2.  42%|████▏     | 101/240 [55:56<1:13:59, 31.94s/it]\n",
      "0.  44%|████▍     | 105/240 [55:49<1:10:37, 31.39s/it]\n",
      "1.  40%|████      | 96/240 [55:50<1:18:53, 32.87s/it]\n",
      "3.  45%|████▍     | 107/240 [56:16<1:13:57, 33.37s/it]\n",
      "1.  40%|████      | 97/240 [56:25<1:19:54, 33.53s/it]\n",
      "2.  42%|████▎     | 102/240 [56:21<1:08:23, 29.74s/it]\n",
      "0.  44%|████▍     | 106/240 [56:21<1:10:47, 31.70s/it]\n",
      "0.  45%|████▍     | 107/240 [56:58<1:13:35, 33.20s/it]\n",
      "3.  45%|████▌     | 108/240 [56:53<1:15:55, 34.51s/it]\n",
      "2.  43%|████▎     | 103/240 [56:58<1:13:00, 31.98s/it]\n",
      "1.  41%|████      | 98/240 [56:57<1:18:31, 33.18s/it]\n",
      "3.  45%|████▌     | 109/240 [57:28<1:15:23, 34.53s/it]\n",
      "3.  46%|████▌     | 110/240 [58:01<1:13:46, 34.05s/it]\n",
      "2.  43%|████▎     | 104/240 [57:31<1:12:50, 32.14s/it]\n",
      "0.  45%|████▌     | 108/240 [57:26<1:09:31, 31.60s/it]\n",
      "1.  41%|████▏     | 99/240 [57:40<1:24:35, 36.00s/it]\n",
      "1.  42%|████▏     | 100/240 [58:20<1:26:42, 37.16s/it]\n",
      "0.  45%|████▌     | 109/240 [58:03<1:12:36, 33.26s/it]\n",
      "0.  46%|████▌     | 110/240 [58:33<1:09:37, 32.13s/it]\n",
      "2.  44%|████▍     | 105/240 [58:02<1:11:57, 31.98s/it]\n",
      "3.  46%|████▌     | 110/240 [58:01<1:13:46, 34.05s/it]\n",
      "1.  42%|████▏     | 100/240 [58:20<1:26:42, 37.16s/it]\n",
      "0.  46%|████▌     | 110/240 [58:33<1:09:37, 32.13s/it]\n",
      "2.  44%|████▍     | 106/240 [58:34<1:11:18, 31.93s/it]\n",
      "3.  46%|████▋     | 111/240 [58:37<1:14:54, 34.84s/it]\n",
      "1.  42%|████▏     | 101/240 [58:42<1:15:34, 32.62s/it]\n",
      "0.  46%|████▋     | 111/240 [59:02<1:07:31, 31.40s/it]\n",
      "3.  47%|████▋     | 112/240 [59:07<1:11:10, 33.36s/it]\n",
      "2.  45%|████▍     | 107/240 [59:05<1:09:47, 31.48s/it]\n",
      "1.  42%|████▎     | 102/240 [59:17<1:16:36, 33.31s/it]\n",
      "0.  47%|████▋     | 112/240 [59:32<1:05:37, 30.76s/it]\n",
      "3.  47%|████▋     | 113/240 [59:33<1:06:00, 31.19s/it]\n",
      "2.  45%|████▌     | 108/240 [59:43<1:13:48, 33.55s/it]\n",
      "1.  43%|████▎     | 103/240 [59:49<1:15:09, 32.91s/it]\n",
      "3.  48%|████▊     | 114/240 [1:00:03<1:04:30, 30.72s/it]\n",
      "0.  47%|████▋     | 113/240 [1:00:01<1:04:25, 30.44s/it]\n",
      "2.  45%|████▌     | 109/240 [1:00:20<1:15:37, 34.64s/it]\n",
      "2.  46%|████▌     | 110/240 [1:00:51<1:12:40, 33.54s/it]\n",
      "1.  43%|████▎     | 104/240 [1:00:31<1:21:15, 35.85s/it]\n",
      "3.  48%|████▊     | 115/240 [1:00:37<1:06:17, 31.82s/it]\n",
      "0.  48%|████▊     | 114/240 [1:00:38<1:07:50, 32.30s/it]\n",
      "2.  46%|████▌     | 110/240 [1:00:51<1:12:40, 33.54s/it]\n",
      "1.  44%|████▍     | 105/240 [1:00:56<1:12:48, 32.36s/it]\n",
      "3.  48%|████▊     | 116/240 [1:01:11<1:07:10, 32.51s/it]\n",
      "0.  48%|████▊     | 115/240 [1:01:15<1:10:17, 33.74s/it]\n",
      "2.  46%|████▋     | 111/240 [1:01:29<1:14:47, 34.79s/it]\n",
      "3.  49%|████▉     | 117/240 [1:01:44<1:06:55, 32.65s/it]\n",
      "1.  44%|████▍     | 106/240 [1:01:38<1:19:15, 35.49s/it]\n",
      "0.  48%|████▊     | 116/240 [1:01:52<1:11:42, 34.69s/it]\n",
      "2.  47%|████▋     | 112/240 [1:02:03<1:13:46, 34.59s/it]\n",
      "3.  49%|████▉     | 118/240 [1:02:04<58:32, 28.79s/it]\n",
      "0.  49%|████▉     | 117/240 [1:02:19<1:06:39, 32.52s/it]\n",
      "1.  45%|████▍     | 107/240 [1:02:08<1:14:51, 33.77s/it]\n",
      "3.  50%|████▉     | 119/240 [1:02:39<1:01:33, 30.52s/it]\n",
      "3.  50%|█████     | 120/240 [1:03:00<55:36, 27.80s/it]\n",
      "2.  47%|████▋     | 113/240 [1:02:37<1:12:59, 34.49s/it]\n",
      "0.  49%|████▉     | 118/240 [1:02:41<59:13, 29.13s/it]\n",
      "1.  45%|████▌     | 108/240 [1:02:50<1:19:45, 36.26s/it]\n",
      "3.  50%|█████     | 120/240 [1:03:00<55:36, 27.80s/it]\n",
      "1.  45%|████▌     | 109/240 [1:03:20<1:14:47, 34.26s/it]\n",
      "1.  46%|████▌     | 110/240 [1:03:48<1:10:26, 32.51s/it]\n",
      "2.  48%|████▊     | 114/240 [1:03:15<1:14:15, 35.36s/it]\n",
      "0.  50%|████▉     | 119/240 [1:03:18<1:03:39, 31.57s/it]\n",
      "0.  50%|█████     | 120/240 [1:03:55<1:06:22, 33.19s/it]\n",
      "3.  50%|█████     | 121/240 [1:03:22<51:31, 25.98s/it]\n",
      "2.  48%|████▊     | 115/240 [1:03:52<1:14:58, 35.99s/it]\n",
      "1.  46%|████▌     | 110/240 [1:03:48<1:10:26, 32.51s/it]\n",
      "3.  51%|█████     | 122/240 [1:03:56<56:04, 28.52s/it]\n",
      "0.  50%|█████     | 120/240 [1:03:55<1:06:22, 33.19s/it]\n",
      "2.  48%|████▊     | 116/240 [1:04:24<1:11:59, 34.84s/it]\n",
      "1.  46%|████▋     | 111/240 [1:04:28<1:14:29, 34.64s/it]\n",
      "0.  50%|█████     | 121/240 [1:04:32<1:08:19, 34.45s/it]\n",
      "3.  51%|█████▏    | 123/240 [1:04:29<57:58, 29.73s/it]\n",
      "0.  51%|█████     | 122/240 [1:05:02<1:04:52, 32.98s/it]\n",
      "1.  47%|████▋     | 112/240 [1:04:58<1:10:45, 33.17s/it]\n",
      "2.  49%|████▉     | 117/240 [1:04:56<1:09:40, 33.99s/it]\n",
      "3.  52%|█████▏    | 124/240 [1:05:03<1:00:13, 31.15s/it]\n",
      "3.  52%|█████▏    | 125/240 [1:05:34<59:14, 30.91s/it]\n",
      "2.  49%|████▉     | 118/240 [1:05:34<1:11:09, 35.00s/it]\n",
      "0.  51%|█████▏    | 123/240 [1:05:31<1:02:02, 31.82s/it]\n",
      "1.  47%|████▋     | 113/240 [1:05:33<1:11:34, 33.81s/it]\n",
      "3.  52%|█████▎    | 126/240 [1:05:54<52:52, 27.83s/it]\n",
      "0.  52%|█████▏    | 124/240 [1:06:08<1:04:32, 33.38s/it]\n",
      "1.  48%|████▊     | 114/240 [1:06:12<1:14:23, 35.42s/it]\n",
      "2.  50%|████▉     | 119/240 [1:06:08<1:10:12, 34.81s/it]\n",
      "2.  50%|█████     | 120/240 [1:06:46<1:11:38, 35.82s/it]\n",
      "3.  53%|█████▎    | 127/240 [1:06:31<57:39, 30.61s/it]\n",
      "2.  50%|█████     | 120/240 [1:06:46<1:11:38, 35.82s/it]\n",
      "0.  52%|█████▏    | 125/240 [1:06:41<1:03:45, 33.27s/it]\n",
      "1.  48%|████▊     | 115/240 [1:06:45<1:12:12, 34.66s/it]\n",
      "3.  53%|█████▎    | 128/240 [1:07:06<59:32, 31.90s/it]\n",
      "2.  50%|█████     | 121/240 [1:07:17<1:08:03, 34.32s/it]\n",
      "0.  52%|█████▎    | 126/240 [1:07:18<1:05:27, 34.45s/it]\n",
      "1.  48%|████▊     | 116/240 [1:07:28<1:16:36, 37.07s/it]\n",
      "3.  54%|█████▍    | 129/240 [1:07:36<57:57, 31.33s/it]\n",
      "3.  54%|█████▍    | 130/240 [1:08:13<1:00:33, 33.03s/it]\n",
      "2.  51%|█████     | 122/240 [1:07:54<1:09:21, 35.27s/it]\n",
      "1.  49%|████▉     | 117/240 [1:08:07<1:17:26, 37.78s/it]\n",
      "0.  53%|█████▎    | 127/240 [1:07:55<1:06:11, 35.15s/it]\n",
      "3.  54%|█████▍    | 130/240 [1:08:13<1:00:33, 33.03s/it]\n",
      "0.  53%|█████▎    | 128/240 [1:08:32<1:06:46, 35.77s/it]\n",
      "2.  51%|█████▏    | 123/240 [1:08:32<1:10:13, 36.01s/it]\n",
      "1.  49%|████▉     | 118/240 [1:08:33<1:09:35, 34.23s/it]\n",
      "3.  55%|█████▍    | 131/240 [1:08:50<1:02:00, 34.13s/it]\n",
      "0.  54%|█████▍    | 129/240 [1:08:52<57:40, 31.18s/it]\n",
      "0.  54%|█████▍    | 130/240 [1:09:29<1:00:16, 32.88s/it]\n",
      "2.  52%|█████▏    | 124/240 [1:09:06<1:08:37, 35.50s/it]\n",
      "3.  55%|█████▌    | 132/240 [1:09:19<58:52, 32.71s/it]\n",
      "0.  54%|█████▍    | 130/240 [1:09:29<1:00:16, 32.88s/it]\n",
      "1.  50%|████▉     | 119/240 [1:09:12<1:12:05, 35.75s/it]\n",
      "1.  50%|█████     | 120/240 [1:09:55<1:15:40, 37.84s/it]\n",
      "2.  52%|█████▏    | 125/240 [1:09:40<1:07:08, 35.03s/it]\n",
      "0.  55%|█████▍    | 131/240 [1:09:50<53:16, 29.33s/it]\n",
      "3.  55%|█████▌    | 133/240 [1:09:48<56:00, 31.40s/it]\n",
      "1.  50%|█████     | 120/240 [1:09:55<1:15:40, 37.84s/it]\n",
      "2.  52%|█████▎    | 126/240 [1:10:15<1:06:03, 34.77s/it]\n",
      "3.  56%|█████▌    | 134/240 [1:10:25<58:22, 33.05s/it]\n",
      "0.  55%|█████▌    | 132/240 [1:10:23<54:45, 30.42s/it]\n",
      "1.  50%|█████     | 121/240 [1:10:37<1:17:45, 39.21s/it]\n",
      "2.  53%|█████▎    | 127/240 [1:10:46<1:03:43, 33.84s/it]\n",
      "3.  56%|█████▋    | 135/240 [1:10:57<57:43, 32.98s/it]\n",
      "0.  55%|█████▌    | 133/240 [1:11:00<57:45, 32.39s/it]\n",
      "3.  57%|█████▋    | 136/240 [1:11:27<55:15, 31.88s/it]\n",
      "2.  53%|█████▎    | 128/240 [1:11:24<1:05:11, 34.92s/it]\n",
      "1.  51%|█████     | 122/240 [1:11:19<1:18:44, 40.04s/it]\n",
      "0.  56%|█████▌    | 134/240 [1:11:37<59:40, 33.78s/it]\n",
      "3.  57%|█████▋    | 137/240 [1:11:46<48:02, 27.98s/it]\n",
      "1.  51%|█████▏    | 123/240 [1:11:59<1:17:51, 39.93s/it]\n",
      "2.  54%|█████▍    | 129/240 [1:11:53<1:01:42, 33.35s/it]\n",
      "2.  54%|█████▍    | 130/240 [1:12:31<1:03:35, 34.68s/it]\n",
      "0.  56%|█████▋    | 135/240 [1:12:07<56:57, 32.55s/it]\n",
      "3.  57%|█████▊    | 138/240 [1:12:16<48:37, 28.60s/it]\n",
      "1.  52%|█████▏    | 124/240 [1:12:20<1:06:10, 34.22s/it]\n",
      "2.  54%|█████▍    | 130/240 [1:12:31<1:03:35, 34.68s/it]\n",
      "0.  57%|█████▋    | 136/240 [1:12:36<54:25, 31.40s/it]\n",
      "3.  58%|█████▊    | 139/240 [1:12:50<50:56, 30.26s/it]\n",
      "3.  58%|█████▊    | 140/240 [1:13:23<51:52, 31.13s/it]\n",
      "1.  52%|█████▏    | 125/240 [1:12:52<1:04:36, 33.71s/it]\n",
      "0.  57%|█████▋    | 137/240 [1:13:09<54:38, 31.83s/it]\n",
      "2.  55%|█████▍    | 131/240 [1:13:09<1:04:43, 35.63s/it]\n",
      "1.  52%|█████▎    | 126/240 [1:13:32<1:07:34, 35.56s/it]\n",
      "3.  58%|█████▊    | 140/240 [1:13:23<51:52, 31.13s/it]\n",
      "0.  57%|█████▊    | 138/240 [1:13:38<52:50, 31.09s/it]\n",
      "1.  53%|█████▎    | 127/240 [1:13:52<58:16, 30.94s/it]\n",
      "3.  59%|█████▉    | 141/240 [1:13:58<53:02, 32.15s/it]\n",
      "2.  55%|█████▌    | 132/240 [1:13:44<1:03:43, 35.40s/it]\n",
      "0.  58%|█████▊    | 139/240 [1:14:05<50:32, 30.02s/it]\n",
      "0.  58%|█████▊    | 140/240 [1:14:34<49:31, 29.71s/it]\n",
      "1.  53%|█████▎    | 128/240 [1:14:13<51:40, 27.68s/it]\n",
      "3.  59%|█████▉    | 142/240 [1:14:17<46:25, 28.42s/it]\n",
      "2.  55%|█████▌    | 133/240 [1:14:22<1:04:26, 36.14s/it]\n",
      "0.  58%|█████▊    | 140/240 [1:14:34<49:31, 29.71s/it]\n",
      "3.  60%|█████▉    | 143/240 [1:14:52<48:55, 30.26s/it]\n",
      "1.  54%|█████▍    | 129/240 [1:14:42<52:15, 28.25s/it]\n",
      "1.  54%|█████▍    | 130/240 [1:15:22<57:57, 31.62s/it]\n",
      "2.  56%|█████▌    | 134/240 [1:14:59<1:04:33, 36.54s/it]\n",
      "0.  59%|█████▉    | 141/240 [1:15:04<49:01, 29.71s/it]\n",
      "3.  60%|██████    | 144/240 [1:15:21<47:57, 29.98s/it]\n",
      "1.  54%|█████▍    | 130/240 [1:15:22<57:57, 31.62s/it]\n",
      "2.  56%|█████▋    | 135/240 [1:15:24<57:50, 33.05s/it]\n",
      "0.  59%|█████▉    | 142/240 [1:15:32<47:37, 29.16s/it]\n",
      "3.  60%|██████    | 145/240 [1:15:42<43:18, 27.35s/it]\n",
      "1.  55%|█████▍    | 131/240 [1:16:01<1:01:38, 33.93s/it]\n",
      "2.  57%|█████▋    | 136/240 [1:16:02<59:49, 34.51s/it]\n",
      "0.  60%|█████▉    | 143/240 [1:16:09<51:00, 31.55s/it]\n",
      "3.  61%|██████    | 146/240 [1:16:12<44:05, 28.15s/it]\n",
      "1.  55%|█████▌    | 132/240 [1:16:27<56:42, 31.50s/it]\n",
      "0.  60%|██████    | 144/240 [1:16:46<52:57, 33.10s/it]\n",
      "2.  57%|█████▋    | 137/240 [1:16:39<1:00:46, 35.41s/it]\n",
      "3.  61%|██████▏   | 147/240 [1:16:49<47:40, 30.76s/it]\n",
      "1.  55%|█████▌    | 133/240 [1:17:10<1:02:25, 35.01s/it]\n",
      "0.  60%|██████    | 145/240 [1:17:15<50:43, 32.04s/it]\n",
      "2.  57%|█████▊    | 138/240 [1:17:17<1:01:29, 36.17s/it]\n",
      "3.  62%|██████▏   | 148/240 [1:17:22<48:06, 31.37s/it]\n",
      "1.  56%|█████▌    | 134/240 [1:17:39<58:32, 33.14s/it]\n",
      "2.  58%|█████▊    | 139/240 [1:17:55<1:01:30, 36.54s/it]\n",
      "2.  58%|█████▊    | 140/240 [1:18:27<58:39, 35.20s/it]\n",
      "3.  62%|██████▏   | 149/240 [1:17:56<48:53, 32.23s/it]\n",
      "3.  62%|██████▎   | 150/240 [1:18:31<49:22, 32.92s/it]\n",
      "0.  61%|██████    | 146/240 [1:17:53<52:39, 33.61s/it]\n",
      "1.  56%|█████▋    | 135/240 [1:18:04<54:04, 30.90s/it]\n",
      "3.  62%|██████▎   | 150/240 [1:18:31<49:22, 32.92s/it]\n",
      "2.  58%|█████▊    | 140/240 [1:18:27<58:39, 35.20s/it]\n",
      "1.  57%|█████▋    | 136/240 [1:18:34<52:45, 30.44s/it]\n",
      "0.  61%|██████▏   | 147/240 [1:18:30<53:42, 34.65s/it]\n",
      "3.  63%|██████▎   | 151/240 [1:18:57<45:54, 30.95s/it]\n",
      "2.  59%|█████▉    | 141/240 [1:18:58<55:55, 33.90s/it]\n",
      "1.  57%|█████▋    | 137/240 [1:19:03<51:26, 29.96s/it]\n",
      "0.  62%|██████▏   | 148/240 [1:19:07<54:18, 35.42s/it]\n",
      "3.  63%|██████▎   | 152/240 [1:19:30<46:12, 31.51s/it]\n",
      "1.  57%|█████▊    | 138/240 [1:19:42<55:30, 32.65s/it]\n",
      "0.  62%|██████▏   | 149/240 [1:19:44<54:27, 35.91s/it]\n",
      "0.  62%|██████▎   | 150/240 [1:20:12<50:08, 33.42s/it]\n",
      "2.  59%|█████▉    | 142/240 [1:19:35<57:03, 34.94s/it]\n",
      "3.  64%|██████▍   | 153/240 [1:20:04<47:00, 32.42s/it]\n",
      "0.  62%|██████▎   | 150/240 [1:20:12<50:08, 33.42s/it]\n",
      "1.  58%|█████▊    | 139/240 [1:20:11<53:11, 31.60s/it]\n",
      "1.  58%|█████▊    | 140/240 [1:20:50<56:42, 34.03s/it]\n",
      "2.  60%|█████▉    | 143/240 [1:20:13<58:02, 35.91s/it]\n",
      "0.  63%|██████▎   | 151/240 [1:20:41<47:54, 32.30s/it]\n",
      "3.  64%|██████▍   | 154/240 [1:20:42<48:31, 33.86s/it]\n",
      "2.  60%|██████    | 144/240 [1:20:51<58:16, 36.42s/it]\n",
      "3.  65%|██████▍   | 155/240 [1:21:10<45:33, 32.16s/it]\n",
      "1.  58%|█████▊    | 140/240 [1:20:50<56:42, 34.03s/it]\n",
      "0.  63%|██████▎   | 152/240 [1:21:05<43:32, 29.68s/it]\n",
      "1.  59%|█████▉    | 141/240 [1:21:33<1:00:27, 36.64s/it]\n",
      "2.  60%|██████    | 145/240 [1:21:21<54:47, 34.60s/it]\n",
      "0.  64%|██████▍   | 153/240 [1:21:34<42:55, 29.60s/it]\n",
      "3.  65%|██████▌   | 156/240 [1:21:30<39:50, 28.46s/it]\n",
      "2.  61%|██████    | 146/240 [1:21:59<55:37, 35.51s/it]\n",
      "0.  64%|██████▍   | 154/240 [1:22:03<42:13, 29.46s/it]\n",
      "3.  65%|██████▌   | 157/240 [1:22:05<42:03, 30.40s/it]\n",
      "1.  59%|█████▉    | 142/240 [1:21:59<54:34, 33.41s/it]\n",
      "2.  61%|██████▏   | 147/240 [1:22:33<54:19, 35.05s/it]\n",
      "0.  65%|██████▍   | 155/240 [1:22:32<41:30, 29.31s/it]\n",
      "3.  66%|██████▌   | 158/240 [1:22:38<42:36, 31.18s/it]\n",
      "1.  60%|█████▉    | 143/240 [1:22:39<56:58, 35.24s/it]\n",
      "2.  62%|██████▏   | 148/240 [1:22:55<48:01, 31.32s/it]\n",
      "1.  60%|██████    | 144/240 [1:23:18<58:21, 36.47s/it]\n",
      "0.  65%|██████▌   | 156/240 [1:23:09<44:09, 31.54s/it]\n",
      "3.  66%|██████▋   | 159/240 [1:23:14<44:20, 32.84s/it]\n",
      "3.  67%|██████▋   | 160/240 [1:23:51<45:25, 34.07s/it]\n",
      "2.  62%|██████▏   | 149/240 [1:23:28<47:53, 31.57s/it]\n",
      "2.  62%|██████▎   | 150/240 [1:24:05<49:58, 33.31s/it]\n",
      "0.  65%|██████▌   | 157/240 [1:23:38<42:27, 30.69s/it]\n",
      "1.  60%|██████    | 145/240 [1:23:38<49:58, 31.56s/it]\n",
      "3.  67%|██████▋   | 160/240 [1:23:51<45:25, 34.07s/it]\n",
      "2.  62%|██████▎   | 150/240 [1:24:05<49:58, 33.31s/it]\n",
      "0.  66%|██████▌   | 158/240 [1:24:05<40:35, 29.70s/it]\n",
      "3.  67%|██████▋   | 161/240 [1:24:24<44:16, 33.63s/it]\n",
      "2.  63%|██████▎   | 151/240 [1:24:30<45:44, 30.83s/it]\n",
      "1.  61%|██████    | 146/240 [1:24:20<54:31, 34.81s/it]\n",
      "3.  68%|██████▊   | 162/240 [1:24:44<38:37, 29.71s/it]\n",
      "0.  66%|██████▋   | 159/240 [1:24:42<43:05, 31.93s/it]\n",
      "0.  67%|██████▋   | 160/240 [1:25:19<44:29, 33.37s/it]\n",
      "2.  63%|██████▎   | 152/240 [1:24:55<42:36, 29.06s/it]\n",
      "1.  61%|██████▏   | 147/240 [1:24:56<54:13, 34.98s/it]\n",
      "0.  67%|██████▋   | 160/240 [1:25:19<44:29, 33.37s/it]\n",
      "3.  68%|██████▊   | 163/240 [1:25:19<39:55, 31.11s/it]\n",
      "2.  64%|██████▍   | 153/240 [1:25:29<44:23, 30.62s/it]\n",
      "1.  62%|██████▏   | 148/240 [1:25:31<53:52, 35.14s/it]\n",
      "0.  67%|██████▋   | 161/240 [1:25:48<42:08, 32.00s/it]\n",
      "3.  68%|██████▊   | 164/240 [1:25:52<40:01, 31.60s/it]\n",
      "1.  62%|██████▏   | 149/240 [1:26:10<55:05, 36.33s/it]\n",
      "1.  62%|██████▎   | 150/240 [1:26:43<52:46, 35.18s/it]\n",
      "2.  64%|██████▍   | 154/240 [1:26:07<46:45, 32.63s/it]\n",
      "0.  68%|██████▊   | 162/240 [1:26:25<43:29, 33.45s/it]\n",
      "3.  69%|██████▉   | 165/240 [1:26:29<41:34, 33.26s/it]\n",
      "2.  65%|██████▍   | 155/240 [1:26:45<48:43, 34.40s/it]\n",
      "1.  62%|██████▎   | 150/240 [1:26:43<52:46, 35.18s/it]\n",
      "3.  69%|██████▉   | 166/240 [1:26:57<39:01, 31.64s/it]\n",
      "0.  68%|██████▊   | 163/240 [1:26:54<41:20, 32.22s/it]\n",
      "2.  65%|██████▌   | 156/240 [1:27:10<44:01, 31.44s/it]\n",
      "1.  63%|██████▎   | 151/240 [1:27:18<52:15, 35.23s/it]\n",
      "0.  68%|██████▊   | 164/240 [1:27:31<42:32, 33.59s/it]\n",
      "3.  70%|██████▉   | 167/240 [1:27:29<38:56, 32.01s/it]\n",
      "2.  65%|██████▌   | 157/240 [1:27:47<45:55, 33.20s/it]\n",
      "0.  69%|██████▉   | 165/240 [1:27:51<37:06, 29.68s/it]\n",
      "1.  63%|██████▎   | 152/240 [1:27:50<50:21, 34.33s/it]\n",
      "3.  70%|███████   | 168/240 [1:28:06<40:09, 33.47s/it]\n",
      "2.  66%|██████▌   | 158/240 [1:28:07<40:07, 29.36s/it]\n",
      "0.  69%|██████▉   | 166/240 [1:28:21<36:24, 29.52s/it]\n",
      "1.  64%|██████▍   | 153/240 [1:28:30<52:01, 35.88s/it]\n",
      "3.  70%|███████   | 169/240 [1:28:39<39:28, 33.36s/it]\n",
      "3.  71%|███████   | 170/240 [1:29:14<39:17, 33.68s/it]\n",
      "1.  64%|██████▍   | 154/240 [1:28:50<44:39, 31.16s/it]\n",
      "2.  66%|██████▋   | 159/240 [1:28:42<41:39, 30.86s/it]\n",
      "2.  67%|██████▋   | 160/240 [1:29:15<42:18, 31.73s/it]\n",
      "0.  70%|██████▉   | 167/240 [1:28:48<35:18, 29.02s/it]\n",
      "2.  67%|██████▋   | 160/240 [1:29:15<42:18, 31.73s/it]\n",
      "3.  71%|███████   | 170/240 [1:29:14<39:17, 33.68s/it]\n",
      "0.  70%|███████   | 168/240 [1:29:18<35:00, 29.17s/it]\n",
      "1.  65%|██████▍   | 155/240 [1:29:15<41:42, 29.44s/it]\n",
      "3.  71%|███████▏  | 171/240 [1:29:40<36:03, 31.36s/it]\n",
      "0.  70%|███████   | 169/240 [1:29:45<33:47, 28.56s/it]\n",
      "0.  71%|███████   | 170/240 [1:30:13<33:01, 28.30s/it]\n",
      "2.  67%|██████▋   | 161/240 [1:29:37<37:34, 28.54s/it]\n",
      "1.  65%|██████▌   | 156/240 [1:29:50<43:27, 31.04s/it]\n",
      "0.  71%|███████   | 170/240 [1:30:13<33:01, 28.30s/it]\n",
      "2.  68%|██████▊   | 162/240 [1:30:14<40:34, 31.21s/it]\n",
      "3.  72%|███████▏  | 172/240 [1:30:13<36:04, 31.83s/it]\n",
      "1.  65%|██████▌   | 157/240 [1:30:16<40:40, 29.41s/it]\n",
      "0.  71%|███████▏  | 171/240 [1:30:42<32:58, 28.67s/it]\n",
      "2.  68%|██████▊   | 163/240 [1:30:44<39:44, 30.97s/it]\n",
      "1.  66%|██████▌   | 158/240 [1:30:59<45:40, 33.43s/it]\n",
      "3.  72%|███████▏  | 173/240 [1:30:49<37:11, 33.30s/it]\n",
      "0.  72%|███████▏  | 172/240 [1:31:12<32:41, 28.84s/it]\n",
      "2.  68%|██████▊   | 164/240 [1:31:15<39:08, 30.90s/it]\n",
      "1.  66%|██████▋   | 159/240 [1:31:19<39:55, 29.58s/it]\n",
      "1.  67%|██████▋   | 160/240 [1:31:48<39:15, 29.45s/it]\n",
      "3.  72%|███████▎  | 174/240 [1:31:24<37:09, 33.79s/it]\n",
      "0.  72%|███████▏  | 173/240 [1:31:38<31:28, 28.18s/it]\n",
      "1.  67%|██████▋   | 160/240 [1:31:48<39:15, 29.45s/it]\n",
      "2.  69%|██████▉   | 165/240 [1:31:40<36:19, 29.06s/it]\n",
      "0.  72%|███████▎  | 174/240 [1:32:07<31:12, 28.37s/it]\n",
      "3.  73%|███████▎  | 175/240 [1:32:01<37:37, 34.73s/it]\n",
      "2.  69%|██████▉   | 166/240 [1:32:18<39:05, 31.69s/it]\n",
      "1.  67%|██████▋   | 161/240 [1:32:12<36:32, 27.76s/it]\n",
      "2.  70%|██████▉   | 167/240 [1:32:48<38:07, 31.33s/it]\n",
      "0.  73%|███████▎  | 175/240 [1:32:36<30:54, 28.53s/it]\n",
      "3.  73%|███████▎  | 176/240 [1:32:38<37:48, 35.45s/it]\n",
      "1.  68%|██████▊   | 162/240 [1:32:55<41:55, 32.25s/it]\n",
      "0.  73%|███████▎  | 176/240 [1:33:09<31:46, 29.79s/it]\n",
      "3.  74%|███████▍  | 177/240 [1:33:15<37:38, 35.85s/it]\n",
      "2.  70%|███████   | 168/240 [1:33:07<33:06, 27.59s/it]\n",
      "1.  68%|██████▊   | 163/240 [1:33:17<37:32, 29.25s/it]\n",
      "0.  74%|███████▍  | 177/240 [1:33:42<32:19, 30.78s/it]\n",
      "3.  74%|███████▍  | 178/240 [1:33:43<34:39, 33.55s/it]\n",
      "2.  70%|███████   | 169/240 [1:33:45<36:26, 30.79s/it]\n",
      "2.  71%|███████   | 170/240 [1:34:23<38:19, 32.85s/it]\n",
      "1.  68%|██████▊   | 164/240 [1:34:00<42:07, 33.26s/it]\n",
      "0.  74%|███████▍  | 178/240 [1:34:03<28:47, 27.87s/it]\n",
      "3.  75%|███████▍  | 179/240 [1:34:16<33:57, 33.41s/it]\n",
      "3.  75%|███████▌  | 180/240 [1:34:49<33:10, 33.17s/it]\n",
      "2.  71%|███████   | 170/240 [1:34:23<38:19, 32.85s/it]\n",
      "1.  69%|██████▉   | 165/240 [1:34:30<40:15, 32.21s/it]\n",
      "0.  75%|███████▍  | 179/240 [1:34:40<31:05, 30.58s/it]\n",
      "0.  75%|███████▌  | 180/240 [1:35:12<31:13, 31.22s/it]\n",
      "3.  75%|███████▌  | 180/240 [1:34:49<33:10, 33.17s/it]\n",
      "1.  69%|██████▉   | 166/240 [1:34:59<38:47, 31.46s/it]\n",
      "2.  71%|███████▏  | 171/240 [1:34:55<37:24, 32.52s/it]\n",
      "3.  75%|███████▌  | 181/240 [1:35:16<30:38, 31.16s/it]\n",
      "0.  75%|███████▌  | 180/240 [1:35:12<31:13, 31.22s/it]\n",
      "2.  72%|███████▏  | 172/240 [1:35:27<36:40, 32.37s/it]\n",
      "1.  70%|██████▉   | 167/240 [1:35:22<35:10, 28.91s/it]\n",
      "0.  75%|███████▌  | 181/240 [1:35:49<32:24, 32.96s/it]\n",
      "1.  70%|███████   | 168/240 [1:36:05<39:44, 33.11s/it]\n",
      "3.  76%|███████▌  | 182/240 [1:35:49<30:38, 31.70s/it]\n",
      "2.  72%|███████▏  | 173/240 [1:36:04<37:55, 33.97s/it]\n",
      "0.  76%|███████▌  | 182/240 [1:36:17<30:20, 31.39s/it]\n",
      "3.  76%|███████▋  | 183/240 [1:36:26<31:39, 33.33s/it]\n",
      "1.  70%|███████   | 169/240 [1:36:26<34:46, 29.39s/it]\n",
      "1.  71%|███████   | 170/240 [1:37:05<37:41, 32.31s/it]\n",
      "2.  72%|███████▎  | 174/240 [1:36:36<36:38, 33.31s/it]\n",
      "0.  76%|███████▋  | 183/240 [1:36:44<28:29, 29.98s/it]\n",
      "3.  77%|███████▋  | 184/240 [1:36:58<30:55, 33.14s/it]\n",
      "2.  73%|███████▎  | 175/240 [1:37:07<35:09, 32.46s/it]\n",
      "0.  77%|███████▋  | 184/240 [1:37:17<28:47, 30.84s/it]\n",
      "1.  71%|███████   | 170/240 [1:37:05<37:41, 32.31s/it]\n",
      "3.  77%|███████▋  | 185/240 [1:37:31<30:11, 32.94s/it]\n",
      "0.  77%|███████▋  | 185/240 [1:37:41<26:21, 28.75s/it]\n",
      "2.  73%|███████▎  | 176/240 [1:37:36<33:45, 31.64s/it]\n",
      "1.  71%|███████▏  | 171/240 [1:37:45<39:43, 34.54s/it]\n",
      "3.  78%|███████▊  | 186/240 [1:38:01<28:46, 31.96s/it]\n",
      "0.  78%|███████▊  | 186/240 [1:38:10<25:55, 28.81s/it]\n",
      "1.  72%|███████▏  | 172/240 [1:38:27<41:49, 36.91s/it]\n",
      "2.  74%|███████▍  | 177/240 [1:38:14<34:59, 33.33s/it]\n",
      "3.  78%|███████▊  | 187/240 [1:38:34<28:30, 32.28s/it]\n",
      "2.  74%|███████▍  | 178/240 [1:38:51<35:48, 34.65s/it]\n",
      "0.  78%|███████▊  | 187/240 [1:38:39<25:36, 28.99s/it]\n",
      "1.  72%|███████▏  | 173/240 [1:38:48<35:47, 32.05s/it]\n",
      "2.  75%|███████▍  | 179/240 [1:39:12<31:00, 30.50s/it]\n",
      "2.  75%|███████▌  | 180/240 [1:39:43<30:36, 30.61s/it]\n",
      "3.  78%|███████▊  | 188/240 [1:39:10<29:09, 33.65s/it]\n",
      "0.  78%|███████▊  | 188/240 [1:39:16<27:07, 31.30s/it]\n",
      "1.  72%|███████▎  | 174/240 [1:39:31<38:48, 35.29s/it]\n",
      "2.  75%|███████▌  | 180/240 [1:39:43<30:36, 30.61s/it]\n",
      "3.  79%|███████▉  | 189/240 [1:39:45<28:47, 33.88s/it]\n",
      "3.  79%|███████▉  | 190/240 [1:40:19<28:24, 34.09s/it]\n",
      "0.  79%|███████▉  | 189/240 [1:39:52<27:58, 32.92s/it]\n",
      "0.  79%|███████▉  | 190/240 [1:40:22<26:34, 31.90s/it]\n",
      "1.  73%|███████▎  | 175/240 [1:40:00<36:26, 33.63s/it]\n",
      "3.  79%|███████▉  | 190/240 [1:40:19<28:24, 34.09s/it]\n",
      "2.  75%|███████▌  | 181/240 [1:40:15<30:28, 31.00s/it]\n",
      "0.  79%|███████▉  | 190/240 [1:40:22<26:34, 31.90s/it]\n",
      "1.  73%|███████▎  | 176/240 [1:40:29<34:17, 32.14s/it]\n",
      "3.  80%|███████▉  | 191/240 [1:40:46<25:56, 31.76s/it]\n",
      "2.  76%|███████▌  | 182/240 [1:40:47<30:14, 31.28s/it]\n",
      "0.  80%|███████▉  | 191/240 [1:40:59<27:15, 33.38s/it]\n",
      "3.  80%|████████  | 192/240 [1:41:20<26:05, 32.62s/it]\n",
      "1.  74%|███████▍  | 177/240 [1:41:09<36:01, 34.31s/it]\n",
      "2.  76%|███████▋  | 183/240 [1:41:25<31:33, 33.23s/it]\n",
      "1.  74%|███████▍  | 178/240 [1:41:51<38:07, 36.90s/it]\n",
      "0.  80%|████████  | 192/240 [1:41:36<27:32, 34.42s/it]\n",
      "3.  80%|████████  | 193/240 [1:41:51<24:59, 31.90s/it]\n",
      "1.  75%|███████▍  | 179/240 [1:42:09<31:30, 30.99s/it]\n",
      "1.  75%|███████▌  | 180/240 [1:42:32<28:34, 28.58s/it]\n",
      "0.  80%|████████  | 193/240 [1:42:08<26:34, 33.93s/it]\n",
      "2.  77%|███████▋  | 184/240 [1:42:02<32:15, 34.55s/it]\n",
      "1.  75%|███████▌  | 180/240 [1:42:32<28:34, 28.58s/it]\n",
      "3.  81%|████████  | 194/240 [1:42:28<25:40, 33.48s/it]\n",
      "2.  77%|███████▋  | 185/240 [1:42:40<32:29, 35.45s/it]\n",
      "0.  81%|████████  | 194/240 [1:42:36<24:36, 32.09s/it]\n",
      "1.  75%|███████▌  | 181/240 [1:42:57<27:16, 27.74s/it]\n",
      "3.  81%|████████▏ | 195/240 [1:43:05<25:55, 34.56s/it]\n",
      "2.  78%|███████▊  | 186/240 [1:43:10<30:24, 33.78s/it]\n",
      "0.  81%|████████▏ | 195/240 [1:43:13<25:11, 33.59s/it]\n",
      "1.  76%|███████▌  | 182/240 [1:43:33<29:03, 30.06s/it]\n",
      "3.  82%|████████▏ | 196/240 [1:43:42<25:53, 35.31s/it]\n",
      "2.  78%|███████▊  | 187/240 [1:43:48<30:55, 35.02s/it]\n",
      "0.  82%|████████▏ | 196/240 [1:43:50<25:24, 34.65s/it]\n",
      "1.  76%|███████▋  | 183/240 [1:44:06<29:21, 30.91s/it]\n",
      "3.  82%|████████▏ | 197/240 [1:44:08<23:22, 32.61s/it]\n",
      "2.  78%|███████▊  | 188/240 [1:44:10<26:58, 31.13s/it]\n",
      "0.  82%|████████▏ | 197/240 [1:44:28<25:27, 35.52s/it]\n",
      "1.  77%|███████▋  | 184/240 [1:44:36<28:33, 30.60s/it]\n",
      "3.  82%|████████▎ | 198/240 [1:44:45<23:44, 33.91s/it]\n",
      "2.  79%|███████▉  | 189/240 [1:44:47<28:03, 33.01s/it]\n",
      "2.  79%|███████▉  | 190/240 [1:45:25<28:41, 34.44s/it]\n",
      "0.  82%|████████▎ | 198/240 [1:45:05<25:06, 35.88s/it]\n",
      "3.  83%|████████▎ | 199/240 [1:45:13<21:58, 32.16s/it]\n",
      "3.  83%|████████▎ | 200/240 [1:45:42<20:49, 31.24s/it]\n",
      "1.  77%|███████▋  | 185/240 [1:45:11<29:16, 31.94s/it]\n",
      "2.  79%|███████▉  | 190/240 [1:45:25<28:41, 34.44s/it]\n",
      "3.  83%|████████▎ | 200/240 [1:45:42<20:49, 31.24s/it]\n",
      "0.  83%|████████▎ | 199/240 [1:45:37<23:47, 34.81s/it]\n",
      "0.  83%|████████▎ | 200/240 [1:46:14<23:40, 35.52s/it]\n",
      "1.  78%|███████▊  | 186/240 [1:45:53<31:28, 34.97s/it]\n",
      "2.  80%|███████▉  | 191/240 [1:46:02<28:53, 35.37s/it]\n",
      "0.  83%|████████▎ | 200/240 [1:46:14<23:40, 35.52s/it]\n",
      "3.  84%|████████▍ | 201/240 [1:46:13<20:06, 30.93s/it]\n",
      "2.  80%|████████  | 192/240 [1:46:23<24:43, 30.90s/it]\n",
      "1.  78%|███████▊  | 187/240 [1:46:22<29:15, 33.13s/it]\n",
      "0.  84%|████████▍ | 201/240 [1:46:38<20:47, 31.99s/it]\n",
      "3.  84%|████████▍ | 202/240 [1:46:42<19:16, 30.43s/it]\n",
      "1.  78%|███████▊  | 188/240 [1:46:57<29:14, 33.73s/it]\n",
      "2.  80%|████████  | 193/240 [1:46:52<23:52, 30.49s/it]\n",
      "0.  84%|████████▍ | 202/240 [1:47:06<19:29, 30.78s/it]\n",
      "3.  85%|████████▍ | 203/240 [1:47:10<18:18, 29.70s/it]\n",
      "2.  81%|████████  | 194/240 [1:47:30<24:55, 32.52s/it]\n",
      "1.  79%|███████▉  | 189/240 [1:47:26<27:36, 32.48s/it]\n",
      "1.  79%|███████▉  | 190/240 [1:47:55<26:07, 31.35s/it]\n",
      "0.  85%|████████▍ | 203/240 [1:47:43<20:04, 32.55s/it]\n",
      "3.  85%|████████▌ | 204/240 [1:47:45<18:45, 31.25s/it]\n",
      "1.  79%|███████▉  | 190/240 [1:47:55<26:07, 31.35s/it]\n",
      "2.  81%|████████▏ | 195/240 [1:47:55<22:40, 30.23s/it]\n",
      "3.  85%|████████▌ | 205/240 [1:48:19<18:46, 32.19s/it]\n",
      "0.  85%|████████▌ | 204/240 [1:48:15<19:31, 32.55s/it]\n",
      "1.  80%|███████▉  | 191/240 [1:48:24<25:08, 30.79s/it]\n",
      "2.  82%|████████▏ | 196/240 [1:48:27<22:34, 30.79s/it]\n",
      "3.  86%|████████▌ | 206/240 [1:48:47<17:33, 30.97s/it]\n",
      "0.  85%|████████▌ | 205/240 [1:48:52<19:47, 33.93s/it]\n",
      "2.  82%|████████▏ | 197/240 [1:49:01<22:46, 31.78s/it]\n",
      "1.  80%|████████  | 192/240 [1:48:57<25:04, 31.35s/it]\n",
      "0.  86%|████████▌ | 206/240 [1:49:22<18:31, 32.70s/it]\n",
      "3.  86%|████████▋ | 207/240 [1:49:22<17:38, 32.07s/it]\n",
      "2.  82%|████████▎ | 198/240 [1:49:30<21:47, 31.14s/it]\n",
      "1.  80%|████████  | 193/240 [1:49:40<27:12, 34.73s/it]\n",
      "0.  86%|████████▋ | 207/240 [1:49:52<17:31, 31.85s/it]\n",
      "3.  87%|████████▋ | 208/240 [1:49:59<17:51, 33.48s/it]\n",
      "2.  83%|████████▎ | 199/240 [1:50:02<21:24, 31.34s/it]\n",
      "2.  83%|████████▎ | 200/240 [1:50:34<21:01, 31.53s/it]\n",
      "3.  87%|████████▋ | 209/240 [1:50:29<16:45, 32.44s/it]\n",
      "3.  88%|████████▊ | 210/240 [1:50:49<14:27, 28.91s/it]\n",
      "1.  81%|████████  | 194/240 [1:50:15<26:41, 34.82s/it]\n",
      "0.  87%|████████▋ | 208/240 [1:50:25<17:09, 32.17s/it]\n",
      "2.  83%|████████▎ | 200/240 [1:50:34<21:01, 31.53s/it]\n",
      "1.  81%|████████▏ | 195/240 [1:50:54<27:04, 36.11s/it]\n",
      "3.  88%|████████▊ | 210/240 [1:50:49<14:27, 28.91s/it]\n",
      "0.  87%|████████▋ | 209/240 [1:51:02<17:20, 33.57s/it]\n",
      "0.  88%|████████▊ | 210/240 [1:51:39<17:20, 34.70s/it]\n",
      "1.  82%|████████▏ | 196/240 [1:51:15<23:04, 31.47s/it]\n",
      "2.  84%|████████▍ | 201/240 [1:51:12<21:38, 33.29s/it]\n",
      "3.  88%|████████▊ | 211/240 [1:51:24<14:44, 30.52s/it]\n",
      "0.  88%|████████▊ | 210/240 [1:51:39<17:20, 34.70s/it]\n",
      "1.  82%|████████▏ | 197/240 [1:51:47<22:45, 31.75s/it]\n",
      "2.  84%|████████▍ | 202/240 [1:51:49<21:48, 34.42s/it]\n",
      "3.  88%|████████▊ | 212/240 [1:52:00<15:08, 32.43s/it]\n",
      "0.  88%|████████▊ | 211/240 [1:52:16<17:03, 35.30s/it]\n",
      "1.  82%|████████▎ | 198/240 [1:52:22<22:58, 32.81s/it]\n",
      "2.  85%|████████▍ | 203/240 [1:52:26<21:48, 35.38s/it]\n",
      "3.  89%|████████▉ | 213/240 [1:52:30<14:15, 31.68s/it]\n",
      "3.  89%|████████▉ | 214/240 [1:53:07<14:25, 33.29s/it]\n",
      "0.  88%|████████▊ | 212/240 [1:52:53<16:42, 35.80s/it]\n",
      "2.  85%|████████▌ | 204/240 [1:53:04<21:43, 36.21s/it]\n",
      "1.  83%|████████▎ | 199/240 [1:53:05<24:23, 35.69s/it]\n",
      "1.  83%|████████▎ | 200/240 [1:53:44<24:33, 36.84s/it]\n",
      "0.  89%|████████▉ | 213/240 [1:53:30<16:16, 36.17s/it]\n",
      "3.  90%|████████▉ | 215/240 [1:53:29<12:22, 29.71s/it]\n",
      "2.  85%|████████▌ | 205/240 [1:53:39<20:52, 35.80s/it]\n",
      "1.  83%|████████▎ | 200/240 [1:53:44<24:33, 36.84s/it]\n",
      "0.  89%|████████▉ | 214/240 [1:53:57<14:29, 33.45s/it]\n",
      "3.  90%|█████████ | 216/240 [1:53:59<11:55, 29.82s/it]\n",
      "2.  86%|████████▌ | 206/240 [1:54:14<20:01, 35.34s/it]\n",
      "0.  90%|████████▉ | 215/240 [1:54:24<13:12, 31.70s/it]\n",
      "1.  84%|████████▍ | 201/240 [1:54:24<24:28, 37.66s/it]\n",
      "3.  90%|█████████ | 217/240 [1:54:29<11:25, 29.81s/it]\n",
      "2.  86%|████████▋ | 207/240 [1:54:38<17:42, 32.19s/it]\n",
      "3.  91%|█████████ | 218/240 [1:55:01<11:16, 30.74s/it]\n",
      "0.  90%|█████████ | 216/240 [1:54:51<12:07, 30.29s/it]\n",
      "1.  84%|████████▍ | 202/240 [1:54:56<22:53, 36.14s/it]\n",
      "2.  87%|████████▋ | 208/240 [1:55:09<16:58, 31.81s/it]\n",
      "0.  90%|█████████ | 217/240 [1:55:29<12:25, 32.42s/it]\n",
      "3.  91%|█████████▏| 219/240 [1:55:30<10:30, 30.02s/it]\n",
      "3.  92%|█████████▏| 220/240 [1:56:07<10:43, 32.18s/it]\n",
      "1.  85%|████████▍ | 203/240 [1:55:35<22:50, 37.03s/it]\n",
      "2.  87%|████████▋ | 209/240 [1:55:47<17:20, 33.57s/it]\n",
      "2.  88%|████████▊ | 210/240 [1:56:25<17:24, 34.82s/it]\n",
      "3.  92%|█████████▏| 220/240 [1:56:07<10:43, 32.18s/it]\n",
      "1.  85%|████████▌ | 204/240 [1:56:11<21:52, 36.47s/it]\n",
      "0.  91%|█████████ | 218/240 [1:56:06<12:23, 33.78s/it]\n",
      "2.  88%|████████▊ | 210/240 [1:56:25<17:24, 34.82s/it]\n",
      "1.  85%|████████▌ | 205/240 [1:56:36<19:24, 33.26s/it]\n",
      "3.  92%|█████████▏| 221/240 [1:56:35<09:47, 30.92s/it]\n",
      "0.  91%|█████████▏| 219/240 [1:56:43<12:11, 34.81s/it]\n",
      "0.  92%|█████████▏| 220/240 [1:57:16<11:22, 34.13s/it]\n",
      "2.  88%|████████▊ | 211/240 [1:56:58<16:40, 34.49s/it]\n",
      "3.  92%|█████████▎| 222/240 [1:57:08<09:25, 31.43s/it]\n",
      "1.  86%|████████▌ | 206/240 [1:57:06<18:08, 32.03s/it]\n",
      "0.  92%|█████████▏| 220/240 [1:57:16<11:22, 34.13s/it]\n",
      "2.  88%|████████▊ | 212/240 [1:57:32<16:01, 34.34s/it]\n",
      "3.  93%|█████████▎| 223/240 [1:57:44<09:20, 32.96s/it]\n",
      "0.  92%|█████████▏| 221/240 [1:57:49<10:42, 33.80s/it]\n",
      "1.  86%|████████▋ | 207/240 [1:57:48<19:24, 35.29s/it]\n",
      "2.  89%|████████▉ | 213/240 [1:57:54<13:40, 30.37s/it]\n",
      "3.  93%|█████████▎| 224/240 [1:58:11<08:15, 30.97s/it]\n",
      "0.  92%|█████████▎| 222/240 [1:58:21<10:03, 33.52s/it]\n",
      "1.  87%|████████▋ | 208/240 [1:58:28<19:27, 36.50s/it]\n",
      "2.  89%|████████▉ | 214/240 [1:58:31<14:04, 32.47s/it]\n",
      "3.  94%|█████████▍| 225/240 [1:58:31<06:58, 27.88s/it]\n",
      "0.  93%|█████████▎| 223/240 [1:58:51<09:10, 32.36s/it]\n",
      "1.  87%|████████▋ | 209/240 [1:59:02<18:34, 35.96s/it]\n",
      "1.  88%|████████▊ | 210/240 [1:59:37<17:50, 35.67s/it]\n",
      "3.  94%|█████████▍| 226/240 [1:59:04<06:50, 29.31s/it]\n",
      "2.  90%|████████▉ | 215/240 [1:59:03<13:26, 32.26s/it]\n",
      "0.  93%|█████████▎| 224/240 [1:59:28<09:00, 33.80s/it]\n",
      "1.  88%|████████▊ | 210/240 [1:59:37<17:50, 35.67s/it]\n",
      "3.  95%|█████████▍| 227/240 [1:59:38<06:41, 30.88s/it]\n",
      "2.  90%|█████████ | 216/240 [1:59:40<13:32, 33.85s/it]\n",
      "0.  94%|█████████▍| 225/240 [2:00:05<08:41, 34.78s/it]\n",
      "2.  90%|█████████ | 217/240 [2:00:18<13:24, 35.00s/it]\n",
      "3.  95%|█████████▌| 228/240 [2:00:11<06:17, 31.44s/it]\n",
      "1.  88%|████████▊ | 211/240 [2:00:10<16:45, 34.66s/it]\n",
      "0.  94%|█████████▍| 226/240 [2:00:42<08:16, 35.48s/it]\n",
      "3.  95%|█████████▌| 229/240 [2:00:49<06:06, 33.29s/it]\n",
      "3.  96%|█████████▌| 230/240 [2:01:19<05:23, 32.35s/it]\n",
      "2.  91%|█████████ | 218/240 [2:00:48<12:18, 33.56s/it]\n",
      "1.  88%|████████▊ | 212/240 [2:00:52<17:16, 37.02s/it]\n",
      "0.  95%|█████████▍| 227/240 [2:01:04<06:45, 31.22s/it]\n",
      "3.  96%|█████████▌| 230/240 [2:01:19<05:23, 32.35s/it]\n",
      "2.  91%|█████████▏| 219/240 [2:01:22<11:47, 33.68s/it]\n",
      "2.  92%|█████████▏| 220/240 [2:01:56<11:16, 33.81s/it]\n",
      "1.  89%|████████▉ | 213/240 [2:01:35<17:22, 38.60s/it]\n",
      "0.  95%|█████████▌| 228/240 [2:01:41<06:35, 32.94s/it]\n",
      "3.  96%|█████████▋| 231/240 [2:01:56<05:03, 33.74s/it]\n",
      "2.  92%|█████████▏| 220/240 [2:01:56<11:16, 33.81s/it]\n",
      "0.  95%|█████████▌| 229/240 [2:02:18<06:16, 34.27s/it]\n",
      "0.  96%|█████████▌| 230/240 [2:02:51<05:37, 33.76s/it]\n",
      "1.  89%|████████▉ | 214/240 [2:02:14<16:48, 38.78s/it]\n",
      "3.  97%|█████████▋| 232/240 [2:02:24<04:16, 32.09s/it]\n",
      "2.  92%|█████████▏| 221/240 [2:02:34<11:05, 35.01s/it]\n",
      "0.  96%|█████████▌| 230/240 [2:02:51<05:37, 33.76s/it]\n",
      "3.  97%|█████████▋| 233/240 [2:03:01<03:54, 33.48s/it]\n",
      "1.  90%|████████▉ | 215/240 [2:02:56<16:37, 39.90s/it]\n",
      "2.  92%|█████████▎| 222/240 [2:03:06<10:16, 34.23s/it]\n",
      "0.  96%|█████████▋| 231/240 [2:03:27<05:11, 34.65s/it]\n",
      "2.  93%|█████████▎| 223/240 [2:03:38<09:30, 33.55s/it]\n",
      "3.  98%|█████████▊| 234/240 [2:03:29<03:11, 31.84s/it]\n",
      "1.  90%|█████████ | 216/240 [2:03:36<15:54, 39.78s/it]\n",
      "0.  97%|█████████▋| 232/240 [2:03:55<04:19, 32.47s/it]\n",
      "3.  98%|█████████▊| 235/240 [2:04:03<02:43, 32.68s/it]\n",
      "2.  93%|█████████▎| 224/240 [2:04:03<08:15, 30.96s/it]\n",
      "1.  90%|█████████ | 217/240 [2:04:15<15:11, 39.65s/it]\n",
      "0.  97%|█████████▋| 233/240 [2:04:27<03:47, 32.51s/it]\n",
      "3.  98%|█████████▊| 236/240 [2:04:30<02:03, 30.78s/it]\n",
      "2.  94%|█████████▍| 225/240 [2:04:33<07:40, 30.70s/it]\n",
      "1.  91%|█████████ | 218/240 [2:04:50<14:02, 38.29s/it]\n",
      "0.  98%|█████████▊| 234/240 [2:04:54<03:04, 30.81s/it]\n",
      "2.  94%|█████████▍| 226/240 [2:05:11<07:37, 32.70s/it]\n",
      "3.  99%|█████████▉| 237/240 [2:05:07<01:37, 32.62s/it]\n",
      "1.  91%|█████████▏| 219/240 [2:05:19<12:23, 35.40s/it]\n",
      "1.  92%|█████████▏| 220/240 [2:06:02<12:34, 37.74s/it]\n",
      "0.  98%|█████████▊| 235/240 [2:05:31<02:43, 32.67s/it]\n",
      "3.  99%|█████████▉| 238/240 [2:05:44<01:07, 33.94s/it]\n",
      "2.  95%|█████████▍| 227/240 [2:05:41<06:56, 32.03s/it]\n",
      "0.  98%|█████████▊| 236/240 [2:06:08<02:15, 33.95s/it]\n",
      "3. 100%|█████████▉| 239/240 [2:06:18<00:34, 34.09s/it]\n",
      "3. 100%|██████████| 240/240 [2:06:38<00:00, 29.89s/it]\n",
      "3. 100%|██████████| 240/240 [2:06:38<00:00, 29.89s/it]\n",
      "3. 100%|██████████| 240/240 [2:06:38<00:00, 29.89s/it]\n",
      "3. 100%|██████████| 240/240 [2:06:38<00:00, 31.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. {'loss': 0.1501, 'grad_norm': 0.7867273688316345, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.08}\n",
      "3. {'loss': 0.1056, 'grad_norm': 1.294529914855957, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.17}\n",
      "3. {'loss': 0.0681, 'grad_norm': 0.8311694860458374, 'learning_rate': 6.25e-05, 'epoch': 0.25}\n",
      "3. {'loss': 0.0698, 'grad_norm': 0.7124578356742859, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.33}\n",
      "3. {'loss': 0.0509, 'grad_norm': 0.7246041297912598, 'learning_rate': 9.997322937381829e-05, 'epoch': 0.42}\n",
      "3. {'loss': 0.0485, 'grad_norm': 0.9762706160545349, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.5}\n",
      "3. {'loss': 0.0392, 'grad_norm': 0.4533740282058716, 'learning_rate': 9.67952963378663e-05, 'epoch': 0.58}\n",
      "3. {'loss': 0.0281, 'grad_norm': 0.4865405857563019, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.67}\n",
      "3. {'loss': 0.0293, 'grad_norm': 0.7114354372024536, 'learning_rate': 8.865052266813685e-05, 'epoch': 0.75}\n",
      "3. {'loss': 0.023, 'grad_norm': 0.8910411596298218, 'learning_rate': 8.296729075500344e-05, 'epoch': 0.83}\n",
      "3. {'loss': 0.0258, 'grad_norm': 0.4354704022407532, 'learning_rate': 7.64033925325184e-05, 'epoch': 0.92}\n",
      "3. {'loss': 0.0298, 'grad_norm': 0.9789188504219055, 'learning_rate': 6.91341716182545e-05, 'epoch': 1.0}\n",
      "3. {'loss': 0.0098, 'grad_norm': 0.2985602021217346, 'learning_rate': 6.135381315171867e-05, 'epoch': 1.08}\n",
      "3. {'loss': 0.0109, 'grad_norm': 0.40559035539627075, 'learning_rate': 5.327015646150716e-05, 'epoch': 1.17}\n",
      "3. {'loss': 0.0131, 'grad_norm': 0.21007056534290314, 'learning_rate': 4.509914298352197e-05, 'epoch': 1.25}\n",
      "3. {'loss': 0.0074, 'grad_norm': 0.4455797076225281, 'learning_rate': 3.705904774487396e-05, 'epoch': 1.33}\n",
      "3. {'loss': 0.0086, 'grad_norm': 0.8013473153114319, 'learning_rate': 2.936464850978027e-05, 'epoch': 1.42}\n",
      "3. {'loss': 0.0068, 'grad_norm': 0.4444386661052704, 'learning_rate': 2.2221488349019903e-05, 'epoch': 1.5}\n",
      "3. {'loss': 0.0054, 'grad_norm': 0.32134726643562317, 'learning_rate': 1.5820384898856434e-05, 'epoch': 1.58}\n",
      "3. {'loss': 0.0057, 'grad_norm': 0.45806005597114563, 'learning_rate': 1.0332332985438248e-05, 'epoch': 1.67}\n",
      "3. {'loss': 0.0054, 'grad_norm': 1.3809828758239746, 'learning_rate': 5.903936782582253e-06, 'epoch': 1.75}\n",
      "3. {'loss': 0.0048, 'grad_norm': 0.1885480135679245, 'learning_rate': 2.653493525244721e-06, 'epoch': 1.83}\n",
      "3. {'loss': 0.0046, 'grad_norm': 0.4249493181705475, 'learning_rate': 6.678333957560512e-07, 'epoch': 1.92}\n",
      "3. {'loss': 0.005, 'grad_norm': 0.39725103974342346, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "3. {'train_runtime': 7598.7902, 'train_samples_per_second': 0.253, 'train_steps_per_second': 0.032, 'train_loss': 0.03148957515756289, 'epoch': 2.0}\n",
      "3. *** -> Training took 7598.7902 seconds.\n",
      "3. *** Saving model/tokenizer to '/kaggle/temp/finetuned_model_gpu3'...\n",
      "3. *** GPU: NVIDIA L4, used 18.5 / 22.3 GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.  92%|█████████▏| 220/240 [2:06:02<12:34, 37.74s/it]\n",
      "2.  95%|█████████▌| 228/240 [2:06:19<06:44, 33.72s/it]\n",
      "7. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "7. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "7. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "7. *** Load base model and tokenizer from '/kaggle/temp/finetuned_model_gpu3'...\n",
      "7. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "7. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "7.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "7. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "7. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "7.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "7. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "7. Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.69s/it]\n",
      "7. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.65s/it]\n",
      "7. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.59s/it]\n",
      "7. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.61s/it]\n",
      "7. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n",
      "0.  99%|█████████▉| 237/240 [2:06:36<01:36, 32.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Augment dataset...\n",
      "7. 704\n",
      "7. *** Load peft state_dict from '/kaggle/temp/finetuned_model_gpu3'...\n",
      "7. *** Load stored data...\n",
      "7. *** Start inference run...\n",
      "7.   0%|          | 0/30 [00:00<?, ?it/s]retraining model for key '16b78196' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 483.92 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 477.29 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.  92%|█████████▏| 221/240 [2:06:45<12:25, 39.24s/it]\n",
      "2.  95%|█████████▌| 229/240 [2:06:56<06:23, 34.89s/it]\n",
      "2.  96%|█████████▌| 230/240 [2:07:34<05:57, 35.72s/it]\n",
      "0.  99%|█████████▉| 238/240 [2:07:13<01:06, 33.49s/it]\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "2.  96%|█████████▌| 230/240 [2:07:34<05:57, 35.72s/it]\n",
      "1.  92%|█████████▎| 222/240 [2:07:28<12:06, 40.39s/it]\n",
      "0. 100%|█████████▉| 239/240 [2:07:41<00:31, 31.96s/it]\n",
      "0. 100%|██████████| 240/240 [2:08:18<00:00, 33.53s/it]\n",
      "0. 100%|██████████| 240/240 [2:08:18<00:00, 33.53s/it]\n",
      "0. 100%|██████████| 240/240 [2:08:18<00:00, 33.53s/it]\n",
      "0. 100%|██████████| 240/240 [2:08:18<00:00, 32.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. {'loss': 0.1471, 'grad_norm': 1.2236523628234863, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.08}\n",
      "0. {'loss': 0.0945, 'grad_norm': 0.9049831032752991, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.17}\n",
      "0. {'loss': 0.0848, 'grad_norm': 0.8789643049240112, 'learning_rate': 6.25e-05, 'epoch': 0.25}\n",
      "0. {'loss': 0.0763, 'grad_norm': 0.7441339492797852, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.33}\n",
      "0. {'loss': 0.0504, 'grad_norm': 0.7478144764900208, 'learning_rate': 9.997322937381829e-05, 'epoch': 0.42}\n",
      "0. {'loss': 0.0459, 'grad_norm': 0.8582068681716919, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.5}\n",
      "0. {'loss': 0.0501, 'grad_norm': 0.6952024698257446, 'learning_rate': 9.67952963378663e-05, 'epoch': 0.58}\n",
      "0. {'loss': 0.0349, 'grad_norm': 1.5011838674545288, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.67}\n",
      "0. {'loss': 0.027, 'grad_norm': 0.5692598223686218, 'learning_rate': 8.865052266813685e-05, 'epoch': 0.75}\n",
      "0. {'loss': 0.0313, 'grad_norm': 0.42170265316963196, 'learning_rate': 8.296729075500344e-05, 'epoch': 0.83}\n",
      "0. {'loss': 0.0265, 'grad_norm': 1.03132963180542, 'learning_rate': 7.64033925325184e-05, 'epoch': 0.92}\n",
      "0. {'loss': 0.0191, 'grad_norm': 0.41755813360214233, 'learning_rate': 6.91341716182545e-05, 'epoch': 1.0}\n",
      "0. {'loss': 0.0084, 'grad_norm': 0.5484284162521362, 'learning_rate': 6.135381315171867e-05, 'epoch': 1.08}\n",
      "0. {'loss': 0.0116, 'grad_norm': 0.40474632382392883, 'learning_rate': 5.327015646150716e-05, 'epoch': 1.17}\n",
      "0. {'loss': 0.0084, 'grad_norm': 0.7845310568809509, 'learning_rate': 4.509914298352197e-05, 'epoch': 1.25}\n",
      "0. {'loss': 0.0068, 'grad_norm': 0.4088945984840393, 'learning_rate': 3.705904774487396e-05, 'epoch': 1.33}\n",
      "0. {'loss': 0.0078, 'grad_norm': 0.6372847557067871, 'learning_rate': 2.936464850978027e-05, 'epoch': 1.42}\n",
      "0. {'loss': 0.0098, 'grad_norm': 0.36856457591056824, 'learning_rate': 2.2221488349019903e-05, 'epoch': 1.5}\n",
      "0. {'loss': 0.0053, 'grad_norm': 0.43737855553627014, 'learning_rate': 1.5820384898856434e-05, 'epoch': 1.58}\n",
      "0. {'loss': 0.0044, 'grad_norm': 0.1269257515668869, 'learning_rate': 1.0332332985438248e-05, 'epoch': 1.67}\n",
      "0. {'loss': 0.0058, 'grad_norm': 0.15615013241767883, 'learning_rate': 5.903936782582253e-06, 'epoch': 1.75}\n",
      "0. {'loss': 0.0041, 'grad_norm': 0.3061184287071228, 'learning_rate': 2.653493525244721e-06, 'epoch': 1.83}\n",
      "0. {'loss': 0.0032, 'grad_norm': 0.338888555765152, 'learning_rate': 6.678333957560512e-07, 'epoch': 1.92}\n",
      "0. {'loss': 0.0049, 'grad_norm': 0.25018763542175293, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "0. {'train_runtime': 7698.6426, 'train_samples_per_second': 0.249, 'train_steps_per_second': 0.031, 'train_loss': 0.032014706156527004, 'epoch': 2.0}\n",
      "0. *** -> Training took 7698.6426 seconds.\n",
      "0. *** Saving model/tokenizer to '/kaggle/temp/finetuned_model_gpu0'...\n",
      "0. *** GPU: NVIDIA L4, used 18.6 / 22.3 GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "4. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "4. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "4. *** Load base model and tokenizer from '/kaggle/temp/finetuned_model_gpu0'...\n",
      "4. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "4. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "4.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "4. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "4. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "4.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "4. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "4. Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.65s/it]\n",
      "2.  96%|█████████▋| 231/240 [2:08:06<05:11, 34.61s/it]\n",
      "4. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.64s/it]\n",
      "4. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.58s/it]\n",
      "4. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.60s/it]\n",
      "1.  93%|█████████▎| 223/240 [2:08:08<11:22, 40.15s/it]\n",
      "4. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n",
      "7.   6%|▋         | 1/16 [00:40<10:04, 40.31s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Augment dataset...\n",
      "4. 688\n",
      "4. *** Load peft state_dict from '/kaggle/temp/finetuned_model_gpu0'...\n",
      "4. *** Load stored data...\n",
      "4. *** Start inference run...\n",
      "4.   0%|          | 0/30 [00:00<?, ?it/s]retraining model for key '1ae2feb7' (retrain_dataset_size=15)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1288.69 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "2.  97%|█████████▋| 232/240 [2:08:37<04:27, 33.40s/it]\n",
      "1.  93%|█████████▎| 224/240 [2:08:40<10:06, 37.89s/it]\n",
      "4.   6%|▋         | 1/16 [00:13<03:27, 13.84s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [01:19<09:12, 39.47s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:26<03:00, 12.89s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:38<02:43, 12.59s/it]\u001b[A\n",
      "2.  97%|█████████▋| 233/240 [2:09:07<03:48, 32.57s/it]\n",
      "4.  25%|██▌       | 4/16 [00:50<02:29, 12.46s/it]\u001b[A\n",
      "1.  94%|█████████▍| 225/240 [2:09:15<09:15, 37.02s/it]\n",
      "7.  19%|█▉        | 3/16 [01:58<08:30, 39.30s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:02<02:16, 12.39s/it]\u001b[A\n",
      "2.  98%|█████████▊| 234/240 [2:09:45<03:24, 34.17s/it]\n",
      "4.  38%|███▊      | 6/16 [01:15<02:03, 12.37s/it]\u001b[A\n",
      "1.  94%|█████████▍| 226/240 [2:09:58<09:00, 38.64s/it]\n",
      "4.  44%|████▍     | 7/16 [01:27<01:51, 12.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0097, 'grad_norm': 0.22342471778392792, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:39<01:38, 12.34s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  25%|██▌       | 4/16 [02:37<07:51, 39.27s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [01:39<01:38, 12.34s/it]\u001b[A\n",
      "2.  98%|█████████▊| 235/240 [2:10:16<02:45, 33.02s/it]\n",
      "4.  56%|█████▋    | 9/16 [01:52<01:26, 12.34s/it]\u001b[A\n",
      "2.  98%|█████████▊| 236/240 [2:10:46<02:08, 32.24s/it]\n",
      "1.  95%|█████████▍| 227/240 [2:10:26<07:44, 35.72s/it]\n",
      "4.  62%|██████▎   | 10/16 [02:04<01:14, 12.34s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [03:16<07:11, 39.27s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [02:16<01:01, 12.35s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [02:29<00:49, 12.36s/it]\u001b[A\n",
      "2.  99%|█████████▉| 237/240 [2:11:07<01:26, 28.88s/it]\n",
      "4.  81%|████████▏ | 13/16 [02:41<00:37, 12.35s/it]\u001b[A\n",
      "1.  95%|█████████▌| 228/240 [2:11:09<07:32, 37.70s/it]\n",
      "4.  88%|████████▊ | 14/16 [02:53<00:24, 12.34s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [03:56<06:33, 39.31s/it]\u001b[A\n",
      "1.  95%|█████████▌| 229/240 [2:11:48<07:00, 38.26s/it]\n",
      "1.  96%|█████████▌| 230/240 [2:12:06<05:19, 31.94s/it]\n",
      "4.  94%|█████████▍| 15/16 [03:06<00:12, 12.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/30 [01:43<?, ?it/s]\n",
      "4. {'loss': 0.001, 'grad_norm': 0.5332469940185547, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.   0%|          | 0/30 [03:21<?, ?it/s]\n",
      "4. {'train_runtime': 198.5782, 'train_samples_per_second': 0.645, 'train_steps_per_second': 0.081, 'train_loss': 0.0053152337204664946, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [03:18<00:00, 12.34s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [03:18<00:00, 12.34s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [03:18<00:00, 12.34s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [03:18<00:00, 12.41s/it]\n",
      "2.  99%|█████████▉| 238/240 [2:11:45<01:03, 31.53s/it]\n",
      "1.  96%|█████████▌| 230/240 [2:12:06<05:19, 31.94s/it]\n",
      "7.  44%|████▍     | 7/16 [04:35<05:54, 39.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0026, 'grad_norm': 0.17885564267635345, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [05:15<05:15, 39.38s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "2. 100%|█████████▉| 239/240 [2:12:15<00:31, 31.24s/it]\n",
      "2. 100%|██████████| 240/240 [2:12:53<00:00, 33.16s/it]\n",
      "2. 100%|██████████| 240/240 [2:12:53<00:00, 33.16s/it]\n",
      "2. 100%|██████████| 240/240 [2:12:53<00:00, 33.16s/it]\n",
      "2. 100%|██████████| 240/240 [2:12:53<00:00, 33.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. {'loss': 0.0907, 'grad_norm': 1.0151928663253784, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.08}\n",
      "2. {'loss': 0.0721, 'grad_norm': 0.8373411297798157, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.17}\n",
      "2. {'loss': 0.0502, 'grad_norm': 0.46275344491004944, 'learning_rate': 6.25e-05, 'epoch': 0.25}\n",
      "2. {'loss': 0.0587, 'grad_norm': 1.2480559349060059, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.33}\n",
      "2. {'loss': 0.0378, 'grad_norm': 0.8364644050598145, 'learning_rate': 9.997322937381829e-05, 'epoch': 0.42}\n",
      "2. {'loss': 0.0347, 'grad_norm': 0.7663957476615906, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.5}\n",
      "2. {'loss': 0.0348, 'grad_norm': 0.8123700618743896, 'learning_rate': 9.67952963378663e-05, 'epoch': 0.58}\n",
      "2. {'loss': 0.0246, 'grad_norm': 0.4959016740322113, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.67}\n",
      "2. {'loss': 0.0259, 'grad_norm': 0.7990118265151978, 'learning_rate': 8.865052266813685e-05, 'epoch': 0.75}\n",
      "2. {'loss': 0.0196, 'grad_norm': 0.7071285247802734, 'learning_rate': 8.296729075500344e-05, 'epoch': 0.83}\n",
      "2. {'loss': 0.0179, 'grad_norm': 0.5766162276268005, 'learning_rate': 7.64033925325184e-05, 'epoch': 0.92}\n",
      "2. {'loss': 0.0239, 'grad_norm': 0.41299301385879517, 'learning_rate': 6.91341716182545e-05, 'epoch': 1.0}\n",
      "2. {'loss': 0.0119, 'grad_norm': 0.266673743724823, 'learning_rate': 6.135381315171867e-05, 'epoch': 1.08}\n",
      "2. {'loss': 0.006, 'grad_norm': 0.20020362734794617, 'learning_rate': 5.327015646150716e-05, 'epoch': 1.17}\n",
      "2. {'loss': 0.0053, 'grad_norm': 0.29246512055397034, 'learning_rate': 4.509914298352197e-05, 'epoch': 1.25}\n",
      "2. {'loss': 0.0082, 'grad_norm': 0.73838871717453, 'learning_rate': 3.705904774487396e-05, 'epoch': 1.33}\n",
      "2. {'loss': 0.0077, 'grad_norm': 0.32453450560569763, 'learning_rate': 2.936464850978027e-05, 'epoch': 1.42}\n",
      "2. {'loss': 0.0076, 'grad_norm': 0.23671536147594452, 'learning_rate': 2.2221488349019903e-05, 'epoch': 1.5}\n",
      "2. {'loss': 0.0046, 'grad_norm': 0.20022030174732208, 'learning_rate': 1.5820384898856434e-05, 'epoch': 1.58}\n",
      "2. {'loss': 0.0059, 'grad_norm': 0.3681850731372833, 'learning_rate': 1.0332332985438248e-05, 'epoch': 1.67}\n",
      "2. {'loss': 0.004, 'grad_norm': 0.15471573173999786, 'learning_rate': 5.903936782582253e-06, 'epoch': 1.75}\n",
      "2. {'loss': 0.0044, 'grad_norm': 0.4664885699748993, 'learning_rate': 2.653493525244721e-06, 'epoch': 1.83}\n",
      "2. {'loss': 0.0043, 'grad_norm': 0.22157369554042816, 'learning_rate': 6.678333957560512e-07, 'epoch': 1.92}\n",
      "2. {'loss': 0.0029, 'grad_norm': 0.38928714394569397, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "2. {'train_runtime': 7973.5023, 'train_samples_per_second': 0.241, 'train_steps_per_second': 0.03, 'train_loss': 0.02348911192578574, 'epoch': 2.0}\n",
      "2. *** -> Training took 7973.5023 seconds.\n",
      "2. *** Saving model/tokenizer to '/kaggle/temp/finetuned_model_gpu2'...\n",
      "2. *** GPU: NVIDIA L4, used 18.5 / 22.3 GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "6. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "6. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "6. *** Load base model and tokenizer from '/kaggle/temp/finetuned_model_gpu2'...\n",
      "6. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "6. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "6.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "6. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "6. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "6.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "6. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "6. Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.65s/it]\n",
      "6. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.63s/it]\n",
      "6. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.57s/it]\n",
      "6. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.59s/it]\n",
      "6. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n",
      "1.  96%|█████████▋| 231/240 [2:12:38<04:49, 32.15s/it]\n",
      "7.  50%|█████     | 8/16 [05:15<05:15, 39.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Augment dataset...\n",
      "6. 672\n",
      "6. *** Load peft state_dict from '/kaggle/temp/finetuned_model_gpu2'...\n",
      "6. *** Load stored data...\n",
      "6. *** Start inference run...\n",
      "6.   0%|          | 0/30 [00:00<?, ?it/s]retraining model for key '135a2760' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1363.25 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "1.  97%|█████████▋| 232/240 [2:13:13<04:23, 32.97s/it]\n",
      "6.   6%|▋         | 1/16 [00:13<03:22, 13.52s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [05:54<04:36, 39.43s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:25<02:55, 12.56s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:37<02:39, 12.29s/it]\u001b[A\n",
      "1.  97%|█████████▋| 233/240 [2:13:42<03:41, 31.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/30 [03:21<?, ?it/s]*** -> Training took 198.5782 seconds.\n",
      "4.   3%|▎         | 1/30 [05:28<2:38:35, 328.11s/it]retraining model for key '269e22fb' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 598.18 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 589.77 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  25%|██▌       | 4/16 [00:49<02:25, 12.15s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:01<02:12, 12.08s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [06:34<03:57, 39.66s/it]\u001b[A\n",
      "1.  98%|█████████▊| 234/240 [2:14:17<03:15, 32.64s/it]\n",
      "6.  38%|███▊      | 6/16 [01:13<02:00, 12.04s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:25<01:48, 12.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0012, 'grad_norm': 0.05094897001981735, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:37<01:36, 12.05s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6.  50%|█████     | 8/16 [01:37<01:36, 12.05s/it]\u001b[A\n",
      "1.  98%|█████████▊| 235/240 [2:14:42<02:32, 30.58s/it]\n",
      "4.   6%|▋         | 1/16 [00:28<07:02, 28.16s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [07:14<03:18, 39.73s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:49<01:24, 12.07s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:01<01:12, 12.08s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:56<06:33, 28.10s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [02:13<01:00, 12.10s/it]\u001b[A\n",
      "1.  98%|█████████▊| 236/240 [2:15:17<02:07, 31.95s/it]\n",
      "7.  75%|███████▌  | 12/16 [07:54<02:38, 39.74s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:25<00:48, 12.10s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [02:37<00:36, 12.12s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:24<06:05, 28.11s/it]\u001b[A\n",
      "1.  99%|█████████▉| 237/240 [2:15:53<01:38, 32.92s/it]\n",
      "6.  88%|████████▊ | 14/16 [02:49<00:24, 12.08s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [08:34<01:59, 39.74s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [03:02<00:12, 12.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/30 [01:40<?, ?it/s]\n",
      "6. {'loss': 0.0012, 'grad_norm': 0.822670578956604, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.   0%|          | 0/30 [03:17<?, ?it/s]\n",
      "6. {'train_runtime': 194.1493, 'train_samples_per_second': 0.659, 'train_steps_per_second': 0.082, 'train_loss': 0.001207019027788192, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [03:14<00:00, 12.08s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:14<00:00, 12.08s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:14<00:00, 12.08s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [03:14<00:00, 12.13s/it]\n",
      "4.  25%|██▌       | 4/16 [01:52<05:37, 28.13s/it]\u001b[A\n",
      "1.  99%|█████████▉| 238/240 [2:16:17<01:00, 30.30s/it]\n",
      "4.  31%|███▏      | 5/16 [02:20<05:09, 28.18s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [09:13<01:19, 39.72s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [02:49<04:42, 28.20s/it]\u001b[A\n",
      "1. 100%|█████████▉| 239/240 [2:17:00<00:34, 34.07s/it]\n",
      "1. 100%|██████████| 240/240 [2:17:42<00:00, 36.50s/it]\n",
      "1. 100%|██████████| 240/240 [2:17:42<00:00, 36.50s/it]\n",
      "1. 100%|██████████| 240/240 [2:17:42<00:00, 36.50s/it]\n",
      "1. 100%|██████████| 240/240 [2:17:42<00:00, 34.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. {'loss': 0.1341, 'grad_norm': 2.360454797744751, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.08}\n",
      "1. {'loss': 0.0941, 'grad_norm': 0.7104193568229675, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.17}\n",
      "1. {'loss': 0.0759, 'grad_norm': 0.6203359365463257, 'learning_rate': 6.25e-05, 'epoch': 0.25}\n",
      "1. {'loss': 0.0601, 'grad_norm': 0.7786904573440552, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.33}\n",
      "1. {'loss': 0.0543, 'grad_norm': 1.0464279651641846, 'learning_rate': 9.997322937381829e-05, 'epoch': 0.42}\n",
      "1. {'loss': 0.0387, 'grad_norm': 0.722430944442749, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.5}\n",
      "1. {'loss': 0.0411, 'grad_norm': 0.8128759264945984, 'learning_rate': 9.67952963378663e-05, 'epoch': 0.58}\n",
      "1. {'loss': 0.0309, 'grad_norm': 0.5200320482254028, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.67}\n",
      "1. {'loss': 0.0255, 'grad_norm': 0.5879812240600586, 'learning_rate': 8.865052266813685e-05, 'epoch': 0.75}\n",
      "1. {'loss': 0.025, 'grad_norm': 0.5690993666648865, 'learning_rate': 8.296729075500344e-05, 'epoch': 0.83}\n",
      "1. {'loss': 0.0192, 'grad_norm': 0.5561724901199341, 'learning_rate': 7.64033925325184e-05, 'epoch': 0.92}\n",
      "1. {'loss': 0.0166, 'grad_norm': 0.350303590297699, 'learning_rate': 6.91341716182545e-05, 'epoch': 1.0}\n",
      "1. {'loss': 0.0117, 'grad_norm': 0.26505473256111145, 'learning_rate': 6.135381315171867e-05, 'epoch': 1.08}\n",
      "1. {'loss': 0.0082, 'grad_norm': 0.18696846067905426, 'learning_rate': 5.327015646150716e-05, 'epoch': 1.17}\n",
      "1. {'loss': 0.0067, 'grad_norm': 0.4086516201496124, 'learning_rate': 4.509914298352197e-05, 'epoch': 1.25}\n",
      "1. {'loss': 0.0095, 'grad_norm': 0.4090069830417633, 'learning_rate': 3.705904774487396e-05, 'epoch': 1.33}\n",
      "1. {'loss': 0.0071, 'grad_norm': 0.3651888072490692, 'learning_rate': 2.936464850978027e-05, 'epoch': 1.42}\n",
      "1. {'loss': 0.0074, 'grad_norm': 0.2714189887046814, 'learning_rate': 2.2221488349019903e-05, 'epoch': 1.5}\n",
      "1. {'loss': 0.0045, 'grad_norm': 0.2512831389904022, 'learning_rate': 1.5820384898856434e-05, 'epoch': 1.58}\n",
      "1. {'loss': 0.0077, 'grad_norm': 0.40112006664276123, 'learning_rate': 1.0332332985438248e-05, 'epoch': 1.67}\n",
      "1. {'loss': 0.0048, 'grad_norm': 0.3171476125717163, 'learning_rate': 5.903936782582253e-06, 'epoch': 1.75}\n",
      "1. {'loss': 0.0038, 'grad_norm': 0.17508746683597565, 'learning_rate': 2.653493525244721e-06, 'epoch': 1.83}\n",
      "1. {'loss': 0.004, 'grad_norm': 0.30343523621559143, 'learning_rate': 6.678333957560512e-07, 'epoch': 1.92}\n",
      "1. {'loss': 0.0043, 'grad_norm': 0.3144787847995758, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "1. {'train_runtime': 8262.3563, 'train_samples_per_second': 0.232, 'train_steps_per_second': 0.029, 'train_loss': 0.02895727731908361, 'epoch': 2.0}\n",
      "1. *** -> Training took 8262.3563 seconds.\n",
      "1. *** Saving model/tokenizer to '/kaggle/temp/finetuned_model_gpu1'...\n",
      "1. *** GPU: NVIDIA L4, used 20.1 / 22.3 GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  94%|█████████▍| 15/16 [09:53<00:39, 39.71s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/30 [05:19<?, ?it/s]\n",
      "7. {'loss': 0.0006, 'grad_norm': 0.020181803032755852, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.   0%|          | 0/30 [10:37<?, ?it/s]\n",
      "7. {'train_runtime': 633.425, 'train_samples_per_second': 0.202, 'train_steps_per_second': 0.025, 'train_loss': 0.0015664689417462796, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [10:33<00:00, 39.77s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [10:33<00:00, 39.77s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [10:33<00:00, 39.77s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [10:33<00:00, 39.59s/it]\n",
      "5. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "5. Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'...\n",
      "5. *** -> Fake test set detected, setting flag 'is_fake' to True.\n",
      "5. *** Load base model and tokenizer from '/kaggle/temp/finetuned_model_gpu1'...\n",
      "5. 🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "5. ==((====))==  Unsloth 2024.9.post4: Fast Mistral patching. Transformers = 4.44.0.\n",
      "5.    \\\\   /|    GPU: NVIDIA L4. Max memory: 22.278 GB. Platform = Linux.\n",
      "5. O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "5. \\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      "5.  \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "5. Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "4.  44%|████▍     | 7/16 [03:17<04:14, 28.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0055, 'grad_norm': 0.223825603723526, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [03:45<03:46, 28.27s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5. Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.66s/it]\n",
      "5. Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.65s/it]\n",
      "5. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.59s/it]\n",
      "5. Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.61s/it]\n",
      "5. Unsloth 2024.9.post4 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Augment dataset...\n",
      "5. 688\n",
      "5. *** Load peft state_dict from '/kaggle/temp/finetuned_model_gpu1'...\n",
      "5. *** Load stored data...\n",
      "5. *** Start inference run...\n",
      "5.   0%|          | 0/30 [00:00<?, ?it/s]retraining model for key '0934a4d8' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 423.92 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 419.70 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [03:45<03:46, 28.27s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [04:14<03:18, 28.40s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [04:42<02:50, 28.46s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:42<10:41, 42.75s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [05:11<02:22, 28.46s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [05:39<01:53, 28.46s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [01:24<09:47, 41.98s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [06:08<01:25, 28.48s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [02:05<09:04, 41.88s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [06:36<00:56, 28.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/30 [10:37<?, ?it/s]*** -> Training took 633.425 seconds.\n",
      "7.   3%|▎         | 1/30 [14:29<7:00:16, 869.54s/it]retraining model for key '16de56c4' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1872.27 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  94%|█████████▍| 15/16 [07:05<00:28, 28.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   3%|▎         | 1/30 [09:17<2:38:35, 328.11s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [07:33<00:00, 28.45s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [07:33<00:00, 28.45s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0009, 'grad_norm': 0.3851648271083832, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.   3%|▎         | 1/30 [13:05<2:38:35, 328.11s/it]\n",
      "4. {'train_runtime': 453.7301, 'train_samples_per_second': 0.282, 'train_steps_per_second': 0.035, 'train_loss': 0.00318871031049639, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [07:33<00:00, 28.45s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [07:33<00:00, 28.36s/it]\n",
      "5.  25%|██▌       | 4/16 [02:48<08:23, 41.98s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/30 [03:17<?, ?it/s]*** -> Training took 194.1493 seconds.\n",
      "6.   3%|▎         | 1/30 [08:50<4:16:14, 530.17s/it]retraining model for key '136b0064' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1838.66 examples/s]\n",
      "7.   6%|▋         | 1/16 [00:09<02:23,  9.56s/it]\u001b[A\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:19<02:13,  9.54s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:28<02:04,  9.54s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:08<02:11,  8.77s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [03:30<07:42, 42.04s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:17<02:02,  8.74s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:38<01:54,  9.54s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:26<01:53,  8.74s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:47<01:44,  9.53s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:35<01:45,  8.76s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [00:57<01:35,  9.52s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [00:43<01:36,  8.75s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:06<01:25,  9.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0028, 'grad_norm': 0.4009556174278259, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:16<01:15,  9.49s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.  38%|███▊      | 6/16 [00:52<01:27,  8.75s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [01:16<01:15,  9.49s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [04:12<07:01, 42.11s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:01<01:18,  8.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0355, 'grad_norm': 1.471221685409546, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:09<01:09,  8.74s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:25<01:06,  9.49s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [01:09<01:09,  8.74s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [01:35<00:56,  9.48s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:18<01:01,  8.75s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [01:44<00:47,  9.47s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [01:27<00:52,  8.75s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [01:53<00:37,  9.46s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [01:36<00:43,  8.74s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [04:54<06:18, 42.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0064, 'grad_norm': 1.3462674617767334, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [05:36<05:36, 42.12s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.  81%|████████▏ | 13/16 [02:03<00:28,  9.45s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [01:44<00:34,  8.74s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [02:12<00:18,  9.44s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [01:53<00:26,  8.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   3%|▎         | 1/30 [13:05<2:38:35, 328.11s/it]*** -> Training took 453.7301 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  94%|█████████▍| 15/16 [02:22<00:09,  9.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.   3%|▎         | 1/30 [15:48<7:00:16, 869.54s/it]\n",
      "7. {'loss': 0.0013, 'grad_norm': 0.23983830213546753, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.   3%|▎         | 1/30 [17:04<7:00:16, 869.54s/it]\n",
      "7. {'train_runtime': 151.6363, 'train_samples_per_second': 0.844, 'train_steps_per_second': 0.106, 'train_loss': 0.0020786550594493747, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [02:31<00:00,  9.43s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [02:31<00:00,  9.43s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [02:31<00:00,  9.43s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [02:31<00:00,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   7%|▋         | 2/30 [15:34<3:49:23, 491.56s/it]retraining model for key '271d71e2' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1432.52 examples/s]\n",
      "6.  88%|████████▊ | 14/16 [02:02<00:17,  8.74s/it]\u001b[A\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  94%|█████████▍| 15/16 [02:11<00:08,  8.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   3%|▎         | 1/30 [10:03<4:16:14, 530.17s/it]\n",
      "6. {'loss': 0.0052, 'grad_norm': 1.783237338066101, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.   3%|▎         | 1/30 [11:13<4:16:14, 530.17s/it]\n",
      "6. {'train_runtime': 139.8643, 'train_samples_per_second': 0.915, 'train_steps_per_second': 0.114, 'train_loss': 0.020358080742880702, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.73s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.73s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.73s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.74s/it]\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  50%|█████     | 8/16 [05:36<05:36, 42.12s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:11<02:47, 11.17s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:22<02:35, 11.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   3%|▎         | 1/30 [11:13<4:16:14, 530.17s/it]*** -> Training took 139.8643 seconds.\n",
      "6.   7%|▋         | 2/30 [11:47<2:30:35, 322.68s/it]retraining model for key '2b83f449' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1633.79 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  19%|█▉        | 3/16 [00:33<02:24, 11.13s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:44<02:13, 11.13s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [06:18<04:55, 42.16s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:10<02:33, 10.26s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:55<02:02, 11.13s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:20<02:24, 10.33s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:06<01:51, 11.13s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:30<02:14, 10.32s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:17<01:40, 11.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0114, 'grad_norm': 0.4247930943965912, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:28<01:28, 11.11s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:41<02:03, 10.27s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.   3%|▎         | 1/30 [17:04<7:00:16, 869.54s/it]*** -> Training took 151.6363 seconds.\n",
      "7.   7%|▋         | 2/30 [18:46<3:57:35, 509.13s/it]retraining model for key '1818057f' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1500.34 examples/s]\n",
      "4.  50%|█████     | 8/16 [01:28<01:28, 11.11s/it]\u001b[A\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n",
      "5.  62%|██████▎   | 10/16 [07:01<04:13, 42.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  31%|███▏      | 5/16 [00:51<01:52, 10.20s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [01:40<01:17, 11.11s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:01<01:41, 10.14s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:51<01:06, 11.11s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:11<01:31, 10.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0353, 'grad_norm': 0.3578176200389862, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:21<01:20, 10.09s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.   6%|▋         | 1/16 [00:11<02:56, 11.79s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [02:02<00:55, 11.12s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [01:21<01:20, 10.09s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:23<02:44, 11.76s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [07:43<03:30, 42.14s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [02:13<00:44, 11.13s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:31<01:10, 10.09s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:35<02:33, 11.78s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [01:41<01:00, 10.09s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [02:24<00:33, 11.14s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:47<02:21, 11.79s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [01:51<00:50, 10.08s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [02:35<00:22, 11.14s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:58<02:09, 11.79s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:01<00:40, 10.09s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [02:46<00:11, 11.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   7%|▋         | 2/30 [17:06<3:49:23, 491.56s/it]\n",
      "4. {'loss': 0.0015, 'grad_norm': 0.8052920699119568, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.   7%|▋         | 2/30 [18:35<3:49:23, 491.56s/it]\n",
      "4. {'train_runtime': 178.0875, 'train_samples_per_second': 0.719, 'train_steps_per_second': 0.09, 'train_loss': 0.006414992094505578, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:58<00:00, 11.14s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:58<00:00, 11.14s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:58<00:00, 11.14s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:58<00:00, 11.13s/it]\n",
      "7.  38%|███▊      | 6/16 [01:10<01:58, 11.81s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [02:11<00:30, 10.10s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [08:25<02:48, 42.16s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [02:21<00:20, 10.12s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:22<01:46, 11.81s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0086, 'grad_norm': 0.07340596616268158, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:34<01:34, 11.81s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.  94%|█████████▍| 15/16 [02:32<00:10, 10.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   7%|▋         | 2/30 [13:12<2:30:35, 322.68s/it]\n",
      "6. {'loss': 0.0071, 'grad_norm': 0.3827928304672241, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:42<00:00, 10.12s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:42<00:00, 10.12s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   7%|▋         | 2/30 [14:32<2:30:35, 322.68s/it]\n",
      "6. {'train_runtime': 162.2313, 'train_samples_per_second': 0.789, 'train_steps_per_second': 0.099, 'train_loss': 0.021195611683651805, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:42<00:00, 10.12s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:42<00:00, 10.14s/it]\n",
      "7.  50%|█████     | 8/16 [01:34<01:34, 11.81s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:46<01:22, 11.82s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [09:07<02:06, 42.19s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [01:58<01:10, 11.83s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:09<00:59, 11.83s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:21<00:47, 11.86s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [02:33<00:35, 11.90s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [09:49<01:24, 42.18s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [02:45<00:23, 11.94s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.   7%|▋         | 2/30 [14:32<2:30:35, 322.68s/it]*** -> Training took 162.2313 seconds.\n",
      "6.  10%|█         | 3/30 [15:54<2:09:42, 288.23s/it]retraining model for key '2d0172a1' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 762.91 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 751.68 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  94%|█████████▍| 15/16 [02:57<00:11, 11.96s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.   7%|▋         | 2/30 [20:23<3:57:35, 509.13s/it]\n",
      "7. {'loss': 0.0046, 'grad_norm': 0.07071002572774887, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.   7%|▋         | 2/30 [21:59<3:57:35, 509.13s/it]\n",
      "7. {'train_runtime': 189.8117, 'train_samples_per_second': 0.674, 'train_steps_per_second': 0.084, 'train_loss': 0.006597050232812762, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [03:09<00:00, 11.95s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:09<00:00, 11.95s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:09<00:00, 11.95s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:09<00:00, 11.86s/it]\n",
      "5.  94%|█████████▍| 15/16 [10:32<00:42, 42.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.   0%|          | 0/30 [05:41<?, ?it/s]\n",
      "5. {'loss': 0.004, 'grad_norm': 0.058422211557626724, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.   0%|          | 0/30 [11:18<?, ?it/s]\n",
      "5. {'train_runtime': 674.0827, 'train_samples_per_second': 0.19, 'train_steps_per_second': 0.024, 'train_loss': 0.00519835390150547, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [11:14<00:00, 42.12s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [11:14<00:00, 42.12s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [11:14<00:00, 42.12s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [11:14<00:00, 42.13s/it]\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.   0%|          | 0/30 [11:18<?, ?it/s]*** -> Training took 674.0827 seconds.\n",
      "5.   3%|▎         | 1/30 [11:30<5:33:48, 690.65s/it]retraining model for key '13e47133' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 835.01 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 822.61 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.   7%|▋         | 2/30 [18:35<3:49:23, 491.56s/it]*** -> Training took 178.0875 seconds.\n",
      "5. *** Start training run...\n",
      "4.  10%|█         | 3/30 [21:05<3:08:21, 418.59s/it]retraining model for key '291dc1e1' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1448.75 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   6%|▋         | 1/16 [00:22<05:30, 22.04s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:11<02:46, 11.08s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:44<05:08, 22.01s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:22<02:34, 11.04s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:21<05:19, 21.28s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:33<02:23, 11.03s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:06<04:46, 22.04s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:44<02:12, 11.01s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:42<04:58, 21.35s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:55<02:01, 11.00s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:28<04:23, 22.00s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:06<01:49, 10.99s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:03<04:36, 21.27s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:17<01:38, 11.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0305, 'grad_norm': 0.614399254322052, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:28<01:27, 10.99s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:49<04:01, 21.97s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [01:28<01:27, 10.99s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:24<04:14, 21.20s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [01:39<01:16, 11.00s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [02:11<03:39, 21.97s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:50<01:06, 11.01s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:46<03:52, 21.18s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [02:01<00:55, 11.02s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [02:33<03:17, 21.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0183, 'grad_norm': 0.8062732815742493, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:55<02:55, 21.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "4.  75%|███████▌  | 12/16 [02:12<00:44, 11.03s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:07<03:31, 21.18s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [02:23<00:33, 11.04s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:55<02:55, 21.98s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [02:34<00:22, 11.04s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:28<03:10, 21.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0038, 'grad_norm': 0.1412174105644226, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:49<02:49, 21.16s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  94%|█████████▍| 15/16 [02:45<00:11, 11.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  10%|█         | 3/30 [22:37<3:08:21, 418.59s/it]\n",
      "4. {'loss': 0.0058, 'grad_norm': 0.9204539060592651, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:56<00:00, 11.03s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  10%|█         | 3/30 [24:05<3:08:21, 418.59s/it]\n",
      "4. {'train_runtime': 176.3531, 'train_samples_per_second': 0.726, 'train_steps_per_second': 0.091, 'train_loss': 0.01817387924529612, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:56<00:00, 11.03s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:56<00:00, 11.03s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:56<00:00, 11.02s/it]\n",
      "6.  56%|█████▋    | 9/16 [03:17<02:33, 21.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.   7%|▋         | 2/30 [21:59<3:57:35, 509.13s/it]*** -> Training took 189.8117 seconds.\n",
      "7.  10%|█         | 3/30 [25:42<3:30:04, 466.83s/it]retraining model for key '20270e3b' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 2849.33 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n",
      "4.  10%|█         | 3/30 [24:05<3:08:21, 418.59s/it]*** -> Training took 176.3531 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:49<02:49, 21.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  13%|█▎        | 4/30 [24:16<2:22:19, 328.43s/it]retraining model for key '2c181942' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 575.57 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 568.02 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:05<01:29,  5.96s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [03:39<02:11, 21.98s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:11<01:22,  5.92s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [03:10<02:28, 21.16s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:17<01:16,  5.91s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:23<01:10,  5.91s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:29<01:04,  5.90s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [04:01<01:49, 21.97s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [00:35<00:59,  5.90s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [03:31<02:06, 21.15s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [00:41<00:53,  5.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0106, 'grad_norm': 1.7349493503570557, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [00:47<00:47,  5.90s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [00:47<00:47,  5.90s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [04:23<01:27, 21.96s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [00:53<00:41,  5.90s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:29<07:18, 29.26s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:53<01:45, 21.15s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [00:59<00:35,  5.91s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [01:04<00:29,  5.91s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [01:10<00:23,  5.92s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [04:45<01:05, 21.96s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [01:16<00:17,  5.91s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [04:14<01:24, 21.16s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [01:22<00:11,  5.91s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:58<06:49, 29.27s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [01:28<00:05,  5.91s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  10%|█         | 3/30 [26:33<3:30:04, 466.83s/it]\n",
      "7. {'loss': 0.0014, 'grad_norm': 0.04534924030303955, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  10%|█         | 3/30 [27:20<3:30:04, 466.83s/it]\n",
      "7. {'train_runtime': 94.5337, 'train_samples_per_second': 1.354, 'train_steps_per_second': 0.169, 'train_loss': 0.005970537837129086, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [01:34<00:00,  5.91s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [01:34<00:00,  5.91s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [01:34<00:00,  5.91s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [01:34<00:00,  5.91s/it]\n",
      "6.  88%|████████▊ | 14/16 [05:07<00:43, 21.97s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [04:35<01:03, 21.15s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:27<06:20, 29.29s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [05:29<00:21, 21.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  10%|█         | 3/30 [18:54<2:09:42, 288.23s/it]\n",
      "6. {'loss': 0.005, 'grad_norm': 0.6942006349563599, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  10%|█         | 3/30 [21:50<2:09:42, 288.23s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:51<00:00, 21.94s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:51<00:00, 21.94s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'train_runtime': 351.5196, 'train_samples_per_second': 0.364, 'train_steps_per_second': 0.046, 'train_loss': 0.011648109648376703, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:51<00:00, 21.94s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [05:51<00:00, 21.97s/it]\n",
      "5.  88%|████████▊ | 14/16 [04:56<00:42, 21.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  10%|█         | 3/30 [27:20<3:30:04, 466.83s/it]*** -> Training took 94.5337 seconds.\n",
      "7.  13%|█▎        | 4/30 [28:03<2:26:27, 337.96s/it]retraining model for key '20a9e565' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 632.36 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 623.64 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  94%|█████████▍| 15/16 [05:17<00:21, 21.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.   3%|▎         | 1/30 [14:24<5:33:48, 690.65s/it]\n",
      "5. {'loss': 0.0012, 'grad_norm': 0.4062530994415283, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.   3%|▎         | 1/30 [17:13<5:33:48, 690.65s/it]\n",
      "5. {'train_runtime': 338.863, 'train_samples_per_second': 0.378, 'train_steps_per_second': 0.047, 'train_loss': 0.0024959638831205666, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [05:38<00:00, 21.17s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [05:38<00:00, 21.17s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [05:38<00:00, 21.17s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [05:38<00:00, 21.18s/it]\n",
      "4.  25%|██▌       | 4/16 [01:57<05:51, 29.32s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [02:26<05:22, 29.34s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:29<07:25, 29.70s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  10%|█         | 3/30 [21:50<2:09:42, 288.23s/it]*** -> Training took 351.5196 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  38%|███▊      | 6/16 [02:55<04:53, 29.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  13%|█▎        | 4/30 [23:14<2:30:47, 348.00s/it]retraining model for key '36a08778' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 784.05 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 772.85 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:59<06:56, 29.76s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [03:25<04:24, 29.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0048, 'grad_norm': 0.23753371834754944, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [03:54<03:55, 29.38s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.   6%|▋         | 1/16 [00:22<05:33, 22.23s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:29<06:28, 29.86s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [03:54<03:55, 29.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.   3%|▎         | 1/30 [17:13<5:33:48, 690.65s/it]*** -> Training took 338.863 seconds.\n",
      "5.   7%|▋         | 2/30 [19:21<4:21:55, 561.27s/it]retraining model for key '142ca369' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 693.16 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 683.30 examples/s]\n",
      "6.  12%|█▎        | 2/16 [00:44<05:11, 22.22s/it]\u001b[A\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  25%|██▌       | 4/16 [01:59<05:59, 29.93s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [04:24<03:25, 29.40s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:06<04:48, 22.16s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [02:29<05:29, 29.94s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:28<04:25, 22.12s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [04:53<02:56, 29.42s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:25<06:22, 25.50s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:50<04:03, 22.11s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:59<04:59, 29.95s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [05:23<02:27, 29.45s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:51<05:57, 25.56s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [02:12<03:40, 22.10s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [03:29<04:29, 29.97s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [03:59<03:59, 29.98s/it]\u001b[A\n",
      "7. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0259, 'grad_norm': 2.005958318710327, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  19%|█▉        | 3/16 [01:16<05:32, 25.54s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [05:52<01:57, 29.46s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [02:34<03:19, 22.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0037, 'grad_norm': 0.21396669745445251, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:57<02:57, 22.13s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:42<05:06, 25.51s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [03:59<03:59, 29.98s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:57<02:57, 22.13s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [06:22<01:28, 29.46s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [02:07<04:40, 25.50s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [03:19<02:34, 22.13s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [04:29<03:30, 30.00s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [06:51<00:58, 29.44s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [03:41<02:12, 22.13s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:33<04:14, 25.50s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [04:59<02:59, 30.00s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [07:20<00:29, 29.43s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  13%|█▎        | 4/30 [28:14<2:22:19, 328.43s/it]\n",
      "4. {'loss': 0.0007, 'grad_norm': 0.39564278721809387, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  13%|█▎        | 4/30 [32:10<2:22:19, 328.43s/it]\n",
      "4. {'train_runtime': 470.3338, 'train_samples_per_second': 0.272, 'train_steps_per_second': 0.034, 'train_loss': 0.002748749335296452, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [07:50<00:00, 29.42s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [07:50<00:00, 29.42s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [07:50<00:00, 29.42s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [07:50<00:00, 29.40s/it]\n",
      "6.  69%|██████▉   | 11/16 [04:03<01:50, 22.12s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:58<03:49, 25.49s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [03:24<03:23, 25.50s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0032, 'grad_norm': 0.21135690808296204, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  75%|███████▌  | 12/16 [04:25<01:28, 22.12s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [05:29<02:29, 29.99s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [03:24<03:23, 25.50s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [04:47<01:06, 22.10s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [05:59<01:59, 29.99s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [03:49<02:58, 25.51s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [05:09<00:44, 22.11s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [06:29<01:29, 29.99s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [04:15<02:33, 25.51s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [05:31<00:22, 22.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  13%|█▎        | 4/30 [26:15<2:30:47, 348.00s/it]\n",
      "6. {'loss': 0.0008, 'grad_norm': 0.28369802236557007, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  13%|█▎        | 4/30 [29:12<2:30:47, 348.00s/it]\n",
      "6. {'train_runtime': 353.9219, 'train_samples_per_second': 0.362, 'train_steps_per_second': 0.045, 'train_loss': 0.0022598178475163877, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:53<00:00, 22.10s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:53<00:00, 22.10s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:53<00:00, 22.10s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [05:53<00:00, 22.12s/it]\n",
      "5.  69%|██████▉   | 11/16 [04:40<02:07, 25.50s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [06:59<00:59, 29.98s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [05:06<01:42, 25.50s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [07:29<00:29, 29.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  13%|█▎        | 4/30 [32:06<2:26:27, 337.96s/it]\n",
      "7. {'loss': 0.0046, 'grad_norm': 1.8752752542495728, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  13%|█▎        | 4/30 [36:06<2:26:27, 337.96s/it]\n",
      "7. {'train_runtime': 479.5393, 'train_samples_per_second': 0.267, 'train_steps_per_second': 0.033, 'train_loss': 0.015251360600814223, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [07:59<00:00, 30.03s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [07:59<00:00, 30.03s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [07:59<00:00, 30.03s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [07:59<00:00, 29.97s/it]\n",
      "5.  81%|████████▏ | 13/16 [05:31<01:16, 25.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  13%|█▎        | 4/30 [36:06<2:26:27, 337.96s/it]*** -> Training took 479.5393 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  88%|████████▊ | 14/16 [05:57<00:51, 25.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  17%|█▋        | 5/30 [36:48<2:48:55, 405.42s/it]retraining model for key '21897d95' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1451.56 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n",
      "4.  13%|█▎        | 4/30 [32:10<2:22:19, 328.43s/it]*** -> Training took 470.3338 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  17%|█▋        | 5/30 [35:33<3:09:20, 454.40s/it]retraining model for key '3dc255db' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1745.62 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  94%|█████████▍| 15/16 [06:22<00:25, 25.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.   7%|▋         | 2/30 [22:49<4:21:55, 561.27s/it]\n",
      "5. {'loss': 0.0004, 'grad_norm': 0.12700526416301727, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.   7%|▋         | 2/30 [26:13<4:21:55, 561.27s/it]\n",
      "5. {'train_runtime': 408.1125, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'train_loss': 0.001786214779713191, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [06:48<00:00, 25.50s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [06:48<00:00, 25.50s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [06:48<00:00, 25.50s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [06:48<00:00, 25.51s/it]\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:12<03:01, 12.09s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:09<02:16,  9.08s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:24<02:48, 12.06s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:18<02:06,  9.05s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:36<02:36, 12.05s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:27<01:57,  9.04s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:36<01:48,  9.04s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:48<02:24, 12.05s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:45<01:39,  9.05s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:00<02:12, 12.05s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [00:54<01:30,  9.05s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:12<02:00, 12.05s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:03<01:21,  9.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0061, 'grad_norm': 1.0123660564422607, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:12<01:12,  9.07s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:24<01:48, 12.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0172, 'grad_norm': 0.4784879684448242, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:36<01:36, 12.03s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4.  50%|█████     | 8/16 [01:12<01:12,  9.07s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [01:21<01:03,  9.07s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [01:36<01:36, 12.03s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:30<00:54,  9.06s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:48<01:24, 12.01s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [01:39<00:45,  9.06s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:00<01:12, 12.00s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [01:48<00:36,  9.07s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [01:57<00:27,  9.06s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:12<01:00, 12.00s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [02:06<00:18,  9.07s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:24<00:48, 12.00s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [02:15<00:09,  9.07s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:24<00:00,  9.07s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:24<00:00,  9.07s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  17%|█▋        | 5/30 [36:49<3:09:20, 454.40s/it]\n",
      "4. {'loss': 0.0006, 'grad_norm': 0.15371665358543396, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  17%|█▋        | 5/30 [38:01<3:09:20, 454.40s/it]\n",
      "4. {'train_runtime': 144.9918, 'train_samples_per_second': 0.883, 'train_steps_per_second': 0.11, 'train_loss': 0.003327933431137353, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:24<00:00,  9.07s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:24<00:00,  9.06s/it]\n",
      "7.  81%|████████▏ | 13/16 [02:36<00:36, 12.01s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [02:48<00:24, 12.01s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [03:00<00:12, 12.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  17%|█▋        | 5/30 [38:28<2:48:55, 405.42s/it]\n",
      "7. {'loss': 0.0027, 'grad_norm': 1.3539406061172485, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  17%|█▋        | 5/30 [40:04<2:48:55, 405.42s/it]\n",
      "7. {'train_runtime': 192.2878, 'train_samples_per_second': 0.666, 'train_steps_per_second': 0.083, 'train_loss': 0.009992693667300045, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [03:12<00:00, 11.99s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:12<00:00, 11.99s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:12<00:00, 11.99s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:12<00:00, 12.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  17%|█▋        | 5/30 [38:01<3:09:20, 454.40s/it]*** -> Training took 144.9918 seconds.\n",
      "4.  20%|██        | 6/30 [38:55<2:27:24, 368.51s/it]retraining model for key '3e6067c3' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 570.13 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 563.00 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.   7%|▋         | 2/30 [26:13<4:21:55, 561.27s/it]*** -> Training took 408.1125 seconds.\n",
      "5.  10%|█         | 3/30 [30:01<4:28:42, 597.13s/it]retraining model for key '195c6913' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 481.66 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 476.09 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n",
      "7.  17%|█▋        | 5/30 [40:04<2:48:55, 405.42s/it]*** -> Training took 192.2878 seconds.\n",
      "7.  20%|██        | 6/30 [41:22<2:24:21, 360.90s/it]retraining model for key '221dfab4' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 667.20 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 658.25 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   6%|▋         | 1/16 [00:30<07:35, 30.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  13%|█▎        | 4/30 [29:12<2:30:47, 348.00s/it]*** -> Training took 353.9219 seconds.\n",
      "6.  17%|█▋        | 5/30 [35:33<3:23:47, 489.11s/it]retraining model for key '409aa875' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 972.95 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 956.54 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [01:00<07:05, 30.41s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:17<04:22, 17.50s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:37<09:26, 37.74s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:27<06:56, 27.77s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:35<04:05, 17.51s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:31<06:36, 30.48s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:52<03:47, 17.49s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:55<06:29, 27.83s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [01:15<08:47, 37.64s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [02:01<06:05, 30.46s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:10<03:30, 17.51s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:23<06:02, 27.86s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:27<03:12, 17.50s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [02:32<05:34, 30.45s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:52<08:09, 37.62s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:44<02:54, 17.48s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:51<05:34, 27.86s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [02:02<02:37, 17.47s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0087, 'grad_norm': 0.1414438635110855, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:19<02:19, 17.47s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "4.  38%|███▊      | 6/16 [03:02<05:04, 30.49s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [02:19<05:06, 27.87s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [02:30<07:32, 37.67s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:19<02:19, 17.47s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [02:37<02:02, 17.51s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [03:33<04:34, 30.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0037, 'grad_norm': 0.08021893352270126, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [04:03<04:03, 30.48s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:47<04:39, 27.90s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:55<01:45, 17.53s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [03:08<06:53, 37.62s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [04:03<04:03, 30.48s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [03:12<01:27, 17.52s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [03:15<04:11, 27.91s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0111, 'grad_norm': 0.07515857368707657, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [03:43<03:43, 27.89s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.  75%|███████▌  | 12/16 [03:30<01:10, 17.51s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [03:45<06:16, 37.62s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [04:34<03:33, 30.50s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [03:43<03:43, 27.89s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [03:47<00:52, 17.49s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [04:04<00:35, 17.50s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [05:04<03:02, 30.48s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [04:10<03:15, 27.89s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [04:23<05:38, 37.61s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0096, 'grad_norm': 0.14613480865955353, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [05:01<05:01, 37.63s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "6.  94%|█████████▍| 15/16 [04:22<00:17, 17.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  17%|█▋        | 5/30 [37:57<3:23:47, 489.11s/it]\n",
      "6. {'loss': 0.0063, 'grad_norm': 0.14759868383407593, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  17%|█▋        | 5/30 [40:17<3:23:47, 489.11s/it]\n",
      "6. {'train_runtime': 280.0812, 'train_samples_per_second': 0.457, 'train_steps_per_second': 0.057, 'train_loss': 0.007496953010559082, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [04:40<00:00, 17.52s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:40<00:00, 17.52s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:40<00:00, 17.52s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [04:40<00:00, 17.50s/it]\n",
      "7.  62%|██████▎   | 10/16 [04:38<02:47, 27.91s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [05:35<02:32, 30.50s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [05:01<05:01, 37.63s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [05:06<02:19, 27.91s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [06:05<02:01, 30.50s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [05:38<04:23, 37.63s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [05:34<01:51, 27.93s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [06:36<01:31, 30.50s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [06:03<01:24, 28.03s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [06:16<03:45, 37.65s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [07:06<01:01, 30.50s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  17%|█▋        | 5/30 [40:17<3:23:47, 489.11s/it]*** -> Training took 280.0812 seconds.\n",
      "6.  20%|██        | 6/30 [42:12<3:03:19, 458.31s/it]retraining model for key '446ef5d2' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 2008.57 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [06:31<00:56, 28.08s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:08<02:02,  8.19s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [07:37<00:30, 30.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  20%|██        | 6/30 [43:03<2:27:24, 368.51s/it]\n",
      "4. {'loss': 0.0013, 'grad_norm': 0.21752473711967468, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  20%|██        | 6/30 [47:07<2:27:24, 368.51s/it]\n",
      "4. {'train_runtime': 487.6793, 'train_samples_per_second': 0.262, 'train_steps_per_second': 0.033, 'train_loss': 0.0024729594006203115, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [08:07<00:00, 30.48s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [08:07<00:00, 30.48s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [08:07<00:00, 30.48s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [08:07<00:00, 30.48s/it]\n",
      "5.  69%|██████▉   | 11/16 [06:53<03:08, 37.62s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:16<01:54,  8.17s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:24<01:45,  8.14s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [06:59<00:28, 28.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  20%|██        | 6/30 [45:09<2:24:21, 360.90s/it]\n",
      "7. {'loss': 0.0019, 'grad_norm': 0.04953395575284958, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  20%|██        | 6/30 [48:54<2:24:21, 360.90s/it]\n",
      "7. {'train_runtime': 447.5307, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.036, 'train_loss': 0.006508138554636389, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [07:27<00:00, 28.12s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [07:27<00:00, 28.12s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [07:27<00:00, 28.12s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [07:27<00:00, 27.97s/it]\n",
      "6.  25%|██▌       | 4/16 [00:32<01:37,  8.14s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [00:40<01:29,  8.12s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [00:48<01:21,  8.12s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [07:31<02:30, 37.62s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [00:56<01:13,  8.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0125, 'grad_norm': 0.641453206539154, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:05<01:04,  8.12s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6.  50%|█████     | 8/16 [01:05<01:04,  8.12s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:13<00:56,  8.11s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [01:21<00:48,  8.12s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [01:29<00:40,  8.11s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [08:09<01:52, 37.61s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [01:37<00:32,  8.11s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [01:45<00:24,  8.12s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [01:53<00:16,  8.12s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [02:01<00:08,  8.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  20%|██        | 6/30 [43:20<3:03:19, 458.31s/it]\n",
      "6. {'loss': 0.0021, 'grad_norm': 0.6489315032958984, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  20%|██        | 6/30 [44:25<3:03:19, 458.31s/it]\n",
      "6. {'train_runtime': 129.9315, 'train_samples_per_second': 0.985, 'train_steps_per_second': 0.123, 'train_loss': 0.007295522023923695, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:09<00:00,  8.11s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:09<00:00,  8.11s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:09<00:00,  8.11s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:09<00:00,  8.12s/it]\n",
      "5.  88%|████████▊ | 14/16 [08:46<01:15, 37.60s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [09:24<00:37, 37.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  10%|█         | 3/30 [35:06<4:28:42, 597.13s/it]\n",
      "5. {'loss': 0.0008, 'grad_norm': 0.21408672630786896, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  10%|█         | 3/30 [40:07<4:28:42, 597.13s/it]\n",
      "5. {'train_runtime': 601.8986, 'train_samples_per_second': 0.213, 'train_steps_per_second': 0.027, 'train_loss': 0.005195580772124231, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [10:01<00:00, 37.59s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [10:01<00:00, 37.59s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [10:01<00:00, 37.59s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [10:01<00:00, 37.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  20%|██        | 6/30 [47:07<2:27:24, 368.51s/it]*** -> Training took 487.6793 seconds.\n",
      "4.  23%|██▎       | 7/30 [54:22<3:31:15, 551.13s/it]retraining model for key '45a5af55' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 942.40 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 927.08 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:17<04:19, 17.31s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:34<04:02, 17.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  20%|██        | 6/30 [44:25<3:03:19, 458.31s/it]*** -> Training took 129.9315 seconds.\n",
      "6.  23%|██▎       | 7/30 [50:47<3:02:50, 476.96s/it]retraining model for key '4c416de3' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 558.72 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 550.86 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  19%|█▉        | 3/16 [00:52<03:45, 17.35s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:09<03:28, 17.36s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  10%|█         | 3/30 [40:07<4:28:42, 597.13s/it]*** -> Training took 601.8986 seconds.\n",
      "5.  13%|█▎        | 4/30 [46:31<5:26:00, 752.31s/it]retraining model for key '28a6681f' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 2186.39 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  31%|███▏      | 5/16 [01:26<03:10, 17.36s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:07<01:54,  7.61s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:31<07:46, 31.11s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:44<02:53, 17.37s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:15<01:46,  7.61s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  20%|██        | 6/30 [48:54<2:24:21, 360.90s/it]*** -> Training took 447.5307 seconds.\n",
      "7.  23%|██▎       | 7/30 [58:04<3:38:35, 570.24s/it]retraining model for key '247ef758' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1523.94 examples/s]\n",
      "5.  19%|█▉        | 3/16 [00:22<01:39,  7.62s/it]\u001b[A\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  25%|██▌       | 4/16 [00:30<01:31,  7.63s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:01<02:36, 17.39s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0033, 'grad_norm': 0.37372922897338867, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:19<02:19, 17.41s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [00:38<01:23,  7.64s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [01:02<07:17, 31.25s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [00:45<01:16,  7.64s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:11<02:58, 11.87s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:19<02:19, 17.41s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [00:53<01:08,  7.63s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [01:00<01:00,  7.62s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0097, 'grad_norm': 0.6163346171379089, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:23<02:45, 11.84s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [01:00<01:00,  7.62s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:36<02:01, 17.41s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:08<00:53,  7.60s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:35<02:33, 11.83s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:34<06:48, 31.43s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [01:16<00:45,  7.59s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:47<02:21, 11.80s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [01:23<00:37,  7.58s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [02:53<01:44, 17.40s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [01:31<00:30,  7.57s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:59<02:09, 11.78s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [01:38<00:22,  7.57s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:11<01:26, 17.39s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [02:05<06:18, 31.53s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [01:46<00:15,  7.57s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:10<01:57, 11.77s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [01:53<00:07,  7.58s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  13%|█▎        | 4/30 [47:35<5:26:00, 752.31s/it]\n",
      "5. {'loss': 0.0014, 'grad_norm': 0.10246770083904266, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  13%|█▎        | 4/30 [48:35<5:26:00, 752.31s/it]\n",
      "5. {'train_runtime': 121.5859, 'train_samples_per_second': 1.053, 'train_steps_per_second': 0.132, 'train_loss': 0.005566170555539429, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [02:01<00:00,  7.59s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:01<00:00,  7.59s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:01<00:00,  7.59s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [02:01<00:00,  7.60s/it]\n",
      "7.  44%|████▍     | 7/16 [01:22<01:45, 11.76s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0065, 'grad_norm': 0.5761913061141968, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:34<01:34, 11.76s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4.  75%|███████▌  | 12/16 [03:28<01:09, 17.39s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [01:34<01:34, 11.76s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [03:45<00:52, 17.39s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [02:37<05:47, 31.56s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:45<01:22, 11.75s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  13%|█▎        | 4/30 [48:35<5:26:00, 752.31s/it]*** -> Training took 121.5859 seconds.\n",
      "5.  17%|█▋        | 5/30 [49:12<3:44:37, 539.10s/it]retraining model for key '31f7f899' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1594.51 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  88%|████████▊ | 14/16 [04:03<00:34, 17.41s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [01:57<01:10, 11.75s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:09<00:58, 11.75s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [03:09<05:15, 31.60s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [04:20<00:17, 17.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  23%|██▎       | 7/30 [56:45<3:31:15, 551.13s/it]\n",
      "4. {'loss': 0.0001, 'grad_norm': 0.07672607898712158, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  23%|██▎       | 7/30 [59:04<3:31:15, 551.13s/it]\n",
      "4. {'train_runtime': 278.4982, 'train_samples_per_second': 0.46, 'train_steps_per_second': 0.057, 'train_loss': 0.0017289094175794162, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:38<00:00, 17.48s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:38<00:00, 17.48s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:38<00:00, 17.48s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [04:38<00:00, 17.41s/it]\n",
      "5.   6%|▋         | 1/16 [00:10<02:38, 10.57s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:21<00:46, 11.75s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:21<02:27, 10.53s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [02:32<00:35, 11.74s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:31<02:16, 10.52s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [02:44<00:23, 11.74s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [03:40<04:44, 31.62s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0015, 'grad_norm': 0.11966341733932495, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [04:12<04:12, 31.62s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:42<02:06, 10.54s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [02:56<00:11, 11.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  23%|██▎       | 7/30 [59:41<3:38:35, 570.24s/it]\n",
      "7. {'loss': 0.0022, 'grad_norm': 0.12008596211671829, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  23%|██▎       | 7/30 [1:01:15<3:38:35, 570.24s/it]\n",
      "7. {'train_runtime': 188.0736, 'train_samples_per_second': 0.681, 'train_steps_per_second': 0.085, 'train_loss': 0.004364358610473573, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [03:08<00:00, 11.72s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:08<00:00, 11.72s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:08<00:00, 11.72s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:08<00:00, 11.75s/it]\n",
      "5.  31%|███▏      | 5/16 [00:52<01:55, 10.54s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:03<01:45, 10.53s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [04:12<04:12, 31.62s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:13<01:34, 10.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.006, 'grad_norm': 0.21452713012695312, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:24<01:24, 10.53s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5.  50%|█████     | 8/16 [01:24<01:24, 10.53s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:34<01:13, 10.53s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [04:43<03:41, 31.61s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [01:45<01:03, 10.53s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [01:55<00:52, 10.53s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [02:06<00:42, 10.53s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [05:15<03:09, 31.60s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [02:16<00:31, 10.53s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [02:27<00:21, 10.53s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [02:37<00:10, 10.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  17%|█▋        | 5/30 [50:39<3:44:37, 539.10s/it]\n",
      "5. {'loss': 0.0012, 'grad_norm': 0.18923121690750122, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [02:48<00:00, 10.52s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:48<00:00, 10.52s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:48<00:00, 10.52s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [02:48<00:00, 10.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  17%|█▋        | 5/30 [52:03<3:44:37, 539.10s/it]\n",
      "5. {'train_runtime': 168.4683, 'train_samples_per_second': 0.76, 'train_steps_per_second': 0.095, 'train_loss': 0.0035798161989077926, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  69%|██████▉   | 11/16 [05:47<02:38, 31.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  23%|██▎       | 7/30 [1:01:15<3:38:35, 570.24s/it]*** -> Training took 188.0736 seconds.\n",
      "7.  27%|██▋       | 8/30 [1:03:37<3:01:24, 494.73s/it]retraining model for key '2ba387bc' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 720.58 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 709.94 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  75%|███████▌  | 12/16 [06:18<02:06, 31.59s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [06:50<01:34, 31.60s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:25<06:17, 25.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  23%|██▎       | 7/30 [59:04<3:31:15, 551.13s/it]*** -> Training took 278.4982 seconds.\n",
      "4.  27%|██▋       | 8/30 [1:03:14<3:19:47, 544.91s/it]retraining model for key '4a21e3da' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 778.51 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 767.13 examples/s]\n",
      "6.  88%|████████▊ | 14/16 [07:21<01:03, 31.60s/it]\u001b[A\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n",
      "5.  17%|█▋        | 5/30 [52:03<3:44:37, 539.10s/it]*** -> Training took 168.4683 seconds.\n",
      "5.  20%|██        | 6/30 [53:52<3:00:28, 451.20s/it]retraining model for key '35ab12c3' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 819.00 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 806.86 examples/s]\n",
      "7.  12%|█▎        | 2/16 [00:50<05:52, 25.14s/it]\u001b[A\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [07:53<00:31, 31.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  23%|██▎       | 7/30 [55:03<3:02:50, 476.96s/it]\n",
      "6. {'loss': 0.0007, 'grad_norm': 0.08046546578407288, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  23%|██▎       | 7/30 [59:16<3:02:50, 476.96s/it]\n",
      "6. {'train_runtime': 505.1297, 'train_samples_per_second': 0.253, 'train_steps_per_second': 0.032, 'train_loss': 0.0011045857390854508, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [08:25<00:00, 31.60s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [08:25<00:00, 31.60s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [08:25<00:00, 31.60s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [08:25<00:00, 31.57s/it]\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:15<05:26, 25.15s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:21<05:23, 21.56s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:21<05:29, 21.97s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:40<05:02, 25.21s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:43<05:02, 21.61s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:43<05:06, 21.89s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [02:06<04:37, 25.23s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:04<04:40, 21.58s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:05<04:43, 21.79s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:26<04:18, 21.54s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:31<04:12, 25.27s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:27<04:21, 21.76s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:47<03:56, 21.52s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [02:56<03:47, 25.33s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0234, 'grad_norm': 0.7927687168121338, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [03:22<03:22, 25.37s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:48<03:59, 21.75s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  23%|██▎       | 7/30 [59:16<3:02:50, 476.96s/it]*** -> Training took 505.1297 seconds.\n",
      "6.  27%|██▋       | 8/30 [1:01:10<3:11:55, 523.43s/it]retraining model for key '53fb4810' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1186.95 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1160.82 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  38%|███▊      | 6/16 [02:09<03:35, 21.52s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [03:22<03:22, 25.37s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:10<03:37, 21.76s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:30<03:13, 21.52s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0061, 'grad_norm': 0.45660945773124695, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:52<02:52, 21.52s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.   6%|▋         | 1/16 [00:14<03:39, 14.62s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:32<03:15, 21.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0039, 'grad_norm': 0.35104602575302124, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:54<02:53, 21.72s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.  56%|█████▋    | 9/16 [03:47<02:57, 25.40s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:29<03:24, 14.63s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:52<02:52, 21.52s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:43<03:10, 14.67s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [02:54<02:53, 21.72s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [04:13<02:32, 25.46s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [03:13<02:30, 21.51s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:58<02:56, 14.68s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [03:15<02:32, 21.71s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:13<02:41, 14.66s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [03:35<02:09, 21.50s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [04:38<02:07, 25.49s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:27<02:26, 14.65s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [03:37<02:10, 21.71s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:56<01:47, 21.50s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [05:04<01:41, 25.48s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:42<02:11, 14.61s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0026, 'grad_norm': 0.052767444401979446, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:57<01:56, 14.60s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:59<01:48, 21.72s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [01:57<01:56, 14.60s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [04:18<01:26, 21.51s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [05:29<01:16, 25.46s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [02:11<01:42, 14.60s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [04:20<01:26, 21.71s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [04:39<01:04, 21.51s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:26<01:27, 14.60s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [05:55<00:50, 25.47s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [04:42<01:05, 21.71s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [02:40<01:13, 14.62s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [05:01<00:43, 21.51s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [05:04<00:43, 21.71s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:55<00:58, 14.62s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [06:20<00:25, 25.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  27%|██▋       | 8/30 [1:07:03<3:01:24, 494.73s/it]\n",
      "7. {'loss': 0.01, 'grad_norm': 0.4063149094581604, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  27%|██▋       | 8/30 [1:10:27<3:01:24, 494.73s/it]\n",
      "7. {'train_runtime': 406.1544, 'train_samples_per_second': 0.315, 'train_steps_per_second': 0.039, 'train_loss': 0.016728307586163282, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [06:46<00:00, 25.45s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [06:46<00:00, 25.45s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [06:46<00:00, 25.45s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [06:46<00:00, 25.38s/it]\n",
      "4.  94%|█████████▍| 15/16 [05:22<00:21, 21.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  27%|██▋       | 8/30 [1:06:10<3:19:47, 544.91s/it]\n",
      "4. {'loss': 0.0013, 'grad_norm': 0.47741594910621643, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  27%|██▋       | 8/30 [1:09:02<3:19:47, 544.91s/it]\n",
      "4. {'train_runtime': 344.3067, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.046, 'train_loss': 0.003687505377456546, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [05:44<00:00, 21.50s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:44<00:00, 21.50s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:44<00:00, 21.50s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [05:44<00:00, 21.52s/it]\n",
      "6.  81%|████████▏ | 13/16 [03:10<00:43, 14.60s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [05:26<00:21, 21.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  20%|██        | 6/30 [56:50<3:00:28, 451.20s/it]\n",
      "5. {'loss': 0.0011, 'grad_norm': 0.05104278028011322, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  20%|██        | 6/30 [59:44<3:00:28, 451.20s/it]\n",
      "5. {'train_runtime': 347.7408, 'train_samples_per_second': 0.368, 'train_steps_per_second': 0.046, 'train_loss': 0.0025256011285819113, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [05:47<00:00, 21.70s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [05:47<00:00, 21.70s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [05:47<00:00, 21.70s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [05:47<00:00, 21.73s/it]\n",
      "6.  88%|████████▊ | 14/16 [03:24<00:29, 14.61s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [03:39<00:14, 14.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  27%|██▋       | 8/30 [1:03:10<3:11:55, 523.43s/it]\n",
      "6. {'loss': 0.0003, 'grad_norm': 0.0065006716176867485, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  27%|██▋       | 8/30 [1:05:07<3:11:55, 523.43s/it]\n",
      "6. {'train_runtime': 233.925, 'train_samples_per_second': 0.547, 'train_steps_per_second': 0.068, 'train_loss': 0.0014484730490949005, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [03:53<00:00, 14.61s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:53<00:00, 14.61s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:53<00:00, 14.61s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [03:53<00:00, 14.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  27%|██▋       | 8/30 [1:10:27<3:01:24, 494.73s/it]*** -> Training took 406.1544 seconds.\n",
      "7.  30%|███       | 9/30 [1:11:10<2:48:41, 481.96s/it]retraining model for key '332f06d7' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1156.24 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1134.18 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:15<03:56, 15.76s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:31<03:40, 15.76s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:47<03:25, 15.79s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:03<03:09, 15.80s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:19<02:54, 15.83s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  20%|██        | 6/30 [59:44<3:00:28, 451.20s/it]*** -> Training took 347.7408 seconds.\n",
      "5.  23%|██▎       | 7/30 [1:01:58<2:57:18, 462.53s/it]retraining model for key '38007db0' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1323.96 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  38%|███▊      | 6/16 [01:34<02:38, 15.85s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  27%|██▋       | 8/30 [1:09:02<3:19:47, 544.91s/it]*** -> Training took 344.3067 seconds.\n",
      "4.  30%|███       | 9/30 [1:11:50<3:07:35, 535.98s/it]retraining model for key '4c3d4a41' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 2100.10 examples/s]\n",
      "7.  44%|████▍     | 7/16 [01:50<02:22, 15.86s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0026, 'grad_norm': 0.31086745858192444, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:06<02:07, 15.88s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   6%|▋         | 1/16 [00:12<03:10, 12.72s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  50%|█████     | 8/16 [02:06<02:07, 15.88s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:07<01:57,  7.85s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:25<02:59, 12.80s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:15<01:49,  7.80s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:22<01:51, 15.89s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:38<02:46, 12.80s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:23<01:41,  7.79s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:31<01:33,  7.77s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:51<02:33, 12.76s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:38<01:35, 15.88s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:38<01:25,  7.76s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [00:46<01:17,  7.76s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:03<02:19, 12.72s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:54<01:19, 15.88s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [00:54<01:09,  7.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0262, 'grad_norm': 0.36836034059524536, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:02<01:01,  7.74s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:16<02:06, 12.69s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [01:02<01:01,  7.74s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [03:10<01:03, 15.88s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [01:09<00:54,  7.74s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:29<01:54, 12.68s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0205, 'grad_norm': 0.22525005042552948, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:41<01:41, 12.68s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:17<00:46,  7.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  27%|██▋       | 8/30 [1:05:07<3:11:55, 523.43s/it]*** -> Training took 233.925 seconds.\n",
      "6.  30%|███       | 9/30 [1:08:49<2:56:09, 503.31s/it]retraining model for key '6e453dd6' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1181.97 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1160.04 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  69%|██████▉   | 11/16 [01:25<00:38,  7.74s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [03:26<00:47, 15.88s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [01:41<01:41, 12.68s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [01:33<00:30,  7.73s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:54<01:28, 12.67s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [01:40<00:23,  7.74s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [03:42<00:31, 15.89s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [01:48<00:15,  7.74s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:14<03:33, 14.23s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:07<01:16, 12.67s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [01:56<00:07,  7.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  30%|███       | 9/30 [1:12:55<3:07:35, 535.98s/it]\n",
      "4. {'loss': 0.0013, 'grad_norm': 1.6327157020568848, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  30%|███       | 9/30 [1:13:57<3:07:35, 535.98s/it]\n",
      "4. {'train_runtime': 123.9769, 'train_samples_per_second': 1.032, 'train_steps_per_second': 0.129, 'train_loss': 0.013764757371973246, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:03<00:00,  7.74s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:03<00:00,  7.74s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:03<00:00,  7.74s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:03<00:00,  7.75s/it]\n",
      "7.  94%|█████████▍| 15/16 [03:57<00:15, 15.88s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  30%|███       | 9/30 [1:13:21<2:48:41, 481.96s/it]\n",
      "7. {'loss': 0.0005, 'grad_norm': 0.32568860054016113, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  30%|███       | 9/30 [1:15:28<2:48:41, 481.96s/it]\n",
      "7. {'train_runtime': 253.8248, 'train_samples_per_second': 0.504, 'train_steps_per_second': 0.063, 'train_loss': 0.001590103463968262, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [04:13<00:00, 15.88s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:13<00:00, 15.88s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:13<00:00, 15.88s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [04:13<00:00, 15.86s/it]\n",
      "5.  69%|██████▉   | 11/16 [02:19<01:03, 12.67s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:28<03:19, 14.25s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [02:32<00:50, 12.68s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:42<03:05, 14.28s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [02:45<00:38, 12.69s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:57<02:51, 14.30s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [02:57<00:25, 12.68s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:11<02:37, 14.30s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [03:10<00:12, 12.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  23%|██▎       | 7/30 [1:03:43<2:57:18, 462.53s/it]\n",
      "5. {'loss': 0.003, 'grad_norm': 0.31435272097587585, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  23%|██▎       | 7/30 [1:05:25<2:57:18, 462.53s/it]\n",
      "5. {'train_runtime': 203.0803, 'train_samples_per_second': 0.63, 'train_steps_per_second': 0.079, 'train_loss': 0.011754624196328223, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [03:23<00:00, 12.66s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:23<00:00, 12.66s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:23<00:00, 12.66s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [03:23<00:00, 12.69s/it]\n",
      "6.  38%|███▊      | 6/16 [01:25<02:23, 14.31s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:40<02:08, 14.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0146, 'grad_norm': 0.20179688930511475, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:54<01:54, 14.32s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6.  50%|█████     | 8/16 [01:54<01:54, 14.32s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [02:08<01:40, 14.32s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:23<01:25, 14.31s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [02:37<01:11, 14.32s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:51<00:57, 14.31s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [03:05<00:42, 14.31s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  30%|███       | 9/30 [1:13:57<3:07:35, 535.98s/it]*** -> Training took 123.9769 seconds.\n",
      "4.  33%|███▎      | 10/30 [1:16:44<2:33:45, 461.25s/it]retraining model for key '58f5dbd5' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 941.03 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 925.43 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n",
      "7.  30%|███       | 9/30 [1:15:28<2:48:41, 481.96s/it]*** -> Training took 253.8248 seconds.\n",
      "7.  33%|███▎      | 10/30 [1:18:25<2:35:45, 467.25s/it]retraining model for key '3a25b0d8' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1355.97 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n",
      "6.  88%|████████▊ | 14/16 [03:20<00:28, 14.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [03:34<00:14, 14.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  30%|███       | 9/30 [1:10:47<2:56:09, 503.31s/it]\n",
      "6. {'loss': 0.0039, 'grad_norm': 0.059993449598550797, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  30%|███       | 9/30 [1:12:42<2:56:09, 503.31s/it]\n",
      "6. {'train_runtime': 228.959, 'train_samples_per_second': 0.559, 'train_steps_per_second': 0.07, 'train_loss': 0.009262607200071216, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [03:48<00:00, 14.32s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:48<00:00, 14.32s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:48<00:00, 14.32s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [03:48<00:00, 14.31s/it]\n",
      "4.   6%|▋         | 1/16 [00:17<04:22, 17.51s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:12<03:14, 12.96s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  23%|██▎       | 7/30 [1:05:25<2:57:18, 462.53s/it]*** -> Training took 203.0803 seconds.\n",
      "5.  27%|██▋       | 8/30 [1:07:54<2:37:05, 428.44s/it]retraining model for key '5545f144' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1364.11 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:25<03:00, 12.92s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:35<04:05, 17.54s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:38<02:48, 12.92s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:12<03:09, 12.66s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:52<03:48, 17.58s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:51<02:35, 12.96s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:25<02:57, 12.65s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:10<03:31, 17.63s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:04<02:23, 13.01s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:37<02:44, 12.65s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:18<02:10, 13.06s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:50<02:31, 12.63s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:28<03:14, 17.64s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:31<01:57, 13.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0126, 'grad_norm': 0.6833515167236328, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:44<01:44, 13.12s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:03<02:18, 12.62s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:45<02:56, 17.63s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [01:44<01:44, 13.12s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:15<02:05, 12.59s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:03<02:38, 17.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0304, 'grad_norm': 1.027778148651123, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:20<02:20, 17.60s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:57<01:31, 13.13s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:28<01:53, 12.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0076, 'grad_norm': 0.06842312216758728, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:40<01:40, 12.59s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  30%|███       | 9/30 [1:12:42<2:56:09, 503.31s/it]*** -> Training took 228.959 seconds.\n",
      "6.  33%|███▎      | 10/30 [1:14:47<2:32:50, 458.55s/it]retraining model for key '7b0280bc' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 799.11 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 787.75 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:40<01:40, 12.59s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:10<01:18, 13.15s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:20<02:20, 17.60s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:53<01:28, 12.59s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:23<01:05, 13.17s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:38<02:03, 17.60s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:06<01:15, 12.59s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:37<00:52, 13.19s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [02:18<01:02, 12.59s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [02:56<01:45, 17.62s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [02:50<00:39, 13.20s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:21<05:23, 21.54s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [02:31<00:50, 12.59s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [03:03<00:26, 13.19s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:13<01:28, 17.62s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [02:43<00:37, 12.59s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:43<05:01, 21.55s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [03:16<00:13, 13.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  33%|███▎      | 10/30 [1:20:12<2:35:45, 467.25s/it]\n",
      "7. {'loss': 0.0009, 'grad_norm': 1.0549622774124146, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  33%|███▎      | 10/30 [1:21:58<2:35:45, 467.25s/it]\n",
      "7. {'train_runtime': 209.8548, 'train_samples_per_second': 0.61, 'train_steps_per_second': 0.076, 'train_loss': 0.006767401471734047, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [03:29<00:00, 13.16s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:29<00:00, 13.16s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:29<00:00, 13.16s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:29<00:00, 13.12s/it]\n",
      "4.  75%|███████▌  | 12/16 [03:31<01:10, 17.63s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [02:56<00:25, 12.59s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:04<04:39, 21.51s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [03:09<00:12, 12.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  27%|██▋       | 8/30 [1:09:38<2:37:05, 428.44s/it]\n",
      "5. {'loss': 0.0032, 'grad_norm': 0.8958998918533325, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  27%|██▋       | 8/30 [1:11:19<2:37:05, 428.44s/it]\n",
      "5. {'train_runtime': 201.6586, 'train_samples_per_second': 0.635, 'train_steps_per_second': 0.079, 'train_loss': 0.005392419756390154, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [03:21<00:00, 12.60s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:21<00:00, 12.60s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:21<00:00, 12.60s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [03:21<00:00, 12.60s/it]\n",
      "4.  81%|████████▏ | 13/16 [03:48<00:52, 17.63s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:25<04:17, 21.47s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [04:06<00:35, 17.63s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  33%|███▎      | 10/30 [1:21:58<2:35:45, 467.25s/it]*** -> Training took 209.8548 seconds.\n",
      "7.  37%|███▋      | 11/30 [1:22:44<2:07:47, 403.56s/it]retraining model for key '4c7dc4dd' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 917.69 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 902.05 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  94%|█████████▍| 15/16 [04:24<00:17, 17.64s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  33%|███▎      | 10/30 [1:19:09<2:33:45, 461.25s/it]\n",
      "4. {'loss': 0.006, 'grad_norm': 0.8989460468292236, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  33%|███▎      | 10/30 [1:21:30<2:33:45, 461.25s/it]\n",
      "4. {'train_runtime': 282.0379, 'train_samples_per_second': 0.454, 'train_steps_per_second': 0.057, 'train_loss': 0.018169240560382605, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:42<00:00, 17.67s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:42<00:00, 17.67s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:42<00:00, 17.67s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [04:42<00:00, 17.63s/it]\n",
      "6.  31%|███▏      | 5/16 [01:47<03:56, 21.46s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [02:08<03:34, 21.47s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:19<04:57, 19.81s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  33%|███▎      | 10/30 [1:21:30<2:33:45, 461.25s/it]*** -> Training took 282.0379 seconds.\n",
      "4.  37%|███▋      | 11/30 [1:22:10<2:12:58, 419.92s/it]retraining model for key '5961cc34' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 415.52 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 410.75 examples/s]\n",
      "6.  44%|████▍     | 7/16 [02:30<03:13, 21.47s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0076, 'grad_norm': 0.12882071733474731, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:51<02:51, 21.44s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:39<04:37, 19.80s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:51<02:51, 21.44s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:59<04:17, 19.79s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  27%|██▋       | 8/30 [1:11:19<2:37:05, 428.44s/it]*** -> Training took 201.6586 seconds.\n",
      "5.  30%|███       | 9/30 [1:13:06<2:17:14, 392.12s/it]retraining model for key '581f7754' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1218.53 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1194.52 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [03:13<02:30, 21.44s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:19<03:57, 19.80s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:14<03:31, 14.08s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:39<03:37, 19.81s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [03:34<02:08, 21.44s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:28<03:16, 14.04s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:39<09:59, 40.00s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:58<03:18, 19.83s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:42<03:02, 14.03s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [03:56<01:47, 21.45s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:56<02:48, 14.04s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [02:18<02:58, 19.84s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.074, 'grad_norm': 3.3172171115875244, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:38<02:38, 19.84s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.  75%|███████▌  | 12/16 [04:17<01:25, 21.48s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:10<02:34, 14.05s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [01:19<09:18, 39.89s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [02:38<02:38, 19.84s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:24<02:20, 14.06s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [04:39<01:04, 21.48s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:38<02:06, 14.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0058, 'grad_norm': 0.5094557404518127, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:52<01:52, 14.08s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:58<02:18, 19.84s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [05:00<00:42, 21.48s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [01:52<01:52, 14.08s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:59<08:37, 39.84s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [03:18<01:59, 19.84s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:06<01:38, 14.07s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [05:22<00:21, 21.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  33%|███▎      | 10/30 [1:17:43<2:32:50, 458.55s/it]\n",
      "6. {'loss': 0.0047, 'grad_norm': 0.1960277557373047, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:43<00:00, 21.48s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:43<00:00, 21.48s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  33%|███▎      | 10/30 [1:20:35<2:32:50, 458.55s/it]\n",
      "6. {'train_runtime': 343.5605, 'train_samples_per_second': 0.373, 'train_steps_per_second': 0.047, 'train_loss': 0.00612979126162827, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:43<00:00, 21.48s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [05:43<00:00, 21.47s/it]\n",
      "7.  69%|██████▉   | 11/16 [03:38<01:39, 19.84s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:20<01:24, 14.07s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [02:34<01:10, 14.07s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [02:39<07:57, 39.78s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [03:57<01:19, 19.83s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [02:48<00:56, 14.07s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [04:17<00:59, 19.82s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:02<00:42, 14.07s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [03:16<00:28, 14.08s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [03:18<07:17, 39.74s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [04:37<00:39, 19.82s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [03:31<00:14, 14.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  30%|███       | 9/30 [1:15:02<2:17:14, 392.12s/it]\n",
      "5. {'loss': 0.0008, 'grad_norm': 0.05141795799136162, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  30%|███       | 9/30 [1:16:54<2:17:14, 392.12s/it]\n",
      "5. {'train_runtime': 225.1004, 'train_samples_per_second': 0.569, 'train_steps_per_second': 0.071, 'train_loss': 0.003282766498159617, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [03:45<00:00, 14.08s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:45<00:00, 14.08s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:45<00:00, 14.08s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [03:45<00:00, 14.07s/it]\n",
      "7.  94%|█████████▍| 15/16 [04:57<00:19, 19.82s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  37%|███▋      | 11/30 [1:25:26<2:07:47, 403.56s/it]\n",
      "7. {'loss': 0.0095, 'grad_norm': 10.883174896240234, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  37%|███▋      | 11/30 [1:28:04<2:07:47, 403.56s/it]\n",
      "7. {'train_runtime': 317.1824, 'train_samples_per_second': 0.404, 'train_steps_per_second': 0.05, 'train_loss': 0.04173897113651037, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [05:17<00:00, 19.82s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:17<00:00, 19.82s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:17<00:00, 19.82s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [05:17<00:00, 19.82s/it]\n",
      "4.  38%|███▊      | 6/16 [03:58<06:37, 39.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  37%|███▋      | 11/30 [1:28:04<2:07:47, 403.56s/it]*** -> Training took 317.1824 seconds.\n",
      "7.  40%|████      | 12/30 [1:28:36<1:56:20, 387.79s/it]retraining model for key '4e34c42c' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1201.56 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1175.48 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [04:38<05:57, 39.71s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0016, 'grad_norm': 0.06306633353233337, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [05:18<05:18, 39.79s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.   6%|▋         | 1/16 [00:14<03:43, 14.90s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:29<03:28, 14.89s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:44<03:13, 14.88s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [05:18<05:18, 39.79s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:59<02:58, 14.87s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:14<02:43, 14.87s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  30%|███       | 9/30 [1:16:54<2:17:14, 392.12s/it]*** -> Training took 225.1004 seconds.\n",
      "5.  33%|███▎      | 10/30 [1:19:14<2:08:13, 384.70s/it]retraining model for key '58490d8a' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 963.27 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 946.98 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  56%|█████▋    | 9/16 [05:58<04:38, 39.82s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:29<02:28, 14.87s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:44<02:13, 14.87s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0183, 'grad_norm': 0.9747369289398193, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:58<01:58, 14.87s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [01:58<01:58, 14.87s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:17<04:20, 17.38s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [06:38<03:58, 39.82s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:13<01:44, 14.87s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:34<04:03, 17.43s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:28<01:29, 14.87s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:52<03:47, 17.50s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  33%|███▎      | 10/30 [1:20:35<2:32:50, 458.55s/it]*** -> Training took 343.5605 seconds.\n",
      "6.  37%|███▋      | 11/30 [1:25:30<2:43:03, 514.93s/it]retraining model for key '8698868d' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 996.18 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 977.95 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  69%|██████▉   | 11/16 [02:43<01:14, 14.87s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [07:17<03:19, 39.85s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:10<03:30, 17.54s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:58<00:59, 14.86s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:27<03:13, 17.57s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [03:13<00:44, 14.87s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:16<04:14, 16.96s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:45<02:55, 17.60s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [03:28<00:29, 14.89s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [07:57<02:39, 39.88s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:34<03:58, 17.01s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [03:43<00:14, 14.91s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.90s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.90s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.90s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  40%|████      | 12/30 [1:30:38<1:56:20, 387.79s/it]\n",
      "7. {'loss': 0.0011, 'grad_norm': 0.6665194630622864, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  40%|████      | 12/30 [1:32:37<1:56:20, 387.79s/it]\n",
      "7. {'train_runtime': 238.1081, 'train_samples_per_second': 0.538, 'train_steps_per_second': 0.067, 'train_loss': 0.00969544576946646, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  44%|████▍     | 7/16 [02:02<02:38, 17.63s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0166, 'grad_norm': 0.7819599509239197, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:20<02:21, 17.65s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:51<03:41, 17.03s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [02:20<02:21, 17.65s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:08<03:24, 17.04s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [08:37<01:59, 39.88s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  40%|████      | 12/30 [1:32:37<1:56:20, 387.79s/it]*** -> Training took 238.1081 seconds.\n",
      "7.  43%|████▎     | 13/30 [1:33:10<1:40:08, 353.46s/it]retraining model for key '62593bfd' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 603.22 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 595.10 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  56%|█████▋    | 9/16 [02:38<02:03, 17.68s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:25<03:07, 17.03s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:56<01:46, 17.69s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:42<02:50, 17.01s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [09:17<01:19, 39.87s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:59<02:33, 17.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0098, 'grad_norm': 0.3585757911205292, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:16<02:16, 17.03s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:13<01:28, 17.70s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:16<02:16, 17.03s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [03:31<01:10, 17.70s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:31<07:47, 31.19s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [09:57<00:39, 39.84s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  37%|███▋      | 11/30 [1:27:33<2:12:58, 419.92s/it]\n",
      "4. {'loss': 0.0002, 'grad_norm': 0.1908053159713745, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  37%|███▋      | 11/30 [1:32:52<2:12:58, 419.92s/it]\n",
      "4. {'train_runtime': 637.0568, 'train_samples_per_second': 0.201, 'train_steps_per_second': 0.025, 'train_loss': 0.0009092597611015663, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [10:37<00:00, 39.79s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [10:37<00:00, 39.79s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [10:37<00:00, 39.79s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [10:37<00:00, 39.82s/it]\n",
      "6.  56%|█████▋    | 9/16 [02:33<01:59, 17.02s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:49<00:53, 17.70s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:50<01:42, 17.01s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [04:06<00:35, 17.70s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [01:02<07:17, 31.22s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [03:07<01:25, 17.00s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [04:24<00:17, 17.71s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  33%|███▎      | 10/30 [1:21:38<2:08:13, 384.70s/it]\n",
      "5. {'loss': 0.0076, 'grad_norm': 0.3308018147945404, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  33%|███▎      | 10/30 [1:24:00<2:08:13, 384.70s/it]\n",
      "5. {'train_runtime': 282.4602, 'train_samples_per_second': 0.453, 'train_steps_per_second': 0.057, 'train_loss': 0.012092412449419498, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:42<00:00, 17.72s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:42<00:00, 17.72s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:42<00:00, 17.72s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:42<00:00, 17.65s/it]\n",
      "6.  75%|███████▌  | 12/16 [03:24<01:08, 17.00s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:33<06:46, 31.23s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [03:41<00:50, 17.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  33%|███▎      | 10/30 [1:24:00<2:08:13, 384.70s/it]*** -> Training took 282.4602 seconds.\n",
      "5.  37%|███▋      | 11/30 [1:24:36<1:55:41, 365.32s/it]retraining model for key '65b59efc' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1189.53 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1167.53 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  88%|████████▊ | 14/16 [03:58<00:33, 17.00s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [02:04<06:14, 31.24s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [04:15<00:16, 16.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  37%|███▋      | 11/30 [1:27:50<2:43:03, 514.93s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [04:32<00:00, 16.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:32<00:00, 16.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:32<00:00, 16.98s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [04:32<00:00, 17.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0009, 'grad_norm': 0.1422388106584549, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  37%|███▋      | 11/30 [1:30:06<2:43:03, 514.93s/it]\n",
      "6. {'train_runtime': 272.0739, 'train_samples_per_second': 0.47, 'train_steps_per_second': 0.059, 'train_loss': 0.005361829826142639, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   6%|▋         | 1/16 [00:14<03:38, 14.60s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [02:36<05:43, 31.26s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:29<03:24, 14.58s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:43<03:09, 14.56s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:58<02:54, 14.54s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [03:07<05:12, 31.29s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  37%|███▋      | 11/30 [1:30:06<2:43:03, 514.93s/it]*** -> Training took 272.0739 seconds.\n",
      "6.  40%|████      | 12/30 [1:30:53<2:16:57, 456.50s/it]retraining model for key '8f215267' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 517.85 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 511.22 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  31%|███▏      | 5/16 [01:12<02:39, 14.53s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:27<02:25, 14.52s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [03:38<04:41, 31.33s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0046, 'grad_norm': 0.0870102196931839, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [04:10<04:11, 31.39s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:41<02:10, 14.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0135, 'grad_norm': 0.3909570574760437, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:56<01:56, 14.51s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5.  50%|█████     | 8/16 [01:56<01:56, 14.51s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [04:10<04:11, 31.39s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:10<01:41, 14.51s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:34<08:31, 34.12s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:25<01:27, 14.51s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [04:41<03:39, 31.40s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [02:39<01:12, 14.51s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [01:08<07:56, 34.05s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [02:54<00:58, 14.51s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [05:13<03:08, 31.36s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:08<00:43, 14.50s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:42<07:21, 34.00s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [03:23<00:28, 14.50s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [05:44<02:36, 31.34s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [03:37<00:14, 14.50s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  37%|███▋      | 11/30 [1:26:35<1:55:41, 365.32s/it]\n",
      "5. {'loss': 0.0027, 'grad_norm': 0.4079092741012573, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  37%|███▋      | 11/30 [1:28:31<1:55:41, 365.32s/it]\n",
      "5. {'train_runtime': 232.2491, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.069, 'train_loss': 0.008108763489872217, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [03:52<00:00, 14.50s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:52<00:00, 14.50s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:52<00:00, 14.50s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [03:52<00:00, 14.52s/it]\n",
      "6.  25%|██▌       | 4/16 [02:16<06:48, 34.01s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [06:15<02:05, 31.34s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [02:50<06:14, 34.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  37%|███▋      | 11/30 [1:28:31<1:55:41, 365.32s/it]*** -> Training took 232.2491 seconds.\n",
      "5.  40%|████      | 12/30 [1:29:24<1:42:35, 341.98s/it]retraining model for key '6ffbe589' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 940.27 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 925.27 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  81%|████████▏ | 13/16 [06:47<01:33, 31.32s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  37%|███▋      | 11/30 [1:32:52<2:12:58, 419.92s/it]*** -> Training took 637.0568 seconds.\n",
      "4.  40%|████      | 12/30 [1:39:21<3:01:43, 605.76s/it]retraining model for key '5dbc8537' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1542.52 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  38%|███▊      | 6/16 [03:24<05:40, 34.01s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [07:18<01:02, 31.31s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:18<04:31, 18.08s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:10<02:37, 10.47s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:36<04:13, 18.08s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:20<02:25, 10.41s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [03:58<05:05, 33.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.004, 'grad_norm': 0.22683216631412506, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [04:32<04:32, 34.00s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.  94%|█████████▍| 15/16 [07:49<00:31, 31.29s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  43%|████▎     | 13/30 [1:37:25<1:40:08, 353.46s/it]\n",
      "7. {'loss': 0.0007, 'grad_norm': 0.08454418927431107, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  43%|████▎     | 13/30 [1:41:35<1:40:08, 353.46s/it]\n",
      "7. {'train_runtime': 500.9331, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.032, 'train_loss': 0.0026422272494528443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [08:20<00:00, 31.29s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [08:20<00:00, 31.29s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [08:20<00:00, 31.29s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [08:20<00:00, 31.31s/it]\n",
      "4.  19%|█▉        | 3/16 [00:31<02:15, 10.39s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:54<03:55, 18.09s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:41<02:04, 10.38s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:51<01:54, 10.38s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:12<03:36, 18.08s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [04:32<04:32, 34.00s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:02<01:43, 10.38s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:12<01:33, 10.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0189, 'grad_norm': 0.4354828894138336, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:23<01:23, 10.38s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:30<03:19, 18.11s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [01:23<01:23, 10.38s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:48<03:01, 18.12s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [05:06<03:58, 34.01s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [01:33<01:12, 10.39s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:43<01:02, 10.38s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:06<02:42, 18.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0226, 'grad_norm': 1.3693344593048096, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:24<02:24, 18.07s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  69%|██████▉   | 11/16 [01:54<00:51, 10.38s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [02:04<00:41, 10.37s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [02:24<02:24, 18.07s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [05:40<03:24, 34.00s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [02:14<00:31, 10.38s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:42<02:06, 18.05s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [02:25<00:20, 10.37s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [02:35<00:10, 10.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  40%|████      | 12/30 [1:40:47<3:01:43, 605.76s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:46<00:00, 10.37s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:46<00:00, 10.37s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0022, 'grad_norm': 0.028023062273859978, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  40%|████      | 12/30 [1:42:10<3:01:43, 605.76s/it]\n",
      "4. {'train_runtime': 166.0786, 'train_samples_per_second': 0.771, 'train_steps_per_second': 0.096, 'train_loss': 0.01058955246116966, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:46<00:00, 10.37s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:46<00:00, 10.38s/it]\n",
      "6.  69%|██████▉   | 11/16 [06:14<02:49, 34.00s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [03:00<01:48, 18.09s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:19<01:30, 18.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  40%|████      | 12/30 [1:42:10<3:01:43, 605.76s/it]*** -> Training took 166.0786 seconds.\n",
      "4.  43%|████▎     | 13/30 [1:42:40<2:16:40, 482.40s/it]retraining model for key '64efde09' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 589.27 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 580.46 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  75%|███████▌  | 12/16 [06:48<02:16, 34.00s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [03:37<01:12, 18.12s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:55<00:54, 18.10s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [07:22<01:42, 34.00s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [04:13<00:36, 18.09s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:28<07:10, 28.72s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [04:31<00:18, 18.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  40%|████      | 12/30 [1:31:52<1:42:35, 341.98s/it]\n",
      "5. {'loss': 0.0068, 'grad_norm': 1.3293839693069458, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:49<00:00, 18.13s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  40%|████      | 12/30 [1:34:17<1:42:35, 341.98s/it]\n",
      "5. {'train_runtime': 289.6147, 'train_samples_per_second': 0.442, 'train_steps_per_second': 0.055, 'train_loss': 0.01468356722034514, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:49<00:00, 18.13s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:49<00:00, 18.13s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:49<00:00, 18.10s/it]\n",
      "6.  88%|████████▊ | 14/16 [07:56<01:07, 33.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  40%|████      | 12/30 [1:34:17<1:42:35, 341.98s/it]*** -> Training took 289.6147 seconds.\n",
      "5.  43%|████▎     | 13/30 [1:34:27<1:33:33, 330.22s/it]retraining model for key '800d221b' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 984.70 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 967.73 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  12%|█▎        | 2/16 [00:57<06:42, 28.75s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [08:30<00:34, 34.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  40%|████      | 12/30 [1:35:29<2:16:57, 456.50s/it]\n",
      "6. {'loss': 0.0014, 'grad_norm': 0.1596417874097824, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  40%|████      | 12/30 [1:40:01<2:16:57, 456.50s/it]\n",
      "6. {'train_runtime': 544.0317, 'train_samples_per_second': 0.235, 'train_steps_per_second': 0.029, 'train_loss': 0.0026847630506381392, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [09:04<00:00, 33.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [09:04<00:00, 33.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [09:04<00:00, 33.98s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [09:04<00:00, 34.00s/it]\n",
      "5.   6%|▋         | 1/16 [00:17<04:21, 17.41s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:26<06:14, 28.78s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:34<04:04, 17.48s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:55<05:46, 28.86s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:52<03:47, 17.47s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:09<03:29, 17.43s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [02:24<05:17, 28.87s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:27<03:11, 17.39s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:44<02:53, 17.35s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [02:53<04:48, 28.86s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:01<02:36, 17.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0023, 'grad_norm': 0.20553365349769592, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:19<02:18, 17.37s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  43%|████▎     | 13/30 [1:41:35<1:40:08, 353.46s/it]*** -> Training took 500.9331 seconds.\n",
      "7.  47%|████▋     | 14/30 [1:48:00<2:17:28, 515.54s/it]retraining model for key '67e490f4' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 813.04 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 798.75 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  44%|████▍     | 7/16 [03:21<04:19, 28.85s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.004, 'grad_norm': 0.137748122215271, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [03:50<03:50, 28.86s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5.  50%|█████     | 8/16 [02:19<02:18, 17.37s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:36<02:01, 17.39s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  50%|█████     | 8/16 [03:50<03:50, 28.86s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:53<01:44, 17.38s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:22<05:41, 22.73s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:11<01:26, 17.38s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [04:19<03:22, 28.86s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:45<05:17, 22.71s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [03:28<01:09, 17.39s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [04:48<02:53, 28.87s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:08<04:55, 22.70s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:46<00:52, 17.39s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [04:03<00:34, 17.38s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:30<04:32, 22.70s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [05:17<02:24, 28.87s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  40%|████      | 12/30 [1:40:01<2:16:57, 456.50s/it]*** -> Training took 544.0317 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  94%|█████████▍| 15/16 [04:20<00:17, 17.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  43%|████▎     | 13/30 [1:36:50<1:33:33, 330.22s/it]\n",
      "5. {'loss': 0.0005, 'grad_norm': 0.2673611640930176, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  43%|████▎     | 13/30 [1:39:09<1:33:33, 330.22s/it]\n",
      "5. {'train_runtime': 278.2424, 'train_samples_per_second': 0.46, 'train_steps_per_second': 0.058, 'train_loss': 0.0013723369338549674, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:38<00:00, 17.38s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:38<00:00, 17.38s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:38<00:00, 17.38s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:38<00:00, 17.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  43%|████▎     | 13/30 [1:44:09<2:38:29, 559.37s/it]retraining model for key '9aaea919' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 289.20 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 286.34 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  31%|███▏      | 5/16 [01:53<04:09, 22.70s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [05:46<01:55, 28.87s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:16<03:47, 22.71s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [06:15<01:26, 28.87s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [02:39<03:24, 22.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0105, 'grad_norm': 0.3553115129470825, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [03:01<03:01, 22.74s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [06:44<00:57, 28.91s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [03:01<03:01, 22.74s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [03:24<02:39, 22.73s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [07:13<00:28, 28.94s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  43%|████▎     | 13/30 [1:46:34<2:16:40, 482.40s/it]\n",
      "4. {'loss': 0.0008, 'grad_norm': 0.22575818002223969, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  43%|████▎     | 13/30 [1:50:26<2:16:40, 482.40s/it]\n",
      "4. {'train_runtime': 462.0083, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.035, 'train_loss': 0.0024166252405848354, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [07:41<00:00, 28.93s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [07:41<00:00, 28.93s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [07:42<00:00, 28.93s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [07:42<00:00, 28.88s/it]\n",
      "7.  62%|██████▎   | 10/16 [03:47<02:16, 22.73s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [01:02<15:36, 62.43s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [04:09<01:53, 22.72s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [04:32<01:30, 22.72s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [02:04<14:29, 62.08s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [04:55<01:08, 22.73s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [05:18<00:45, 22.72s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [05:40<00:22, 22.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  47%|████▋     | 14/30 [1:51:06<2:17:28, 515.54s/it]\n",
      "7. {'loss': 0.0007, 'grad_norm': 0.6450966596603394, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  47%|████▋     | 14/30 [1:54:08<2:17:28, 515.54s/it]\n",
      "7. {'train_runtime': 363.5274, 'train_samples_per_second': 0.352, 'train_steps_per_second': 0.044, 'train_loss': 0.005572533002123237, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [06:03<00:00, 22.72s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [06:03<00:00, 22.72s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [06:03<00:00, 22.72s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [06:03<00:00, 22.72s/it]\n",
      "6.  19%|█▉        | 3/16 [03:06<13:26, 62.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  47%|████▋     | 14/30 [1:54:08<2:17:28, 515.54s/it]*** -> Training took 363.5274 seconds.\n",
      "7.  50%|█████     | 15/30 [1:54:46<2:00:35, 482.35s/it]retraining model for key '7491f3cf' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1127.61 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1107.33 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n",
      "4.  43%|████▎     | 13/30 [1:50:26<2:16:40, 482.40s/it]*** -> Training took 462.0083 seconds.\n",
      "4.  47%|████▋     | 14/30 [1:53:31<2:22:15, 533.48s/it]retraining model for key '6e4f6532' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 752.20 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 739.82 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:16<04:10, 16.71s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [04:08<12:24, 62.01s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:33<03:53, 16.68s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:22<05:35, 22.39s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:50<03:36, 16.68s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:44<05:13, 22.37s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:06<03:20, 16.69s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [05:10<11:21, 61.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  43%|████▎     | 13/30 [1:39:09<1:33:33, 330.22s/it]*** -> Training took 278.2424 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  31%|███▏      | 5/16 [01:23<03:03, 16.71s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  47%|████▋     | 14/30 [1:45:28<1:54:41, 430.08s/it]retraining model for key '88e364bc' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 647.44 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 639.12 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  19%|█▉        | 3/16 [01:07<04:50, 22.36s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:40<02:47, 16.71s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:29<04:28, 22.38s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:56<02:30, 16.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0086, 'grad_norm': 0.20788699388504028, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:13<02:13, 16.72s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:51<04:06, 22.40s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [02:13<02:13, 16.72s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:27<06:49, 27.30s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [06:11<10:19, 61.95s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:30<01:56, 16.71s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [02:14<03:43, 22.40s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:47<01:40, 16.71s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:54<06:22, 27.30s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:36<03:21, 22.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0102, 'grad_norm': 0.39253416657447815, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:59<02:58, 22.36s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  69%|██████▉   | 11/16 [03:03<01:23, 16.73s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:21<05:55, 27.32s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:59<02:58, 22.36s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [03:20<01:06, 16.73s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [07:13<09:17, 61.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0051, 'grad_norm': 0.080276720225811, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [08:15<08:15, 61.93s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.  81%|████████▏ | 13/16 [03:37<00:50, 16.72s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [03:21<02:36, 22.34s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:49<05:27, 27.31s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [03:53<00:33, 16.72s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [03:43<02:14, 22.34s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [04:10<00:16, 16.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 15/30 [1:57:03<2:00:35, 482.35s/it]\n",
      "7. {'loss': 0.004, 'grad_norm': 0.14301863312721252, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  50%|█████     | 15/30 [1:59:17<2:00:35, 482.35s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [04:27<00:00, 16.71s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:27<00:00, 16.71s/it]\u001b[A\n",
      "7. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'train_runtime': 267.4178, 'train_samples_per_second': 0.479, 'train_steps_per_second': 0.06, 'train_loss': 0.006332793738692999, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [04:27<00:00, 16.71s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [04:27<00:00, 16.71s/it]\n",
      "5.  31%|███▏      | 5/16 [02:16<05:00, 27.31s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [08:15<08:15, 61.93s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [04:05<01:51, 22.34s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:43<04:33, 27.32s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [04:28<01:29, 22.33s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [03:11<04:05, 27.31s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [03:38<03:38, 27.32s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0029, 'grad_norm': 0.060749270021915436, 'learning_rate': 5e-05, 'epoch': 0.5}\n",
      "7.  50%|█████     | 15/30 [1:59:17<2:00:35, 482.35s/it]*** -> Training took 267.4178 seconds.\n",
      "7.  53%|█████▎    | 16/30 [2:00:17<1:41:57, 436.95s/it]retraining model for key '7666fa5d' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1527.44 examples/s]\n",
      "4.  81%|████████▏ | 13/16 [04:50<01:06, 22.31s/it]\u001b[A\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [09:17<07:13, 61.92s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [03:38<03:38, 27.32s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [05:12<00:44, 22.30s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:11<02:47, 11.20s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:22<02:36, 11.18s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [05:35<00:22, 22.29s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  47%|████▋     | 14/30 [1:56:34<2:22:15, 533.48s/it]\n",
      "4. {'loss': 0.0023, 'grad_norm': 0.18445579707622528, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [05:57<00:00, 22.28s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:57<00:00, 22.28s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:57<00:00, 22.28s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [05:57<00:00, 22.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  47%|████▋     | 14/30 [1:59:32<2:22:15, 533.48s/it]\n",
      "4. {'train_runtime': 357.374, 'train_samples_per_second': 0.358, 'train_steps_per_second': 0.045, 'train_loss': 0.006254275445826352, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  19%|█▉        | 3/16 [00:33<02:25, 11.19s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [04:05<03:11, 27.32s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:44<02:14, 11.19s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:55<02:03, 11.21s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [04:33<02:43, 27.32s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [10:19<06:11, 61.89s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:07<01:52, 11.22s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:18<01:41, 11.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0161, 'grad_norm': 0.28719577193260193, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:29<01:29, 11.24s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [01:29<01:29, 11.24s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [05:00<02:16, 27.31s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:40<01:18, 11.24s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [01:52<01:07, 11.25s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [05:27<01:49, 27.32s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:03<00:56, 11.26s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [11:21<05:09, 61.89s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:14<00:45, 11.26s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [05:55<01:21, 27.32s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [02:26<00:33, 11.27s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [02:37<00:22, 11.28s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [02:48<00:11, 11.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  53%|█████▎    | 16/30 [2:01:50<1:41:57, 436.95s/it]\n",
      "7. {'loss': 0.0047, 'grad_norm': 0.07511958479881287, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  53%|█████▎    | 16/30 [2:03:20<1:41:57, 436.95s/it]\n",
      "7. {'train_runtime': 179.9703, 'train_samples_per_second': 0.711, 'train_steps_per_second': 0.089, 'train_loss': 0.010388202033936977, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [02:59<00:00, 11.28s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [02:59<00:00, 11.28s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [02:59<00:00, 11.28s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [02:59<00:00, 11.25s/it]\n",
      "5.  88%|████████▊ | 14/16 [06:22<00:54, 27.31s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [12:23<04:07, 61.89s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [06:49<00:27, 27.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  47%|████▋     | 14/30 [1:49:11<1:54:41, 430.08s/it]\n",
      "5. {'loss': 0.0004, 'grad_norm': 0.14605487883090973, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  47%|████▋     | 14/30 [1:52:49<1:54:41, 430.08s/it]\n",
      "5. {'train_runtime': 437.0188, 'train_samples_per_second': 0.293, 'train_steps_per_second': 0.037, 'train_loss': 0.0016805720515549183, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [07:17<00:00, 27.31s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [07:17<00:00, 27.31s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [07:17<00:00, 27.31s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [07:17<00:00, 27.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  47%|████▋     | 14/30 [1:59:32<2:22:15, 533.48s/it]*** -> Training took 357.374 seconds.\n",
      "4.  50%|█████     | 15/30 [2:03:06<2:16:30, 546.06s/it]retraining model for key '71e489b6' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 903.14 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 888.13 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  81%|████████▏ | 13/16 [13:25<03:05, 61.88s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:18<04:36, 18.40s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  53%|█████▎    | 16/30 [2:03:20<1:41:57, 436.95s/it]*** -> Training took 179.9703 seconds.\n",
      "7.  57%|█████▋    | 17/30 [2:05:31<1:26:37, 399.85s/it]retraining model for key '78332cb0' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 2361.62 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  12%|█▎        | 2/16 [00:36<04:16, 18.35s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [14:27<02:03, 61.89s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:07<01:47,  7.16s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:55<03:58, 18.35s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:14<01:39,  7.13s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:21<01:32,  7.14s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:28<01:25,  7.15s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:13<03:39, 18.33s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:35<01:18,  7.15s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [00:42<01:11,  7.15s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:31<03:21, 18.34s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [00:50<01:04,  7.15s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0067, 'grad_norm': 0.323025107383728, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [00:57<00:57,  7.16s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [00:57<00:57,  7.16s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:04<00:50,  7.16s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [15:28<01:01, 61.88s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  43%|████▎     | 13/30 [1:52:30<2:38:29, 559.37s/it]\n",
      "6. {'loss': 0.0013, 'grad_norm': 0.046533357352018356, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  43%|████▎     | 13/30 [2:00:45<2:38:29, 559.37s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [16:30<00:00, 61.89s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [16:30<00:00, 61.89s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'train_runtime': 990.8455, 'train_samples_per_second': 0.129, 'train_steps_per_second': 0.016, 'train_loss': 0.00324725522659719, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [16:30<00:00, 61.89s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [16:30<00:00, 61.93s/it]\n",
      "4.  38%|███▊      | 6/16 [01:50<03:03, 18.34s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [01:11<00:42,  7.16s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [01:18<00:35,  7.16s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [01:25<00:28,  7.17s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:08<02:45, 18.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.007, 'grad_norm': 0.3824373185634613, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:26<02:26, 18.34s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  81%|████████▏ | 13/16 [01:33<00:21,  7.17s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [01:40<00:14,  7.18s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:26<02:26, 18.34s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [01:47<00:07,  7.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  57%|█████▋    | 17/30 [2:06:31<1:26:37, 399.85s/it]\n",
      "7. {'loss': 0.0021, 'grad_norm': 0.07523422688245773, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  57%|█████▋    | 17/30 [2:07:28<1:26:37, 399.85s/it]\n",
      "7. {'train_runtime': 114.7096, 'train_samples_per_second': 1.116, 'train_steps_per_second': 0.139, 'train_loss': 0.004440664779394865, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [01:54<00:00,  7.20s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [01:54<00:00,  7.20s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [01:54<00:00,  7.20s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [01:54<00:00,  7.17s/it]\n",
      "4.  56%|█████▋    | 9/16 [02:45<02:08, 18.35s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [03:03<01:50, 18.36s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:21<01:31, 18.35s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [03:40<01:13, 18.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  57%|█████▋    | 17/30 [2:07:28<1:26:37, 399.85s/it]*** -> Training took 114.7096 seconds.\n",
      "7.  60%|██████    | 18/30 [2:08:50<1:07:56, 339.71s/it]retraining model for key '7b3084d4' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1200.14 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1176.53 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  81%|████████▏ | 13/16 [03:58<00:55, 18.36s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [04:16<00:36, 18.36s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:14<03:41, 14.79s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [04:35<00:18, 18.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 15/30 [2:05:37<2:16:30, 546.06s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:53<00:00, 18.36s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:53<00:00, 18.36s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:53<00:00, 18.36s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [04:53<00:00, 18.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.003, 'grad_norm': 0.04418440908193588, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  50%|█████     | 15/30 [2:08:04<2:16:30, 546.06s/it]\n",
      "4. {'train_runtime': 293.6519, 'train_samples_per_second': 0.436, 'train_steps_per_second': 0.054, 'train_loss': 0.004976104479283094, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:29<03:26, 14.78s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:44<03:12, 14.80s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  47%|████▋     | 14/30 [1:52:49<1:54:41, 430.08s/it]*** -> Training took 437.0188 seconds.\n",
      "5.  50%|█████     | 15/30 [1:58:58<2:16:07, 544.50s/it]retraining model for key '898e7135' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 954.82 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 938.20 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  25%|██▌       | 4/16 [00:59<02:57, 14.83s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:14<02:43, 14.85s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:29<02:28, 14.87s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:17<04:27, 17.86s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:43<02:13, 14.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0281, 'grad_norm': 3.3231728076934814, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:58<01:59, 14.90s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:36<04:12, 18.03s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [01:58<01:59, 14.90s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:53<03:53, 17.98s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:13<01:44, 14.90s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:11<03:35, 17.94s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:28<01:29, 14.92s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:29<03:16, 17.91s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:43<01:14, 14.93s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:58<00:59, 14.95s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:47<02:58, 17.89s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [03:13<00:44, 14.96s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:05<02:41, 17.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0147, 'grad_norm': 0.4408934712409973, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:23<02:23, 17.90s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.  88%|████████▊ | 14/16 [03:28<00:29, 14.96s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 15/30 [2:08:04<2:16:30, 546.06s/it]*** -> Training took 293.6519 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:23<02:23, 17.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  53%|█████▎    | 16/30 [2:11:14<2:03:18, 528.48s/it]retraining model for key '7b80bb43' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 675.54 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 666.25 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  94%|█████████▍| 15/16 [03:43<00:14, 14.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  60%|██████    | 18/30 [2:10:52<1:07:56, 339.71s/it]\n",
      "7. {'loss': 0.0047, 'grad_norm': 4.65121603012085, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  60%|██████    | 18/30 [2:12:52<1:07:56, 339.71s/it]\n",
      "7. {'train_runtime': 238.5718, 'train_samples_per_second': 0.537, 'train_steps_per_second': 0.067, 'train_loss': 0.01638570101931691, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.95s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.95s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.95s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:58<00:00, 14.91s/it]\n",
      "5.  56%|█████▋    | 9/16 [02:41<02:05, 17.90s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:59<01:47, 17.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  60%|██████    | 18/30 [2:12:52<1:07:56, 339.71s/it]*** -> Training took 238.5718 seconds.\n",
      "7.  63%|██████▎   | 19/30 [2:13:24<58:38, 319.83s/it]  retraining model for key '7b5033c1' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 2500.41 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:24<06:11, 24.77s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:17<01:29, 17.89s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:07<01:45,  7.06s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:14<01:38,  7.02s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:21<01:30,  7.00s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [03:34<01:11, 17.90s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [00:28<01:23,  7.00s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:49<05:47, 24.80s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:35<01:16,  7.00s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:52<00:53, 17.91s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [00:42<01:09,  7.00s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [00:49<01:02,  7.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.1034, 'grad_norm': 2.891230583190918, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [00:56<00:55,  7.00s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:14<05:22, 24.79s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [00:56<00:55,  7.00s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [04:10<00:35, 17.90s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:03<00:48,  7.00s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [01:10<00:41,  7.00s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [04:28<00:17, 17.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 15/30 [2:01:25<2:16:07, 544.50s/it]\n",
      "5. {'loss': 0.003, 'grad_norm': 0.46902909874916077, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  50%|█████     | 15/30 [2:03:48<2:16:07, 544.50s/it]\n",
      "5. {'train_runtime': 286.5561, 'train_samples_per_second': 0.447, 'train_steps_per_second': 0.056, 'train_loss': 0.008831107523292303, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:46<00:00, 17.90s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:46<00:00, 17.90s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:46<00:00, 17.90s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:46<00:00, 17.91s/it]\n",
      "7.  69%|██████▉   | 11/16 [01:17<00:34,  7.00s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:39<04:57, 24.80s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  43%|████▎     | 13/30 [2:00:45<2:38:29, 559.37s/it]*** -> Training took 990.8455 seconds.\n",
      "6.  47%|████▋     | 14/30 [2:08:54<3:43:43, 838.99s/it]retraining model for key 'a251c730' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 721.16 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 711.11 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n",
      "7.  75%|███████▌  | 12/16 [01:24<00:27,  7.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  81%|████████▏ | 13/16 [01:31<00:20,  7.00s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [01:38<00:13,  7.00s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [02:04<04:33, 24.83s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [01:44<00:06,  7.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  63%|██████▎   | 19/30 [2:14:23<58:38, 319.83s/it]\n",
      "7. {'loss': 0.0149, 'grad_norm': 2.5220959186553955, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  63%|██████▎   | 19/30 [2:15:19<58:38, 319.83s/it]\n",
      "7. {'train_runtime': 112.0054, 'train_samples_per_second': 1.143, 'train_steps_per_second': 0.143, 'train_loss': 0.059156931936740875, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [01:51<00:00,  7.00s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [01:51<00:00,  7.00s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [01:51<00:00,  7.00s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [01:51<00:00,  7.00s/it]\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  63%|██████▎   | 19/30 [2:15:19<58:38, 319.83s/it]*** -> Training took 112.0054 seconds.\n",
      "7.  67%|██████▋   | 20/30 [2:15:33<43:45, 262.59s/it]retraining model for key '80a900e0' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 848.54 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 836.22 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  38%|███▊      | 6/16 [02:28<04:08, 24.86s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:23<05:54, 23.64s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 15/30 [2:03:48<2:16:07, 544.50s/it]*** -> Training took 286.5561 seconds.\n",
      "5.  53%|█████▎    | 16/30 [2:05:01<1:54:20, 490.03s/it]retraining model for key '8b7bacbf' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 720.18 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 710.63 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  44%|████▍     | 7/16 [02:54<03:44, 24.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0014, 'grad_norm': 0.018607618287205696, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [03:19<03:19, 24.96s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:47<05:30, 23.64s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:21<05:26, 21.77s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  50%|█████     | 8/16 [03:19<03:19, 24.96s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:10<05:06, 23.60s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:43<05:04, 21.74s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:24<06:08, 24.54s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:34<04:42, 23.56s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [03:44<02:54, 25.00s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:05<04:42, 21.75s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:49<05:43, 24.51s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:57<04:19, 23.56s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [04:09<02:30, 25.02s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:27<04:21, 21.77s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [02:21<03:55, 23.57s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:13<05:18, 24.49s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:48<03:59, 21.79s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [04:34<02:05, 25.03s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [02:45<03:32, 23.58s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0174, 'grad_norm': 0.8394134044647217, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [03:08<03:08, 23.58s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:10<03:38, 21.80s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:37<04:53, 24.49s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [04:59<01:40, 25.04s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [02:32<03:16, 21.81s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0068, 'grad_norm': 0.09816405177116394, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:54<02:54, 21.81s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "6.  50%|█████     | 8/16 [03:08<03:08, 23.58s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [02:02<04:29, 24.50s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [05:24<01:15, 25.04s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [02:54<02:54, 21.81s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [03:32<02:44, 23.56s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:26<04:04, 24.49s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [05:49<00:50, 25.04s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [03:16<02:32, 21.82s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [03:55<02:21, 23.55s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:51<03:40, 24.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0012, 'grad_norm': 0.37267789244651794, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [03:16<03:16, 24.51s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  94%|█████████▍| 15/16 [06:14<00:25, 25.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  53%|█████▎    | 16/30 [2:14:37<2:03:18, 528.48s/it]\n",
      "4. {'loss': 0.0, 'grad_norm': 0.00011745141819119453, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  53%|█████▎    | 16/30 [2:17:57<2:03:18, 528.48s/it]\n",
      "4. {'train_runtime': 399.4796, 'train_samples_per_second': 0.32, 'train_steps_per_second': 0.04, 'train_loss': 0.0006949786638870137, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [06:39<00:00, 25.04s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [06:39<00:00, 25.04s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [06:39<00:00, 25.04s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [06:39<00:00, 24.97s/it]\n",
      "7.  62%|██████▎   | 10/16 [03:38<02:10, 21.82s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [04:19<01:57, 23.55s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [03:16<03:16, 24.51s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [03:59<01:49, 21.82s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [04:42<01:34, 23.56s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [03:40<02:51, 24.52s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [04:21<01:27, 21.83s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [05:06<01:10, 23.56s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [04:05<02:27, 24.51s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [04:43<01:05, 21.83s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [05:29<00:47, 23.56s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [04:29<02:02, 24.51s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [05:05<00:43, 21.82s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [05:53<00:23, 23.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  47%|████▋     | 14/30 [2:12:07<3:43:43, 838.99s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [06:17<00:00, 23.55s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [06:17<00:00, 23.55s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0032, 'grad_norm': 0.4546780586242676, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  47%|████▋     | 14/30 [2:15:15<3:43:43, 838.99s/it]\n",
      "6. {'train_runtime': 377.0307, 'train_samples_per_second': 0.339, 'train_steps_per_second': 0.042, 'train_loss': 0.010329776909202337, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [06:17<00:00, 23.55s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [06:17<00:00, 23.56s/it]\n",
      "5.  75%|███████▌  | 12/16 [04:54<01:38, 24.50s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [05:27<00:21, 21.82s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  67%|██████▋   | 20/30 [2:18:31<43:45, 262.59s/it]\n",
      "7. {'loss': 0.001, 'grad_norm': 0.18198223412036896, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  67%|██████▋   | 20/30 [2:21:25<43:45, 262.59s/it]\n",
      "7. {'train_runtime': 348.9738, 'train_samples_per_second': 0.367, 'train_steps_per_second': 0.046, 'train_loss': 0.0039198032463900745, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [05:48<00:00, 21.82s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:48<00:00, 21.82s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:48<00:00, 21.82s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [05:48<00:00, 21.81s/it]\n",
      "5.  81%|████████▏ | 13/16 [05:18<01:13, 24.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  47%|████▋     | 14/30 [2:15:15<3:43:43, 838.99s/it]*** -> Training took 377.0307 seconds.\n",
      "6.  50%|█████     | 15/30 [2:15:57<2:58:21, 713.43s/it]retraining model for key 'a32d8b75' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 482.46 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 476.35 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  88%|████████▊ | 14/16 [05:43<00:49, 24.50s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  67%|██████▋   | 20/30 [2:21:25<43:45, 262.59s/it]*** -> Training took 348.9738 seconds.\n",
      "7.  70%|███████   | 21/30 [2:22:24<46:03, 307.02s/it]retraining model for key '8e5c0c38' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 866.18 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 851.61 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [06:07<00:24, 24.50s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  53%|█████▎    | 16/30 [2:08:21<1:54:20, 490.03s/it]\n",
      "5. {'loss': 0.0008, 'grad_norm': 0.23495741188526154, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  53%|█████▎    | 16/30 [2:11:37<1:54:20, 490.03s/it]\n",
      "5. {'train_runtime': 391.9914, 'train_samples_per_second': 0.327, 'train_steps_per_second': 0.041, 'train_loss': 0.0009631152788642794, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [06:31<00:00, 24.49s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [06:31<00:00, 24.49s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [06:31<00:00, 24.49s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [06:31<00:00, 24.50s/it]\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:21<05:18, 21.27s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:36<09:13, 36.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  53%|█████▎    | 16/30 [2:17:57<2:03:18, 528.48s/it]*** -> Training took 399.4796 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  12%|█▎        | 2/16 [00:42<04:56, 21.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  57%|█████▋    | 17/30 [2:22:00<2:02:09, 563.82s/it]retraining model for key '7c66cb00' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 336.72 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 333.31 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  12%|█▎        | 2/16 [01:13<08:35, 36.79s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:03<04:34, 21.15s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:24<04:13, 21.13s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:50<07:56, 36.69s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:45<03:52, 21.16s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:07<03:31, 21.18s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [02:26<07:20, 36.71s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:48<12:03, 48.20s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [02:28<03:10, 21.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0033, 'grad_norm': 0.08343151956796646, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:49<02:49, 21.17s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [02:49<02:49, 21.17s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [03:03<06:43, 36.72s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [03:10<02:28, 21.17s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [01:36<11:13, 48.11s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [03:40<06:07, 36.70s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [03:31<02:07, 21.17s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [03:52<01:45, 21.18s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [02:24<10:25, 48.08s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [04:17<05:30, 36.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0072, 'grad_norm': 0.6384323239326477, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [04:53<04:52, 36.60s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.  75%|███████▌  | 12/16 [04:14<01:24, 21.19s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [04:35<01:03, 21.18s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [04:53<04:52, 36.60s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [03:12<09:37, 48.11s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [04:56<00:42, 21.18s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [05:17<00:21, 21.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  70%|███████   | 21/30 [2:25:16<46:03, 307.02s/it]\n",
      "7. {'loss': 0.0, 'grad_norm': 0.00422581285238266, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  70%|███████   | 21/30 [2:28:06<46:03, 307.02s/it]\n",
      "7. {'train_runtime': 338.7667, 'train_samples_per_second': 0.378, 'train_steps_per_second': 0.047, 'train_loss': 0.00168077244052256, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [05:38<00:00, 21.16s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:38<00:00, 21.16s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:38<00:00, 21.16s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [05:38<00:00, 21.17s/it]\n",
      "6.  56%|█████▋    | 9/16 [05:30<04:16, 36.63s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [04:00<08:49, 48.16s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [06:06<03:39, 36.64s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [04:48<08:01, 48.18s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [06:43<03:03, 36.67s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [07:20<02:26, 36.69s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [05:37<07:13, 48.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.005, 'grad_norm': 0.1763349026441574, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [06:25<06:25, 48.19s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  53%|█████▎    | 16/30 [2:11:37<1:54:20, 490.03s/it]*** -> Training took 391.9914 seconds.\n",
      "5.  57%|█████▋    | 17/30 [2:19:03<2:09:05, 595.79s/it]retraining model for key '8f3a5a89' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 783.71 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 771.58 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [07:56<01:50, 36.68s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:17<04:19, 17.28s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [06:25<06:25, 48.19s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:34<04:02, 17.34s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [08:33<01:13, 36.69s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:52<03:45, 17.38s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:09<03:28, 17.39s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [07:13<05:37, 48.21s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [09:10<00:36, 36.70s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 15/30 [2:20:54<2:58:21, 713.43s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [09:46<00:00, 36.66s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0017, 'grad_norm': 0.2199534773826599, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  50%|█████     | 15/30 [2:25:48<2:58:21, 713.43s/it]\n",
      "6. {'train_runtime': 586.9133, 'train_samples_per_second': 0.218, 'train_steps_per_second': 0.027, 'train_loss': 0.004479747556615621, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [09:46<00:00, 36.66s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [09:46<00:00, 36.66s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [09:46<00:00, 36.68s/it]\n",
      "5.  31%|███▏      | 5/16 [01:26<03:11, 17.38s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:44<02:53, 17.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  70%|███████   | 21/30 [2:28:06<46:03, 307.02s/it]*** -> Training took 338.7667 seconds.\n",
      "7.  73%|███████▎  | 22/30 [2:32:16<52:20, 392.54s/it]retraining model for key '9385bd28' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1190.27 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1167.70 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  62%|██████▎   | 10/16 [08:01<04:49, 48.24s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:01<02:36, 17.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0058, 'grad_norm': 0.05217922106385231, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:18<02:19, 17.38s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  50%|█████     | 8/16 [02:18<02:19, 17.38s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:15<03:47, 15.18s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:36<02:01, 17.39s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:30<03:32, 15.17s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [08:50<04:01, 48.22s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:53<01:44, 17.39s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:45<03:17, 15.18s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:00<03:02, 15.20s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:11<01:26, 17.38s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:15<02:47, 15.21s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [03:28<01:09, 17.37s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [09:38<03:12, 48.22s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:31<02:32, 15.22s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:45<00:52, 17.37s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:46<02:17, 15.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0027, 'grad_norm': 0.2349514663219452, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:01<02:01, 15.23s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "5.  88%|████████▊ | 14/16 [04:03<00:34, 17.37s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [02:01<02:01, 15.23s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [04:20<00:17, 17.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  57%|█████▋    | 17/30 [2:21:25<2:09:05, 595.79s/it]\n",
      "5. {'loss': 0.0029, 'grad_norm': 0.035656873136758804, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  57%|█████▋    | 17/30 [2:23:44<2:09:05, 595.79s/it]\n",
      "5. {'train_runtime': 277.8992, 'train_samples_per_second': 0.461, 'train_steps_per_second': 0.058, 'train_loss': 0.0043258717050775886, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:37<00:00, 17.35s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:37<00:00, 17.35s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:37<00:00, 17.35s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:37<00:00, 17.37s/it]\n",
      "4.  81%|████████▏ | 13/16 [10:26<02:24, 48.21s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:16<01:46, 15.24s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [02:32<01:31, 15.25s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:47<01:16, 15.26s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [11:14<01:36, 48.19s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [03:02<01:01, 15.27s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [03:18<00:45, 15.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  57%|█████▋    | 17/30 [2:23:44<2:09:05, 595.79s/it]*** -> Training took 277.8992 seconds.\n",
      "5.  60%|██████    | 18/30 [2:25:01<1:44:50, 524.24s/it]retraining model for key '97d7923e' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1694.87 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  88%|████████▊ | 14/16 [03:33<00:30, 15.28s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [03:48<00:15, 15.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  73%|███████▎  | 22/30 [2:34:21<52:20, 392.54s/it]\n",
      "7. {'loss': 0.0009, 'grad_norm': 0.31774258613586426, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  73%|███████▎  | 22/30 [2:36:23<52:20, 392.54s/it]\n",
      "7. {'train_runtime': 243.9953, 'train_samples_per_second': 0.525, 'train_steps_per_second': 0.066, 'train_loss': 0.001798907178454101, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [04:03<00:00, 15.28s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:03<00:00, 15.28s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:03<00:00, 15.28s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [04:03<00:00, 15.25s/it]\n",
      "5.   6%|▋         | 1/16 [00:09<02:26,  9.77s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [12:02<00:48, 48.22s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [12:51<00:00, 48.26s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [12:51<00:00, 48.26s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  57%|█████▋    | 17/30 [2:28:30<2:02:09, 563.82s/it]\n",
      "4. {'loss': 0.0011, 'grad_norm': 0.2695651352405548, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  57%|█████▋    | 17/30 [2:34:56<2:02:09, 563.82s/it]\n",
      "4. {'train_runtime': 771.2385, 'train_samples_per_second': 0.166, 'train_steps_per_second': 0.021, 'train_loss': 0.0030371291213668883, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [12:51<00:00, 48.26s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [12:51<00:00, 48.20s/it]\n",
      "5.  12%|█▎        | 2/16 [00:19<02:16,  9.75s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:29<02:06,  9.76s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:39<01:57,  9.78s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [00:48<01:47,  9.82s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [00:58<01:38,  9.85s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:08<01:29,  9.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0028, 'grad_norm': 0.23943601548671722, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:18<01:19,  9.91s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5.  50%|█████     | 8/16 [01:18<01:19,  9.91s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:28<01:09,  9.91s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [01:38<00:59,  9.90s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [01:48<00:49,  9.89s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [01:58<00:39,  9.89s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [02:08<00:29,  9.88s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [02:18<00:19,  9.89s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [02:28<00:09,  9.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  60%|██████    | 18/30 [2:26:23<1:44:50, 524.24s/it]\n",
      "5. {'loss': 0.0006, 'grad_norm': 0.033546242862939835, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  60%|██████    | 18/30 [2:27:42<1:44:50, 524.24s/it]\n",
      "5. {'train_runtime': 157.9504, 'train_samples_per_second': 0.81, 'train_steps_per_second': 0.101, 'train_loss': 0.001687599957222119, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [02:37<00:00,  9.90s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:37<00:00,  9.90s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:37<00:00,  9.90s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [02:37<00:00,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  73%|███████▎  | 22/30 [2:36:23<52:20, 392.54s/it]*** -> Training took 243.9953 seconds.\n",
      "7.  77%|███████▋  | 23/30 [2:38:50<45:51, 393.09s/it]retraining model for key 'a395ee82' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 594.23 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 585.79 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:31<07:54, 31.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  57%|█████▋    | 17/30 [2:34:56<2:02:09, 563.82s/it]*** -> Training took 771.2385 seconds.\n",
      "4.  60%|██████    | 18/30 [2:38:38<2:18:50, 694.23s/it]retraining model for key '7ed72f31' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1164.39 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1142.48 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n",
      "5.  60%|██████    | 18/30 [2:27:42<1:44:50, 524.24s/it]*** -> Training took 157.9504 seconds.\n",
      "5.  63%|██████▎   | 19/30 [2:29:17<1:21:20, 443.70s/it]retraining model for key 'a25697e4' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 574.78 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 567.58 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [01:03<07:23, 31.66s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:14<03:35, 14.39s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:28<03:21, 14.39s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:35<06:52, 31.71s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:43<03:06, 14.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 15/30 [2:25:48<2:58:21, 713.43s/it]*** -> Training took 586.9133 seconds.\n",
      "6.  53%|█████▎    | 16/30 [2:35:08<3:17:14, 845.33s/it]retraining model for key 'b10624e5' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 532.04 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 525.39 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  25%|██▌       | 4/16 [00:57<02:52, 14.36s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:30<07:38, 30.54s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [02:06<06:21, 31.77s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:11<02:37, 14.34s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:26<02:23, 14.34s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [01:01<07:07, 30.51s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [02:38<05:50, 31.82s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:40<02:09, 14.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0061, 'grad_norm': 0.13453492522239685, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:54<01:54, 14.34s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.   6%|▋         | 1/16 [00:32<08:07, 32.48s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [01:54<01:54, 14.34s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:31<06:37, 30.59s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:09<01:40, 14.33s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [03:10<05:18, 31.89s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [02:23<01:25, 14.32s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [01:04<07:34, 32.45s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [02:02<06:07, 30.64s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [02:37<01:11, 14.32s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [03:42<04:47, 31.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0029, 'grad_norm': 0.24899323284626007, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [04:14<04:15, 31.88s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4.  75%|███████▌  | 12/16 [02:51<00:57, 14.30s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:37<07:00, 32.34s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [02:33<05:37, 30.67s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [03:06<00:42, 14.30s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [04:14<04:15, 31.88s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [03:20<00:28, 14.29s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [02:09<06:28, 32.34s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [03:03<05:07, 30.72s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [03:34<00:14, 14.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  60%|██████    | 18/30 [2:40:36<2:18:50, 694.23s/it]\n",
      "4. {'loss': 0.0007, 'grad_norm': 0.021837592124938965, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  60%|██████    | 18/30 [2:42:30<2:18:50, 694.23s/it]\n",
      "4. {'train_runtime': 229.0642, 'train_samples_per_second': 0.559, 'train_steps_per_second': 0.07, 'train_loss': 0.003419779008254409, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [03:49<00:00, 14.27s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [03:49<00:00, 14.27s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [03:49<00:00, 14.27s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [03:49<00:00, 14.32s/it]\n",
      "7.  56%|█████▋    | 9/16 [04:46<03:43, 31.90s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [02:41<05:55, 32.35s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [03:34<04:36, 30.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.004, 'grad_norm': 0.12931400537490845, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [04:05<04:05, 30.74s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.  62%|██████▎   | 10/16 [05:18<03:11, 31.91s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [04:05<04:05, 30.74s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [03:14<05:23, 32.36s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [05:50<02:39, 31.91s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [04:36<03:35, 30.74s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [03:46<04:51, 32.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0045, 'grad_norm': 0.0268641896545887, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [04:18<04:18, 32.33s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "7.  75%|███████▌  | 12/16 [06:22<02:07, 31.91s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [05:06<03:04, 30.74s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [04:18<04:18, 32.33s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [06:54<01:35, 31.90s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [05:37<02:33, 30.74s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [04:51<03:46, 32.32s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [07:26<01:03, 31.88s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [06:08<02:02, 30.73s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [05:23<03:13, 32.33s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [07:57<00:31, 31.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  77%|███████▋  | 23/30 [2:43:09<45:51, 393.09s/it]\n",
      "7. {'loss': 0.0004, 'grad_norm': 0.015531918965280056, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  77%|███████▋  | 23/30 [2:47:24<45:51, 393.09s/it]\n",
      "7. {'train_runtime': 509.8369, 'train_samples_per_second': 0.251, 'train_steps_per_second': 0.031, 'train_loss': 0.001630057638976723, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [08:29<00:00, 31.89s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [08:29<00:00, 31.89s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [08:29<00:00, 31.89s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [08:29<00:00, 31.86s/it]\n",
      "5.  81%|████████▏ | 13/16 [06:39<01:32, 30.74s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [05:55<02:41, 32.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  60%|██████    | 18/30 [2:42:30<2:18:50, 694.23s/it]*** -> Training took 229.0642 seconds.\n",
      "4.  63%|██████▎   | 19/30 [2:46:12<1:54:02, 622.02s/it]retraining model for key '88bcf3b4' (retrain_dataset_size=10)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1095.74 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1072.33 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [07:09<01:01, 30.73s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [06:28<02:09, 32.31s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:14<03:44, 14.98s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:29<03:29, 14.95s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [07:40<00:30, 30.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  63%|██████▎   | 19/30 [2:33:26<1:21:20, 443.70s/it]\n",
      "5. {'loss': 0.0015, 'grad_norm': 0.07839101552963257, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  63%|██████▎   | 19/30 [2:37:32<1:21:20, 443.70s/it]\n",
      "5. {'train_runtime': 491.3718, 'train_samples_per_second': 0.26, 'train_steps_per_second': 0.033, 'train_loss': 0.002762713353149593, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [08:11<00:00, 30.73s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [08:11<00:00, 30.73s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [08:11<00:00, 30.73s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [08:11<00:00, 30.71s/it]\n",
      "4.  19%|█▉        | 3/16 [00:44<03:14, 14.94s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [07:00<01:36, 32.32s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:59<02:59, 14.95s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:14<02:44, 14.95s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [07:32<01:04, 32.31s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:29<02:29, 14.95s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:44<02:14, 14.96s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0028, 'grad_norm': 0.2312879115343094, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:59<01:59, 14.97s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  94%|█████████▍| 15/16 [08:04<00:32, 32.30s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  53%|█████▎    | 16/30 [2:39:31<3:17:14, 845.33s/it]\n",
      "6. {'loss': 0.0005, 'grad_norm': 0.14641882479190826, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  53%|█████▎    | 16/30 [2:43:50<3:17:14, 845.33s/it]\n",
      "6. {'train_runtime': 517.2435, 'train_samples_per_second': 0.247, 'train_steps_per_second': 0.031, 'train_loss': 0.002507351731765084, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [08:37<00:00, 32.29s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [08:37<00:00, 32.29s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [08:37<00:00, 32.29s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [08:37<00:00, 32.33s/it]\n",
      "4.  50%|█████     | 8/16 [01:59<01:59, 14.97s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:14<01:44, 14.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  77%|███████▋  | 23/30 [2:47:24<45:51, 393.09s/it]*** -> Training took 509.8369 seconds.\n",
      "7.  80%|████████  | 24/30 [2:50:18<48:09, 481.57s/it]retraining model for key 'b99e7126' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 339.40 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 335.46 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  62%|██████▎   | 10/16 [02:29<01:29, 14.99s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [02:44<01:14, 14.99s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [02:59<00:59, 14.99s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [03:14<00:44, 15.00s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [03:29<00:30, 15.00s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [03:44<00:15, 15.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  63%|██████▎   | 19/30 [2:48:15<1:54:02, 622.02s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [03:59<00:00, 15.02s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [03:59<00:00, 15.02s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.001, 'grad_norm': 0.5668958425521851, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  63%|██████▎   | 19/30 [2:50:15<1:54:02, 622.02s/it]\n",
      "4. {'train_runtime': 239.7971, 'train_samples_per_second': 0.534, 'train_steps_per_second': 0.067, 'train_loss': 0.0019312590593472123, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [03:59<00:00, 15.02s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [03:59<00:00, 14.99s/it]\n",
      "7.   6%|▋         | 1/16 [00:57<14:17, 57.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  53%|█████▎    | 16/30 [2:43:50<3:17:14, 845.33s/it]*** -> Training took 517.2435 seconds.\n",
      "6.  57%|█████▋    | 17/30 [2:46:20<2:51:50, 793.11s/it]retraining model for key 'b5ca7ac4' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 528.99 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 522.19 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [01:54<13:17, 56.98s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:32<08:10, 32.67s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [01:05<07:37, 32.66s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [02:51<12:22, 57.08s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:37<07:03, 32.56s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [02:10<06:31, 32.59s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [03:48<11:24, 57.01s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [02:43<05:58, 32.60s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [04:45<10:27, 57.02s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [03:15<05:25, 32.59s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [03:48<04:53, 32.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0043, 'grad_norm': 0.18265783786773682, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [04:20<04:20, 32.58s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  63%|██████▎   | 19/30 [2:37:32<1:21:20, 443.70s/it]*** -> Training took 491.3718 seconds.\n",
      "5.  67%|██████▋   | 20/30 [2:45:53<1:41:36, 609.62s/it]retraining model for key 'a47bf94d' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 564.21 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 557.36 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  38%|███▊      | 6/16 [05:42<09:29, 56.99s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [04:20<04:20, 32.58s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [04:53<03:47, 32.57s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [06:38<08:32, 56.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0018, 'grad_norm': 0.05625205859541893, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [07:35<07:35, 56.91s/it]\u001b[A\n",
      "7. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  63%|██████▎   | 19/30 [2:50:15<1:54:02, 622.02s/it]*** -> Training took 239.7971 seconds.\n",
      "4.  67%|██████▋   | 20/30 [2:56:30<1:43:27, 620.79s/it]retraining model for key '89565ca0' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 881.14 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 867.48 examples/s]\n",
      "5.   6%|▋         | 1/16 [00:31<07:53, 31.59s/it]\u001b[A\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [05:25<03:15, 32.56s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [01:03<07:21, 31.55s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:18<04:40, 18.68s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [07:35<07:35, 56.91s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [05:58<02:42, 32.56s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:37<04:21, 18.67s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:34<06:49, 31.50s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:56<04:03, 18.71s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [06:30<02:10, 32.57s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [02:06<06:17, 31.49s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:14<03:44, 18.73s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [08:32<06:38, 56.90s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:33<03:25, 18.73s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [07:03<01:37, 32.57s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [02:37<05:46, 31.49s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:52<03:07, 18.74s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:11<02:48, 18.75s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.161, 'grad_norm': 3.3190667629241943, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:29<02:30, 18.76s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  88%|████████▊ | 14/16 [07:36<01:05, 32.54s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [03:08<05:14, 31.49s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [09:29<05:41, 56.88s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:29<02:30, 18.76s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [08:08<00:32, 32.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  57%|█████▋    | 17/30 [2:50:45<2:51:50, 793.11s/it]\n",
      "6. {'loss': 0.0011, 'grad_norm': 0.20953518152236938, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  57%|█████▋    | 17/30 [2:55:05<2:51:50, 793.11s/it]\n",
      "6. {'train_runtime': 521.1511, 'train_samples_per_second': 0.246, 'train_steps_per_second': 0.031, 'train_loss': 0.0027190311811864376, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [08:41<00:00, 32.55s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [08:41<00:00, 32.55s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [08:41<00:00, 32.55s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [08:41<00:00, 32.57s/it]\n",
      "5.  44%|████▍     | 7/16 [03:40<04:43, 31.49s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [04:11<04:11, 31.48s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0042, 'grad_norm': 0.1652650535106659, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  56%|█████▋    | 9/16 [02:48<02:11, 18.78s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [03:07<01:52, 18.80s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [04:11<04:11, 31.48s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [10:26<04:44, 56.90s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:26<01:34, 18.82s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [03:45<01:15, 18.83s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [04:43<03:40, 31.49s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [04:04<00:56, 18.84s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [11:23<03:47, 56.91s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [05:14<03:08, 31.50s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [04:22<00:37, 18.84s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [04:41<00:18, 18.84s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  67%|██████▋   | 20/30 [2:59:03<1:43:27, 620.79s/it]\n",
      "4. {'loss': 0.039, 'grad_norm': 3.3928568363189697, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  67%|██████▋   | 20/30 [3:01:34<1:43:27, 620.79s/it]\n",
      "4. {'train_runtime': 300.6738, 'train_samples_per_second': 0.426, 'train_steps_per_second': 0.053, 'train_loss': 0.099980054423213, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [05:00<00:00, 18.84s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:00<00:00, 18.84s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:00<00:00, 18.84s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [05:00<00:00, 18.79s/it]\n",
      "5.  69%|██████▉   | 11/16 [05:46<02:37, 31.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  67%|██████▋   | 20/30 [3:01:34<1:43:27, 620.79s/it]*** -> Training took 300.6738 seconds.\n",
      "4.  70%|███████   | 21/30 [3:01:48<1:19:31, 530.17s/it]retraining model for key '8b9c3697' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 611.82 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 603.79 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  81%|████████▏ | 13/16 [12:20<02:50, 56.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  57%|█████▋    | 17/30 [2:55:05<2:51:50, 793.11s/it]*** -> Training took 521.1511 seconds.\n",
      "6.  60%|██████    | 18/30 [2:57:43<2:32:00, 760.07s/it]retraining model for key 'b6f77b65' (retrain_dataset_size=15)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 721.60 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 712.85 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  75%|███████▌  | 12/16 [06:17<02:05, 31.49s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:27<06:52, 27.50s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [06:49<01:34, 31.49s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:16<04:13, 16.89s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [13:17<01:53, 56.95s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:33<03:56, 16.91s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:55<06:25, 27.53s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [07:20<01:03, 31.50s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:50<03:39, 16.92s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:07<03:22, 16.89s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:22<05:57, 27.53s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [07:52<00:31, 31.50s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  67%|██████▋   | 20/30 [2:50:09<1:41:36, 609.62s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [08:23<00:00, 31.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0007, 'grad_norm': 0.04952598735690117, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  67%|██████▋   | 20/30 [2:54:21<1:41:36, 609.62s/it]\n",
      "5. {'train_runtime': 503.908, 'train_samples_per_second': 0.254, 'train_steps_per_second': 0.032, 'train_loss': 0.0024699526838958263, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [08:23<00:00, 31.48s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [08:23<00:00, 31.48s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [08:23<00:00, 31.49s/it]\n",
      "6.  31%|███▏      | 5/16 [01:24<03:05, 16.87s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [14:14<00:57, 57.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  80%|████████  | 24/30 [2:57:58<48:09, 481.57s/it]\n",
      "7. {'loss': 0.0003, 'grad_norm': 0.10868732631206512, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  80%|████████  | 24/30 [3:05:34<48:09, 481.57s/it]\n",
      "7. {'train_runtime': 911.3662, 'train_samples_per_second': 0.14, 'train_steps_per_second': 0.018, 'train_loss': 0.0010310644429409876, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [15:11<00:00, 56.97s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [15:11<00:00, 56.97s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [15:11<00:00, 56.97s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [15:11<00:00, 56.96s/it]\n",
      "4.  25%|██▌       | 4/16 [01:50<05:30, 27.53s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:41<02:48, 16.83s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:57<02:31, 16.80s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0115, 'grad_norm': 0.5122857093811035, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:14<02:14, 16.81s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "4.  31%|███▏      | 5/16 [02:17<05:02, 27.53s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:14<02:14, 16.81s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [02:45<04:35, 27.54s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [02:31<01:57, 16.84s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:48<01:40, 16.82s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [03:12<04:07, 27.55s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0037, 'grad_norm': 0.18326915800571442, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [03:40<03:40, 27.56s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  69%|██████▉   | 11/16 [03:05<01:24, 16.83s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [03:22<01:07, 16.84s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [03:40<03:40, 27.56s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [03:38<00:50, 16.83s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [04:07<03:13, 27.58s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [03:55<00:33, 16.82s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [04:12<00:16, 16.82s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  60%|██████    | 18/30 [3:00:02<2:32:00, 760.07s/it]\n",
      "6. {'loss': 0.0056, 'grad_norm': 0.2947135865688324, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  60%|██████    | 18/30 [3:02:16<2:32:00, 760.07s/it]\n",
      "6. {'train_runtime': 269.3942, 'train_samples_per_second': 0.475, 'train_steps_per_second': 0.059, 'train_loss': 0.008531620958819985, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [04:29<00:00, 16.82s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:29<00:00, 16.82s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:29<00:00, 16.82s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [04:29<00:00, 16.84s/it]\n",
      "4.  62%|██████▎   | 10/16 [04:35<02:45, 27.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  67%|██████▋   | 20/30 [2:54:21<1:41:36, 609.62s/it]*** -> Training took 503.908 seconds.\n",
      "5.  70%|███████   | 21/30 [2:57:39<1:35:47, 638.64s/it]retraining model for key 'a6f40cea' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 752.85 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 742.05 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  69%|██████▉   | 11/16 [05:03<02:17, 27.57s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [05:30<01:50, 27.57s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:23<05:47, 23.17s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [05:58<01:22, 27.60s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:46<05:25, 23.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  60%|██████    | 18/30 [3:02:16<2:32:00, 760.07s/it]*** -> Training took 269.3942 seconds.\n",
      "6.  63%|██████▎   | 19/30 [3:04:06<1:58:35, 646.88s/it]retraining model for key 'c7f57c3e' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1167.98 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1146.08 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  88%|████████▊ | 14/16 [06:25<00:55, 27.59s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:09<05:01, 23.22s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:14<03:40, 14.67s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:32<04:38, 23.23s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [06:53<00:27, 27.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  70%|███████   | 21/30 [3:05:33<1:19:31, 530.17s/it]\n",
      "4. {'loss': 0.0019, 'grad_norm': 0.029650548473000526, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [07:21<00:00, 27.60s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [07:21<00:00, 27.60s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  70%|███████   | 21/30 [3:09:14<1:19:31, 530.17s/it]\n",
      "4. {'train_runtime': 441.1218, 'train_samples_per_second': 0.29, 'train_steps_per_second': 0.036, 'train_loss': 0.0027927608462050557, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [07:21<00:00, 27.60s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [07:21<00:00, 27.57s/it]\n",
      "6.  12%|█▎        | 2/16 [00:29<03:25, 14.70s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:56<04:15, 23.23s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:44<03:11, 14.73s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:58<02:56, 14.71s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:19<03:52, 23.21s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:13<02:41, 14.68s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:42<03:28, 23.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0202, 'grad_norm': 0.36407655477523804, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [03:05<03:05, 23.21s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:28<02:26, 14.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  80%|████████  | 24/30 [3:05:34<48:09, 481.57s/it]*** -> Training took 911.3662 seconds.\n",
      "7.  83%|████████▎ | 25/30 [3:11:58<1:00:36, 727.22s/it]retraining model for key 'c4d067a0' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 523.66 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 516.85 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  44%|████▍     | 7/16 [01:42<02:11, 14.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.002, 'grad_norm': 0.1592325121164322, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:57<01:57, 14.64s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  50%|█████     | 8/16 [03:05<03:05, 23.21s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [01:57<01:57, 14.64s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [02:11<01:42, 14.63s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [03:28<02:42, 23.17s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [02:26<01:27, 14.64s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [03:51<02:18, 23.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  70%|███████   | 21/30 [3:09:14<1:19:31, 530.17s/it]*** -> Training took 441.1218 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  69%|██████▉   | 11/16 [02:41<01:13, 14.65s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  73%|███████▎  | 22/30 [3:11:35<1:12:57, 547.19s/it]retraining model for key '981571dc' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 276.69 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 274.35 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   6%|▋         | 1/16 [00:35<08:45, 35.00s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:55<00:58, 14.65s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [04:15<01:55, 23.17s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [03:10<00:43, 14.65s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [04:38<01:32, 23.18s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [01:10<08:10, 35.02s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [03:25<00:29, 14.64s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [03:39<00:14, 14.63s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  63%|██████▎   | 19/30 [3:06:07<1:58:35, 646.88s/it]\n",
      "6. {'loss': 0.0003, 'grad_norm': 0.009059879928827286, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  63%|██████▎   | 19/30 [3:08:04<1:58:35, 646.88s/it]\n",
      "6. {'train_runtime': 234.4183, 'train_samples_per_second': 0.546, 'train_steps_per_second': 0.068, 'train_loss': 0.0011792338627856225, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [03:54<00:00, 14.62s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:54<00:00, 14.62s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [03:54<00:00, 14.62s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [03:54<00:00, 14.65s/it]\n",
      "5.  81%|████████▏ | 13/16 [05:01<01:09, 23.20s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:44<07:34, 34.97s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [05:24<00:46, 23.19s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [05:47<00:23, 23.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  70%|███████   | 21/30 [3:00:49<1:35:47, 638.64s/it]\n",
      "5. {'loss': 0.0026, 'grad_norm': 0.1758640855550766, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  70%|███████   | 21/30 [3:03:54<1:35:47, 638.64s/it]\n",
      "5. {'train_runtime': 371.1274, 'train_samples_per_second': 0.345, 'train_steps_per_second': 0.043, 'train_loss': 0.011441000388003886, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [06:11<00:00, 23.19s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [06:11<00:00, 23.19s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [06:11<00:00, 23.19s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [06:11<00:00, 23.19s/it]\n",
      "7.  25%|██▌       | 4/16 [02:19<06:59, 34.94s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [01:00<15:06, 60.42s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  70%|███████   | 21/30 [3:03:54<1:35:47, 638.64s/it]*** -> Training took 371.1274 seconds.\n",
      "5.  73%|███████▎  | 22/30 [3:04:17<1:15:31, 566.38s/it]retraining model for key 'aa4ec2a5' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 507.92 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 502.08 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  31%|███▏      | 5/16 [02:55<06:25, 35.03s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [03:30<05:50, 35.09s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [02:00<14:06, 60.43s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:34<08:40, 34.68s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [04:05<05:16, 35.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0017, 'grad_norm': 0.05394019931554794, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [04:40<04:41, 35.19s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "5.  12%|█▎        | 2/16 [01:09<08:05, 34.66s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [03:01<13:08, 60.63s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [04:40<04:41, 35.19s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:43<07:29, 34.56s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [05:15<04:06, 35.21s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [04:02<12:07, 60.63s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [02:18<06:54, 34.57s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [05:51<03:31, 35.24s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [02:53<06:20, 34.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  63%|██████▎   | 19/30 [3:08:04<1:58:35, 646.88s/it]*** -> Training took 234.4183 seconds.\n",
      "6.  67%|██████▋   | 20/30 [3:12:52<1:41:43, 610.36s/it]retraining model for key 'cb2d8a2c' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 665.45 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 655.84 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  69%|██████▉   | 11/16 [06:26<02:56, 35.27s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [05:03<11:07, 60.67s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [03:27<05:46, 34.60s/it]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [07:01<02:21, 35.27s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:26<06:32, 26.15s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [04:02<05:11, 34.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.006, 'grad_norm': 0.14590959250926971, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [04:36<04:36, 34.60s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  38%|███▊      | 6/16 [06:03<10:06, 60.67s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:52<06:07, 26.23s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [07:37<01:45, 35.29s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [04:36<04:36, 34.60s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:18<05:41, 26.27s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [08:12<01:10, 35.28s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:44<05:14, 26.25s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [05:11<04:02, 34.60s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [07:04<09:06, 60.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0003, 'grad_norm': 0.11740069091320038, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [08:05<08:05, 60.65s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "7.  94%|█████████▍| 15/16 [08:47<00:35, 35.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  83%|████████▎ | 25/30 [3:16:43<1:00:36, 727.22s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [09:22<00:00, 35.15s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [09:22<00:00, 35.15s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [09:22<00:00, 35.15s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [09:22<00:00, 35.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0004, 'grad_norm': 0.020928097888827324, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  83%|████████▎ | 25/30 [3:21:25<1:00:36, 727.22s/it]\n",
      "7. {'train_runtime': 562.5701, 'train_samples_per_second': 0.228, 'train_steps_per_second': 0.028, 'train_loss': 0.0010455397423356771, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  31%|███▏      | 5/16 [02:11<04:48, 26.22s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [05:45<03:27, 34.59s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [02:37<04:22, 26.23s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [08:05<08:05, 60.65s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [06:20<02:52, 34.59s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [03:03<03:56, 26.25s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [03:29<03:30, 26.25s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0046, 'grad_norm': 0.15250332653522491, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [03:29<03:30, 26.25s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [06:55<02:18, 34.57s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [09:05<07:04, 60.65s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [03:56<03:03, 26.24s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [07:29<01:43, 34.56s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [04:22<02:37, 26.24s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [08:04<01:09, 34.55s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [04:48<02:11, 26.24s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [10:06<06:03, 60.65s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [08:38<00:34, 34.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  73%|███████▎  | 22/30 [3:08:58<1:15:31, 566.38s/it]\n",
      "5. {'loss': 0.004, 'grad_norm': 0.031392522156238556, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  73%|███████▎  | 22/30 [3:13:35<1:15:31, 566.38s/it]\n",
      "5. {'train_runtime': 553.2117, 'train_samples_per_second': 0.231, 'train_steps_per_second': 0.029, 'train_loss': 0.005018243798986077, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [09:13<00:00, 34.54s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [09:13<00:00, 34.54s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [09:13<00:00, 34.54s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [09:13<00:00, 34.58s/it]\n",
      "6.  75%|███████▌  | 12/16 [05:14<01:44, 26.24s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [05:41<01:18, 26.22s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [11:07<05:03, 60.66s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [06:07<00:52, 26.22s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [06:33<00:26, 26.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  67%|██████▋   | 20/30 [3:16:26<1:41:43, 610.36s/it]\n",
      "6. {'loss': 0.0007, 'grad_norm': 0.05707544833421707, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  67%|██████▋   | 20/30 [3:19:55<1:41:43, 610.36s/it]\n",
      "6. {'train_runtime': 419.6654, 'train_samples_per_second': 0.305, 'train_steps_per_second': 0.038, 'train_loss': 0.0026518430386204273, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [06:59<00:00, 26.21s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [06:59<00:00, 26.21s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [06:59<00:00, 26.21s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [06:59<00:00, 26.23s/it]\n",
      "4.  75%|███████▌  | 12/16 [12:07<04:02, 60.65s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  83%|████████▎ | 25/30 [3:21:25<1:00:36, 727.22s/it]*** -> Training took 562.5701 seconds.\n",
      "7.  87%|████████▋ | 26/30 [3:26:22<51:12, 768.19s/it]  retraining model for key 'db0c5428' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 878.51 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 864.50 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:20<05:13, 20.93s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [13:08<03:01, 60.66s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:41<04:53, 20.93s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [01:02<04:32, 20.93s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:23<04:11, 20.93s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [14:09<02:01, 60.67s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:44<03:50, 20.93s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [02:05<03:29, 20.92s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [02:26<03:08, 20.93s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0019, 'grad_norm': 0.1009528785943985, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:47<02:47, 20.92s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "4.  94%|█████████▍| 15/16 [15:09<01:00, 60.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  73%|███████▎  | 22/30 [3:19:45<1:12:57, 547.19s/it]\n",
      "4. {'loss': 0.0002, 'grad_norm': 0.034590814262628555, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  73%|███████▎  | 22/30 [3:27:51<1:12:57, 547.19s/it]\n",
      "4. {'train_runtime': 970.3322, 'train_samples_per_second': 0.132, 'train_steps_per_second': 0.016, 'train_loss': 0.00027054245583713055, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [16:10<00:00, 60.65s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [16:10<00:00, 60.65s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [16:10<00:00, 60.65s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [16:10<00:00, 60.65s/it]\n",
      "7.  50%|█████     | 8/16 [02:47<02:47, 20.92s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [03:08<02:26, 20.92s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [03:29<02:05, 20.93s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [03:50<01:44, 20.93s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  67%|██████▋   | 20/30 [3:19:55<1:41:43, 610.36s/it]*** -> Training took 419.6654 seconds.\n",
      "5.  73%|███████▎  | 22/30 [3:13:35<1:15:31, 566.38s/it]*** -> Training took 553.2117 seconds.\n",
      "6.  70%|███████   | 21/30 [3:24:37<1:35:51, 639.01s/it]retraining model for key 'cbebaa4b' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 799.86 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 785.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  77%|███████▋  | 23/30 [3:19:37<1:18:27, 672.51s/it]retraining model for key 'abc82100' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1031.19 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1013.62 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n",
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  75%|███████▌  | 12/16 [04:11<01:23, 20.94s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:16<04:09, 16.62s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [04:32<01:02, 20.96s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:21<05:25, 21.71s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:33<03:53, 16.65s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [04:53<00:41, 20.98s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:43<05:04, 21.73s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:49<03:36, 16.63s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [05:14<00:20, 20.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  87%|████████▋ | 26/30 [3:29:13<51:12, 768.19s/it]\n",
      "7. {'loss': 0.0001, 'grad_norm': 0.0061118993908166885, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  87%|████████▋ | 26/30 [3:32:01<51:12, 768.19s/it]\n",
      "7. {'train_runtime': 335.0379, 'train_samples_per_second': 0.382, 'train_steps_per_second': 0.048, 'train_loss': 0.0009704426665848587, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [05:35<00:00, 20.94s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:35<00:00, 20.94s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:35<00:00, 20.94s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [05:35<00:00, 20.94s/it]\n",
      "5.  25%|██▌       | 4/16 [01:06<03:19, 16.58s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [01:05<04:42, 21.71s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:22<03:01, 16.53s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:26<04:19, 21.67s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:39<02:45, 16.53s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:48<03:58, 21.67s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:55<02:28, 16.54s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0053, 'grad_norm': 0.9292880296707153, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:12<02:12, 16.55s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5.  50%|█████     | 8/16 [02:12<02:12, 16.55s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [02:10<03:36, 21.68s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:29<01:55, 16.54s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [02:31<03:15, 21.68s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0061, 'grad_norm': 0.34329506754875183, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:53<02:53, 21.67s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:45<01:39, 16.53s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:53<02:53, 21.67s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:02<01:22, 16.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  87%|████████▋ | 26/30 [3:32:01<51:12, 768.19s/it]*** -> Training took 335.0379 seconds.\n",
      "7.  90%|█████████ | 27/30 [3:34:09<33:53, 677.91s/it]retraining model for key 'de809cff' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1029.89 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1012.14 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  75%|███████▌  | 12/16 [03:18<01:06, 16.54s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [03:15<02:31, 21.67s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:35<00:49, 16.55s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [03:36<02:10, 21.68s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:17<04:19, 17.30s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [03:51<00:33, 16.55s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [03:58<01:48, 21.68s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:34<04:02, 17.33s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [04:08<00:16, 16.54s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  77%|███████▋  | 23/30 [3:21:54<1:18:27, 672.51s/it]\n",
      "5. {'loss': 0.0026, 'grad_norm': 0.11454594880342484, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  77%|███████▋  | 23/30 [3:24:06<1:18:27, 672.51s/it]\n",
      "5. {'train_runtime': 264.7704, 'train_samples_per_second': 0.483, 'train_steps_per_second': 0.06, 'train_loss': 0.0039426099974662066, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:24<00:00, 16.53s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:24<00:00, 16.53s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:24<00:00, 16.53s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:24<00:00, 16.55s/it]\n",
      "7.  19%|█▉        | 3/16 [00:52<03:45, 17.35s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [04:20<01:26, 21.68s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:09<03:28, 17.37s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [04:41<01:05, 21.68s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:26<03:11, 17.40s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [05:03<00:43, 21.68s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:44<02:54, 17.41s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [05:25<00:21, 21.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  70%|███████   | 21/30 [3:27:35<1:35:51, 639.01s/it]\n",
      "6. {'loss': 0.0012, 'grad_norm': 0.09858505427837372, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  70%|███████   | 21/30 [3:30:28<1:35:51, 639.01s/it]\n",
      "6. {'train_runtime': 346.7588, 'train_samples_per_second': 0.369, 'train_steps_per_second': 0.046, 'train_loss': 0.003629448590800166, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:46<00:00, 21.64s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:46<00:00, 21.64s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:46<00:00, 21.64s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [05:46<00:00, 21.67s/it]\n",
      "7.  44%|████▍     | 7/16 [02:01<02:36, 17.42s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0112, 'grad_norm': 0.06932200491428375, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:19<02:19, 17.42s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [02:19<02:19, 17.42s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:36<02:02, 17.43s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  73%|███████▎  | 22/30 [3:27:51<1:12:57, 547.19s/it]*** -> Training took 970.3322 seconds.\n",
      "4.  77%|███████▋  | 23/30 [3:35:50<1:35:37, 819.63s/it]retraining model for key '9bbf930d' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 932.40 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 914.56 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  62%|██████▎   | 10/16 [02:54<01:44, 17.46s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [03:11<01:27, 17.49s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [03:29<01:10, 17.51s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:18<04:34, 18.27s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [03:46<00:52, 17.51s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:36<04:15, 18.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  77%|███████▋  | 23/30 [3:24:06<1:18:27, 672.51s/it]*** -> Training took 264.7704 seconds.\n",
      "5.  80%|████████  | 24/30 [3:27:20<1:00:56, 609.48s/it]retraining model for key 'b0039139' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 2142.29 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [04:04<00:35, 17.51s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:54<03:57, 18.26s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:07<01:54,  7.61s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:15<01:46,  7.60s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [04:21<00:17, 17.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  90%|█████████ | 27/30 [3:36:32<33:53, 677.91s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [04:39<00:00, 17.56s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:39<00:00, 17.56s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [04:39<00:00, 17.56s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [04:39<00:00, 17.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0031, 'grad_norm': 0.15544168651103973, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  90%|█████████ | 27/30 [3:38:52<33:53, 677.91s/it]\n",
      "7. {'train_runtime': 279.4964, 'train_samples_per_second': 0.458, 'train_steps_per_second': 0.057, 'train_loss': 0.007159478031098843, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  19%|█▉        | 3/16 [00:22<01:39,  7.62s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:13<03:38, 18.25s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:30<01:31,  7.63s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [00:38<01:24,  7.65s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:31<03:20, 18.24s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [00:45<01:16,  7.67s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [00:53<01:09,  7.68s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.028, 'grad_norm': 1.4899073839187622, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:01<01:01,  7.70s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:49<03:02, 18.25s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [01:01<01:01,  7.70s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:09<00:53,  7.70s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [01:16<00:46,  7.70s/it]\u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:07<02:44, 18.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.005, 'grad_norm': 0.05529303103685379, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:26<02:25, 18.25s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5.  69%|██████▉   | 11/16 [01:24<00:38,  7.71s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [01:32<00:30,  7.72s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:26<02:25, 18.25s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [01:39<00:23,  7.72s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [01:47<00:15,  7.71s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:44<02:07, 18.23s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [01:55<00:07,  7.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  80%|████████  | 24/30 [3:28:24<1:00:56, 609.48s/it]\n",
      "5. {'loss': 0.0062, 'grad_norm': 0.4147650897502899, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [02:03<00:00,  7.71s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:03<00:00,  7.71s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:03<00:00,  7.71s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [02:03<00:00,  7.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  80%|████████  | 24/30 [3:29:26<1:00:56, 609.48s/it]\n",
      "5. {'train_runtime': 123.0635, 'train_samples_per_second': 1.04, 'train_steps_per_second': 0.13, 'train_loss': 0.0170867380220443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  62%|██████▎   | 10/16 [03:02<01:49, 18.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  90%|█████████ | 27/30 [3:38:52<33:53, 677.91s/it]*** -> Training took 279.4964 seconds.\n",
      "7.  93%|█████████▎| 28/30 [3:40:51<19:49, 594.97s/it]retraining model for key 'f931b4a8' (retrain_dataset_size=10)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 1568.49 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  69%|██████▉   | 11/16 [03:20<01:30, 18.20s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:11<02:48, 11.25s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [03:38<01:12, 18.21s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:22<02:37, 11.25s/it]\u001b[A\n",
      "7.  19%|█▉        | 3/16 [00:33<02:26, 11.25s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [03:57<00:54, 18.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  80%|████████  | 24/30 [3:29:26<1:00:56, 609.48s/it]*** -> Training took 123.0635 seconds.\n",
      "5.  83%|████████▎ | 25/30 [3:30:48<40:45, 489.06s/it]  retraining model for key 'b9e38dc0' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1159.00 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1137.01 examples/s]\n",
      "7.  25%|██▌       | 4/16 [00:45<02:15, 11.25s/it]\u001b[A\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  88%|████████▊ | 14/16 [04:15<00:36, 18.23s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [00:56<02:03, 11.25s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:07<01:52, 11.26s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [04:33<00:18, 18.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  77%|███████▋  | 23/30 [3:38:20<1:35:37, 819.63s/it]\n",
      "4. {'loss': 0.0005, 'grad_norm': 0.17842017114162445, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  77%|███████▋  | 23/30 [3:40:46<1:35:37, 819.63s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:51<00:00, 18.21s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:51<00:00, 18.21s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'train_runtime': 291.673, 'train_samples_per_second': 0.439, 'train_steps_per_second': 0.055, 'train_loss': 0.0027462328143883497, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:51<00:00, 18.21s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [04:51<00:00, 18.23s/it]\n",
      "5.   6%|▋         | 1/16 [00:14<03:37, 14.50s/it]\u001b[A\n",
      "7.  44%|████▍     | 7/16 [01:18<01:41, 11.27s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0054, 'grad_norm': 0.2172616422176361, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [01:30<01:30, 11.27s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7.  50%|█████     | 8/16 [01:30<01:30, 11.27s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:29<03:23, 14.54s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [01:41<01:18, 11.28s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:43<03:09, 14.61s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  70%|███████   | 21/30 [3:30:28<1:35:51, 639.01s/it]*** -> Training took 346.7588 seconds.\n",
      "6.  73%|███████▎  | 22/30 [3:36:55<1:29:08, 668.58s/it]retraining model for key 'da515329' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 859.01 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 846.03 examples/s]\n",
      "7.  62%|██████▎   | 10/16 [01:52<01:07, 11.28s/it]\u001b[A\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  25%|██▌       | 4/16 [00:58<02:56, 14.71s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [02:03<00:56, 11.29s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "7.  75%|███████▌  | 12/16 [02:15<00:45, 11.29s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:13<02:42, 14.78s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [02:26<00:33, 11.30s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:28<02:28, 14.82s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:20<05:00, 20.02s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [02:37<00:22, 11.32s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [01:43<02:13, 14.84s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0042, 'grad_norm': 0.1251697540283203, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:58<01:58, 14.84s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "7.  94%|█████████▍| 15/16 [02:49<00:11, 11.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  93%|█████████▎| 28/30 [3:42:24<19:49, 594.97s/it]\n",
      "7. {'loss': 0.0011, 'grad_norm': 0.26186227798461914, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  93%|█████████▎| 28/30 [3:43:55<19:49, 594.97s/it]\n",
      "7. {'train_runtime': 180.6133, 'train_samples_per_second': 0.709, 'train_steps_per_second': 0.089, 'train_loss': 0.0032686228514648974, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [03:00<00:00, 11.32s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:00<00:00, 11.32s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [03:00<00:00, 11.32s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [03:00<00:00, 11.29s/it]\n",
      "6.  12%|█▎        | 2/16 [00:40<04:40, 20.05s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [01:58<01:58, 14.84s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  93%|█████████▎| 28/30 [3:43:55<19:49, 594.97s/it]*** -> Training took 180.6133 seconds.\n",
      "7.  97%|█████████▋| 29/30 [3:44:09<07:55, 475.95s/it]retraining model for key 'fc7cae8d' (retrain_dataset_size=5)\n",
      "7. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 956.60 examples/s]\u001b[A\n",
      "7. Map: 100%|██████████| 128/128 [00:00<00:00, 940.71 examples/s]\n",
      "7. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "7.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "7. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "7. \\        /    Total batch size = 8 | Total steps = 16\n",
      "7.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  19%|█▉        | 3/16 [01:00<04:20, 20.05s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [02:12<01:43, 14.84s/it]\u001b[A\n",
      "7.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:27<01:29, 14.85s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [01:20<04:00, 20.01s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [02:42<01:14, 14.86s/it]\u001b[A\n",
      "7.   6%|▋         | 1/16 [00:19<04:49, 19.32s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [01:40<03:40, 20.01s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [02:57<00:59, 14.87s/it]\u001b[A\n",
      "7.  12%|█▎        | 2/16 [00:38<04:29, 19.27s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:59<03:19, 19.96s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:12<00:44, 14.87s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  77%|███████▋  | 23/30 [3:40:46<1:35:37, 819.63s/it]*** -> Training took 291.673 seconds.\n",
      "4.  80%|████████  | 24/30 [3:43:56<1:11:55, 719.29s/it]retraining model for key 'bf45cf4b' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 1613.87 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  19%|█▉        | 3/16 [00:57<04:10, 19.27s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [03:27<00:29, 14.87s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [02:19<02:59, 19.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0092, 'grad_norm': 0.45221665501594543, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:40<02:39, 20.00s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "4.   6%|▋         | 1/16 [00:09<02:28,  9.90s/it]\u001b[A\n",
      "7.  25%|██▌       | 4/16 [01:17<03:51, 19.29s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [03:42<00:14, 14.86s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  83%|████████▎ | 25/30 [3:32:49<40:45, 489.06s/it]\n",
      "5. {'loss': 0.001, 'grad_norm': 0.033129796385765076, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  83%|████████▎ | 25/30 [3:34:48<40:45, 489.06s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [03:57<00:00, 14.86s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [03:57<00:00, 14.86s/it]\u001b[A\n",
      "5. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'train_runtime': 237.0597, 'train_samples_per_second': 0.54, 'train_steps_per_second': 0.067, 'train_loss': 0.0026431650621816516, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [03:57<00:00, 14.86s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [03:57<00:00, 14.82s/it]\n",
      "4.  12%|█▎        | 2/16 [00:19<02:18,  9.87s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [02:40<02:39, 20.00s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:29<02:08,  9.86s/it]\u001b[A\n",
      "7.  31%|███▏      | 5/16 [01:36<03:32, 19.30s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:39<01:58,  9.85s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [03:00<02:20, 20.01s/it]\u001b[A\n",
      "7.  38%|███▊      | 6/16 [01:55<03:13, 19.30s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:49<01:48,  9.87s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [00:59<01:38,  9.90s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [03:20<02:00, 20.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  83%|████████▎ | 25/30 [3:34:48<40:45, 489.06s/it]*** -> Training took 237.0597 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  44%|████▍     | 7/16 [02:15<02:53, 19.31s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. {'loss': 0.0283, 'grad_norm': 0.5998648405075073, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7.  50%|█████     | 8/16 [02:34<02:34, 19.30s/it]\u001b[A\n",
      "7. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  87%|████████▋ | 26/30 [3:35:45<28:45, 431.45s/it]retraining model for key 'd59b0160' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 975.81 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 958.80 examples/s]\n",
      "4.  44%|████▍     | 7/16 [01:09<01:29,  9.93s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.007, 'grad_norm': 0.47900575399398804, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:19<01:19,  9.96s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [01:19<01:19,  9.96s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [03:39<01:39, 19.98s/it]\u001b[A\n",
      "7.  50%|█████     | 8/16 [02:34<02:34, 19.30s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [01:29<01:09,  9.99s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:39<01:00, 10.01s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [03:59<01:19, 19.96s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:17<04:27, 17.85s/it]\u001b[A\n",
      "7.  56%|█████▋    | 9/16 [02:53<02:15, 19.30s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [01:49<00:50, 10.02s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [01:59<00:40, 10.01s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [04:19<00:59, 19.95s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:35<04:09, 17.81s/it]\u001b[A\n",
      "7.  62%|██████▎   | 10/16 [03:12<01:55, 19.30s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [02:09<00:29, 10.00s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [02:19<00:19,  9.99s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [04:39<00:39, 19.96s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:53<03:50, 17.76s/it]\u001b[A\n",
      "7.  69%|██████▉   | 11/16 [03:32<01:36, 19.30s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [02:29<00:09,  9.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  80%|████████  | 24/30 [3:45:18<1:11:55, 719.29s/it]\n",
      "4. {'loss': 0.002, 'grad_norm': 0.5310130715370178, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  80%|████████  | 24/30 [3:46:38<1:11:55, 719.29s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:39<00:00,  9.95s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [02:39<00:00,  9.95s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'train_runtime': 159.2183, 'train_samples_per_second': 0.804, 'train_steps_per_second': 0.1, 'train_loss': 0.004507002653554082, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [02:39<00:00,  9.95s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [02:39<00:00,  9.95s/it]\n",
      "5.  25%|██▌       | 4/16 [01:10<03:32, 17.72s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [04:59<00:19, 19.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  73%|███████▎  | 22/30 [3:39:39<1:29:08, 668.58s/it]\n",
      "6. {'loss': 0.0034, 'grad_norm': 0.12535515427589417, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  73%|███████▎  | 22/30 [3:42:19<1:29:08, 668.58s/it]\n",
      "6. {'train_runtime': 319.8285, 'train_samples_per_second': 0.4, 'train_steps_per_second': 0.05, 'train_loss': 0.006279423832893372, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [05:19<00:00, 19.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:19<00:00, 19.98s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [05:19<00:00, 19.98s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [05:19<00:00, 19.99s/it]\n",
      "7.  75%|███████▌  | 12/16 [03:51<01:17, 19.31s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  80%|████████  | 24/30 [3:46:38<1:11:55, 719.29s/it]*** -> Training took 159.2183 seconds.\n",
      "4.  83%|████████▎ | 25/30 [3:46:56<46:27, 557.54s/it]  retraining model for key 'd35bdbdc' (retrain_dataset_size=15)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 2136.70 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [01:28<03:14, 17.71s/it]\u001b[A\n",
      "7.  81%|████████▏ | 13/16 [04:10<00:57, 19.31s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:07<01:49,  7.27s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:14<01:41,  7.25s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [01:46<02:57, 17.72s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:21<01:34,  7.23s/it]\u001b[A\n",
      "7.  88%|████████▊ | 14/16 [04:30<00:38, 19.31s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [00:28<01:26,  7.23s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [02:04<02:39, 17.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0026, 'grad_norm': 0.41488978266716003, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [02:21<02:21, 17.74s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  31%|███▏      | 5/16 [00:36<01:19,  7.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  73%|███████▎  | 22/30 [3:42:19<1:29:08, 668.58s/it]*** -> Training took 319.8285 seconds.\n",
      "6.  77%|███████▋  | 23/30 [3:43:14<1:07:51, 581.66s/it]retraining model for key 'db695cfb' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1050.30 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1032.06 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  38%|███▊      | 6/16 [00:43<01:12,  7.23s/it]\u001b[A\n",
      "7.  94%|█████████▍| 15/16 [04:49<00:19, 19.33s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  97%|█████████▋| 29/30 [3:46:47<07:55, 475.95s/it]\n",
      "7. {'loss': 0.0037, 'grad_norm': 0.4727959632873535, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "7.  97%|█████████▋| 29/30 [3:49:21<07:55, 475.95s/it]\n",
      "7. {'train_runtime': 308.9688, 'train_samples_per_second': 0.414, 'train_steps_per_second': 0.052, 'train_loss': 0.01602553960401565, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7. 100%|██████████| 16/16 [05:08<00:00, 19.34s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:08<00:00, 19.34s/it]\u001b[A\n",
      "7. \u001b[A\n",
      "7. 100%|██████████| 16/16 [05:08<00:00, 19.34s/it]\u001b[A\n",
      "7. 100%|██████████| 16/16 [05:08<00:00, 19.31s/it]\n",
      "4.  44%|████▍     | 7/16 [00:50<01:05,  7.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0096, 'grad_norm': 0.5668365359306335, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [00:57<00:57,  7.23s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "5.  50%|█████     | 8/16 [02:21<02:21, 17.74s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [00:57<00:57,  7.23s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.  97%|█████████▋| 29/30 [3:49:21<07:55, 475.95s/it]*** -> Training took 308.9688 seconds.\n",
      "7. 100%|██████████| 30/30 [3:49:37<00:00, 431.52s/it]\n",
      "7. 100%|██████████| 30/30 [3:49:37<00:00, 459.25s/it]\n",
      "7. *** Completed inference run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  56%|█████▋    | 9/16 [01:05<00:50,  7.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:   0%|          | 0/28 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  56%|█████▋    | 9/16 [02:39<02:04, 17.73s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [01:12<00:43,  7.23s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:16<04:04, 16.32s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [01:19<00:36,  7.23s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [01:26<00:28,  7.23s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [02:57<01:46, 17.73s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:32<03:48, 16.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:   4%|▎         | 1/28 [00:09<04:11,  9.33s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  81%|████████▏ | 13/16 [01:34<00:21,  7.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:   7%|▋         | 2/28 [00:30<07:01, 16.22s/it]\n",
      "7. calculate augmented scores:  11%|█         | 3/28 [00:33<04:13, 10.14s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  88%|████████▊ | 14/16 [01:41<00:14,  7.23s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [03:15<01:28, 17.72s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:48<03:32, 16.33s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [01:48<00:07,  7.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  83%|████████▎ | 25/30 [3:47:57<46:27, 557.54s/it]\n",
      "4. {'loss': 0.0012, 'grad_norm': 0.17086081206798553, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  83%|████████▎ | 25/30 [3:48:55<46:27, 557.54s/it]\n",
      "4. {'train_runtime': 115.7194, 'train_samples_per_second': 1.106, 'train_steps_per_second': 0.138, 'train_loss': 0.0053915337775833905, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [01:55<00:00,  7.23s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [01:55<00:00,  7.23s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [01:55<00:00,  7.23s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [01:55<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  14%|█▍        | 4/28 [00:36<02:54,  7.26s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  25%|██▌       | 4/16 [01:05<03:15, 16.31s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [03:32<01:10, 17.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  18%|█▊        | 5/28 [01:00<05:13, 13.61s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  31%|███▏      | 5/16 [01:21<02:59, 16.28s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [03:50<00:53, 17.72s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:37<02:42, 16.25s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [04:08<00:35, 17.72s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:53<02:26, 16.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0052, 'grad_norm': 0.2116723358631134, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [02:10<02:09, 16.24s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  21%|██▏       | 6/28 [01:13<04:49, 13.15s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  94%|█████████▍| 15/16 [04:25<00:17, 17.73s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  87%|████████▋ | 26/30 [3:38:10<28:45, 431.45s/it]\n",
      "5. {'loss': 0.0004, 'grad_norm': 0.2824195921421051, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  87%|████████▋ | 26/30 [3:40:32<28:45, 431.45s/it]\n",
      "5. {'train_runtime': 283.6792, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.056, 'train_loss': 0.0015252268058247864, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [04:43<00:00, 17.72s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:43<00:00, 17.72s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [04:43<00:00, 17.72s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [04:43<00:00, 17.73s/it]\n",
      "6.  50%|█████     | 8/16 [02:10<02:09, 16.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  25%|██▌       | 7/28 [01:51<07:30, 21.44s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  56%|█████▋    | 9/16 [02:26<01:53, 16.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  29%|██▊       | 8/28 [02:20<07:57, 23.89s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  62%|██████▎   | 10/16 [02:42<01:37, 16.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  32%|███▏      | 9/28 [02:26<05:46, 18.25s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  69%|██████▉   | 11/16 [02:58<01:21, 16.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  36%|███▌      | 10/28 [02:46<05:36, 18.70s/it]\n",
      "4.  83%|████████▎ | 25/30 [3:48:55<46:27, 557.54s/it]*** -> Training took 115.7194 seconds.\n",
      "4.  87%|████████▋ | 26/30 [3:51:12<31:08, 467.06s/it]retraining model for key 'dfadab01' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 763.73 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 752.89 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n",
      "7. calculate augmented scores:  39%|███▉      | 11/28 [03:01<05:01, 17.73s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  75%|███████▌  | 12/16 [03:15<01:04, 16.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  43%|████▎     | 12/28 [03:09<03:51, 14.48s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  81%|████████▏ | 13/16 [03:31<00:48, 16.24s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  46%|████▋     | 13/28 [03:27<03:55, 15.70s/it]\n",
      "7. calculate augmented scores:  50%|█████     | 14/28 [03:37<03:14, 13.88s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  88%|████████▊ | 14/16 [03:47<00:32, 16.23s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:22<05:30, 22.06s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [04:03<00:16, 16.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  77%|███████▋  | 23/30 [3:45:28<1:07:51, 581.66s/it]\n",
      "6. {'loss': 0.0016, 'grad_norm': 0.0637824609875679, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  77%|███████▋  | 23/30 [3:47:38<1:07:51, 581.66s/it]\n",
      "6. {'train_runtime': 260.0914, 'train_samples_per_second': 0.492, 'train_steps_per_second': 0.062, 'train_loss': 0.0033914277446456254, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [04:20<00:00, 16.24s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:20<00:00, 16.24s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [04:20<00:00, 16.24s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [04:20<00:00, 16.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  87%|████████▋ | 26/30 [3:40:32<28:45, 431.45s/it]*** -> Training took 283.6792 seconds.\n",
      "5.  90%|█████████ | 27/30 [3:42:47<21:26, 428.70s/it]retraining model for key 'd8e07eb2' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 288.49 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 285.90 examples/s]\n",
      "4.  12%|█▎        | 2/16 [00:44<05:09, 22.08s/it]\u001b[A\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n",
      "7. calculate augmented scores:  54%|█████▎    | 15/28 [03:43<02:31, 11.62s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  19%|█▉        | 3/16 [01:06<04:48, 22.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  57%|█████▋    | 16/28 [04:37<04:51, 24.28s/it]\n",
      "6.  77%|███████▋  | 23/30 [3:47:38<1:07:51, 581.66s/it]*** -> Training took 260.0914 seconds.\n",
      "6.  80%|████████  | 24/30 [3:48:27<50:07, 501.20s/it]  retraining model for key 'dbff022c' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 2162.29 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  25%|██▌       | 4/16 [01:28<04:27, 22.27s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:07<01:55,  7.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  61%|██████    | 17/28 [04:50<03:50, 20.96s/it]\n",
      "7. calculate augmented scores:  64%|██████▍   | 18/28 [05:12<03:33, 21.34s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  12%|█▎        | 2/16 [00:15<01:47,  7.67s/it]\u001b[A\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:51<04:05, 22.32s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:22<01:39,  7.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  68%|██████▊   | 19/28 [05:16<02:25, 16.12s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  25%|██▌       | 4/16 [00:30<01:31,  7.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  71%|███████▏  | 20/28 [05:31<02:05, 15.75s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  31%|███▏      | 5/16 [00:38<01:24,  7.66s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [02:13<03:42, 22.28s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [00:45<01:16,  7.66s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [00:53<01:08,  7.65s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0023, 'grad_norm': 0.4227255880832672, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:01<01:01,  7.66s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  75%|███████▌  | 21/28 [05:39<01:33, 13.32s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:01<01:01,  7.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  79%|███████▊  | 22/28 [05:58<01:31, 15.26s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  44%|████▍     | 7/16 [02:35<03:20, 22.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0117, 'grad_norm': 0.195554718375206, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:57<02:58, 22.26s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:08<00:53,  7.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  82%|████████▏ | 23/28 [06:05<01:03, 12.74s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  62%|██████▎   | 10/16 [01:16<00:46,  7.67s/it]\u001b[A\n",
      "5.   6%|▋         | 1/16 [01:02<15:43, 62.90s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [01:24<00:38,  7.68s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:57<02:58, 22.26s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [01:32<00:30,  7.68s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  86%|████████▌ | 24/28 [06:12<00:43, 10.94s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  81%|████████▏ | 13/16 [01:39<00:23,  7.67s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [01:47<00:15,  7.67s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [03:20<02:35, 22.27s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [01:54<00:07,  7.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  80%|████████  | 24/30 [3:49:32<50:07, 501.20s/it]\n",
      "6. {'loss': 0.0007, 'grad_norm': 0.015289627015590668, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  80%|████████  | 24/30 [3:50:33<50:07, 501.20s/it]\n",
      "6. {'train_runtime': 122.6108, 'train_samples_per_second': 1.044, 'train_steps_per_second': 0.13, 'train_loss': 0.001503572188084945, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:02<00:00,  7.65s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:02<00:00,  7.65s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:02<00:00,  7.65s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:02<00:00,  7.66s/it]\n",
      "4.  62%|██████▎   | 10/16 [03:42<02:13, 22.28s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [02:05<14:34, 62.49s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [04:04<01:51, 22.27s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [04:27<01:29, 22.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  89%|████████▉ | 25/28 [06:39<00:47, 15.70s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  81%|████████▏ | 13/16 [04:49<01:06, 22.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  80%|████████  | 24/30 [3:50:33<50:07, 501.20s/it]*** -> Training took 122.6108 seconds.\n",
      "6.  83%|████████▎ | 25/30 [3:51:58<34:30, 414.12s/it]retraining model for key 'dd6b8c4b' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1865.00 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  19%|█▉        | 3/16 [03:07<13:31, 62.43s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:08<02:09,  8.61s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [05:11<00:44, 22.24s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:17<02:01,  8.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  93%|█████████▎| 26/28 [08:11<01:17, 38.55s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  19%|█▉        | 3/16 [00:25<01:52,  8.63s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [05:33<00:22, 22.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  87%|████████▋ | 26/30 [3:54:14<31:08, 467.06s/it]\n",
      "4. {'loss': 0.0052, 'grad_norm': 0.2449154555797577, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  87%|████████▋ | 26/30 [3:57:12<31:08, 467.06s/it]\n",
      "4. {'train_runtime': 356.0272, 'train_samples_per_second': 0.36, 'train_steps_per_second': 0.045, 'train_loss': 0.008462432073429227, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [05:56<00:00, 22.25s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:56<00:00, 22.25s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [05:56<00:00, 22.25s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [05:56<00:00, 22.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. calculate augmented scores:  96%|█████████▋| 27/28 [08:59<00:41, 41.34s/it]\n",
      "7. calculate augmented scores: 100%|██████████| 28/28 [09:07<00:00, 31.47s/it]\n",
      "7. calculate augmented scores: 100%|██████████| 28/28 [09:07<00:00, 19.56s/it]\n",
      "7. *** GPU: NVIDIA L4, used 12.9 / 22.3 GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  25%|██▌       | 4/16 [00:34<01:43,  8.62s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [00:43<01:34,  8.63s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [00:51<01:26,  8.65s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [04:09<12:28, 62.36s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:00<01:18,  8.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0177, 'grad_norm': 0.3905198276042938, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:09<01:09,  8.70s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6.  50%|█████     | 8/16 [01:09<01:09,  8.70s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:18<01:01,  8.72s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [01:26<00:52,  8.75s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [01:35<00:43,  8.76s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [01:44<00:35,  8.76s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  87%|████████▋ | 26/30 [3:57:12<31:08, 467.06s/it]*** -> Training took 356.0272 seconds.\n",
      "4.  90%|█████████ | 27/30 [3:58:31<22:55, 458.62s/it]retraining model for key 'e376de54' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 978.40 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 961.52 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n",
      "6.  81%|████████▏ | 13/16 [01:53<00:26,  8.76s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  31%|███▏      | 5/16 [05:12<11:26, 62.38s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [02:01<00:17,  8.76s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [02:10<00:08,  8.75s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  83%|████████▎ | 25/30 [3:53:11<34:30, 414.12s/it]\n",
      "6. {'loss': 0.0031, 'grad_norm': 0.24586121737957, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.74s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  83%|████████▎ | 25/30 [3:54:21<34:30, 414.12s/it]\n",
      "6. {'train_runtime': 139.416, 'train_samples_per_second': 0.918, 'train_steps_per_second': 0.115, 'train_loss': 0.010409485548734665, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.74s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:19<00:00,  8.71s/it]\n",
      "4.   6%|▋         | 1/16 [00:16<04:13, 16.93s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [00:33<03:56, 16.90s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [06:14<10:23, 62.34s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:50<03:39, 16.90s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:07<03:22, 16.91s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  83%|████████▎ | 25/30 [3:54:21<34:30, 414.12s/it]*** -> Training took 139.416 seconds.\n",
      "6.  87%|████████▋ | 26/30 [3:55:41<23:46, 356.66s/it]retraining model for key 'e3721c99' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 444.54 examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 439.41 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n",
      "4.  31%|███▏      | 5/16 [01:24<03:05, 16.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  38%|███▊      | 6/16 [01:41<02:49, 16.90s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [07:16<09:21, 62.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0101, 'grad_norm': 0.12756319344043732, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [08:19<08:18, 62.36s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "4.  44%|████▍     | 7/16 [01:58<02:32, 16.91s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0022, 'grad_norm': 0.15851931273937225, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:15<02:15, 16.91s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:15<02:15, 16.91s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:32<01:58, 16.93s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:40<10:08, 40.60s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [02:49<01:41, 16.94s/it]\u001b[A\n",
      "5.  50%|█████     | 8/16 [08:19<08:18, 62.36s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:06<01:24, 16.95s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [03:23<01:07, 16.95s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [01:20<09:24, 40.34s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [03:40<00:50, 16.96s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [09:21<07:16, 62.36s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [03:57<00:33, 16.95s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [02:00<08:43, 40.28s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [04:13<00:16, 16.96s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  90%|█████████ | 27/30 [4:00:49<22:55, 458.62s/it]\n",
      "4. {'loss': 0.0009, 'grad_norm': 0.01447428297251463, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:30<00:00, 16.96s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:30<00:00, 16.96s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  90%|█████████ | 27/30 [4:03:05<22:55, 458.62s/it]\n",
      "4. {'train_runtime': 270.96, 'train_samples_per_second': 0.472, 'train_steps_per_second': 0.059, 'train_loss': 0.0015798072854522616, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:30<00:00, 16.96s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [04:30<00:00, 16.93s/it]\n",
      "6.  25%|██▌       | 4/16 [02:41<08:03, 40.28s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [10:23<06:14, 62.36s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [03:21<07:22, 40.27s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  90%|█████████ | 27/30 [4:03:05<22:55, 458.62s/it]*** -> Training took 270.96 seconds.\n",
      "4.  93%|█████████▎| 28/30 [4:04:41<14:24, 432.24s/it]retraining model for key 'e87109e9' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 494.32 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 488.10 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  69%|██████▉   | 11/16 [11:26<05:11, 62.35s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [04:01<06:42, 40.28s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [04:42<06:02, 40.27s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0022, 'grad_norm': 0.012254561297595501, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [05:22<05:22, 40.27s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "5.  75%|███████▌  | 12/16 [12:28<04:09, 62.34s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:34<08:37, 34.48s/it]\u001b[A\n",
      "6.  50%|█████     | 8/16 [05:22<05:22, 40.27s/it]\u001b[A\n",
      "4.  12%|█▎        | 2/16 [01:08<08:03, 34.50s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [13:30<03:07, 62.34s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [06:02<04:41, 40.29s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [01:43<07:29, 34.56s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [02:18<06:55, 34.62s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [06:42<04:01, 40.28s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [14:33<02:04, 62.32s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [02:53<06:21, 34.66s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [07:23<03:21, 40.28s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [03:27<05:46, 34.68s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [08:03<02:41, 40.29s/it]\u001b[A\n",
      "5.  94%|█████████▍| 15/16 [15:35<01:02, 62.32s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  90%|█████████ | 27/30 [3:51:11<21:26, 428.70s/it]\n",
      "5. {'loss': 0.0043, 'grad_norm': 0.06269937753677368, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  90%|█████████ | 27/30 [3:59:30<21:26, 428.70s/it]\n",
      "5. {'train_runtime': 997.7572, 'train_samples_per_second': 0.128, 'train_steps_per_second': 0.016, 'train_loss': 0.007199503248557448, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [16:37<00:00, 62.31s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [16:37<00:00, 62.31s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [16:37<00:00, 62.31s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [16:37<00:00, 62.36s/it]\n",
      "4.  44%|████▍     | 7/16 [04:02<05:12, 34.72s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.0087, 'grad_norm': 0.20804952085018158, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [04:37<04:38, 34.76s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  81%|████████▏ | 13/16 [08:43<02:00, 40.27s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [04:37<04:38, 34.76s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [09:23<01:20, 40.27s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [05:12<04:03, 34.82s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [10:04<00:40, 40.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  87%|████████▋ | 26/30 [4:01:08<23:46, 356.66s/it]\n",
      "6. {'loss': 0.0008, 'grad_norm': 0.13306556642055511, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  87%|████████▋ | 26/30 [4:06:30<23:46, 356.66s/it]\n",
      "6. {'train_runtime': 644.4822, 'train_samples_per_second': 0.199, 'train_steps_per_second': 0.025, 'train_loss': 0.0014586011820938438, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [10:44<00:00, 40.26s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [10:44<00:00, 40.26s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [10:44<00:00, 40.26s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [10:44<00:00, 40.28s/it]\n",
      "4.  62%|██████▎   | 10/16 [05:47<03:29, 34.87s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [06:22<02:54, 34.89s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [06:57<02:19, 34.95s/it]\u001b[A\n",
      "4.  81%|████████▏ | 13/16 [07:32<01:44, 34.94s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [08:07<01:09, 34.94s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  90%|█████████ | 27/30 [3:59:30<21:26, 428.70s/it]*** -> Training took 997.7572 seconds.\n",
      "5.  93%|█████████▎| 28/30 [4:04:02<22:45, 682.60s/it]retraining model for key 'e12f9a14' (retrain_dataset_size=10)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 650.52 examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 641.70 examples/s]\n",
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  94%|█████████▍| 15/16 [08:42<00:34, 34.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  93%|█████████▎| 28/30 [4:09:23<14:24, 432.24s/it]\n",
      "4. {'loss': 0.001, 'grad_norm': 0.18142998218536377, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  93%|█████████▎| 28/30 [4:14:03<14:24, 432.24s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [09:17<00:00, 34.98s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [09:17<00:00, 34.98s/it]\u001b[A\n",
      "4. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'train_runtime': 557.3003, 'train_samples_per_second': 0.23, 'train_steps_per_second': 0.029, 'train_loss': 0.004895133955869824, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [09:17<00:00, 34.98s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [09:17<00:00, 34.83s/it]\n",
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.   6%|▋         | 1/16 [00:27<06:47, 27.18s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:54<06:20, 27.16s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [01:21<05:52, 27.15s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [01:48<05:25, 27.15s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  87%|████████▋ | 26/30 [4:06:30<23:46, 356.66s/it]*** -> Training took 644.4822 seconds.\n",
      "6.  90%|█████████ | 27/30 [4:11:42<26:53, 537.84s/it]retraining model for key 'e8686506' (retrain_dataset_size=5)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 3030.98 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [02:15<04:58, 27.16s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:05<01:18,  5.23s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:10<01:12,  5.21s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:15<01:07,  5.21s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:20<01:02,  5.22s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [00:26<00:57,  5.23s/it]\u001b[A\n",
      "5.  38%|███▊      | 6/16 [02:42<04:31, 27.16s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [00:31<00:52,  5.23s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [00:36<00:47,  5.24s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0268, 'grad_norm': 0.6024872660636902, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [00:41<00:41,  5.24s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  93%|█████████▎| 28/30 [4:14:03<14:24, 432.24s/it]*** -> Training took 557.3003 seconds.\n",
      "4.  97%|█████████▋| 29/30 [4:16:58<08:43, 523.62s/it]retraining model for key 'edb79dae' (retrain_dataset_size=5)\n",
      "4. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 971.24 examples/s]\u001b[A\n",
      "4. Map: 100%|██████████| 128/128 [00:00<00:00, 952.35 examples/s]\n",
      "4. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "4.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "4. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "4. \\        /    Total batch size = 8 | Total steps = 16\n",
      "4.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [00:41<00:41,  5.24s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [00:47<00:36,  5.24s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [00:52<00:31,  5.25s/it]\u001b[A\n",
      "5.  44%|████▍     | 7/16 [03:10<04:04, 27.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0035, 'grad_norm': 0.3062807023525238, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [03:37<03:37, 27.13s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "6.  69%|██████▉   | 11/16 [00:57<00:26,  5.24s/it]\u001b[A\n",
      "4.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [01:02<00:20,  5.22s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [01:07<00:15,  5.22s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [01:13<00:10,  5.21s/it]\u001b[A\n",
      "4.   6%|▋         | 1/16 [00:17<04:17, 17.19s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [01:18<00:05,  5.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  90%|█████████ | 27/30 [4:12:26<26:53, 537.84s/it]\n",
      "6. {'loss': 0.0093, 'grad_norm': 1.0301649570465088, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  90%|█████████ | 27/30 [4:13:08<26:53, 537.84s/it]\n",
      "6. {'train_runtime': 83.6065, 'train_samples_per_second': 1.531, 'train_steps_per_second': 0.191, 'train_loss': 0.018059154972434044, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [01:23<00:00,  5.21s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [01:23<00:00,  5.21s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [01:23<00:00,  5.21s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [01:23<00:00,  5.22s/it]\n",
      "5.  50%|█████     | 8/16 [03:37<03:37, 27.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  90%|█████████ | 27/30 [4:13:08<26:53, 537.84s/it]*** -> Training took 83.6065 seconds.\n",
      "6.  93%|█████████▎| 28/30 [4:13:17<13:30, 405.12s/it]retraining model for key 'f560132c' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1500.18 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  12%|█▎        | 2/16 [00:34<04:00, 17.19s/it]\u001b[A\n",
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [04:04<03:10, 27.15s/it]\u001b[A\n",
      "4.  19%|█▉        | 3/16 [00:51<03:43, 17.19s/it]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:10<02:43, 10.87s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:21<02:32, 10.93s/it]\u001b[A\n",
      "4.  25%|██▌       | 4/16 [01:08<03:26, 17.19s/it]\u001b[A\n",
      "6.  19%|█▉        | 3/16 [00:32<02:22, 10.97s/it]\u001b[A\n",
      "5.  62%|██████▎   | 10/16 [04:31<02:42, 27.15s/it]\u001b[A\n",
      "4.  31%|███▏      | 5/16 [01:25<03:09, 17.19s/it]\u001b[A\n",
      "6.  25%|██▌       | 4/16 [00:43<02:11, 10.96s/it]\u001b[A\n",
      "6.  31%|███▏      | 5/16 [00:54<02:00, 10.96s/it]\u001b[A\n",
      "4.  38%|███▊      | 6/16 [01:43<02:51, 17.19s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [04:58<02:15, 27.15s/it]\u001b[A\n",
      "6.  38%|███▊      | 6/16 [01:05<01:49, 10.94s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:16<01:38, 10.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0474, 'grad_norm': 6.112204551696777, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:27<01:27, 10.90s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "4.  44%|████▍     | 7/16 [02:00<02:34, 17.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. {'loss': 0.008, 'grad_norm': 0.3137953579425812, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.  50%|█████     | 8/16 [02:17<02:17, 17.18s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "6.  50%|█████     | 8/16 [01:27<01:27, 10.90s/it]\u001b[A\n",
      "5.  75%|███████▌  | 12/16 [05:25<01:48, 27.13s/it]\u001b[A\n",
      "4.  50%|█████     | 8/16 [02:17<02:17, 17.18s/it]\u001b[A\n",
      "6.  56%|█████▋    | 9/16 [01:38<01:16, 10.91s/it]\u001b[A\n",
      "6.  62%|██████▎   | 10/16 [01:49<01:05, 10.93s/it]\u001b[A\n",
      "4.  56%|█████▋    | 9/16 [02:34<02:00, 17.19s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [05:52<01:21, 27.15s/it]\u001b[A\n",
      "6.  69%|██████▉   | 11/16 [02:00<00:54, 10.91s/it]\u001b[A\n",
      "4.  62%|██████▎   | 10/16 [02:51<01:43, 17.19s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:11<00:43, 10.91s/it]\u001b[A\n",
      "6.  81%|████████▏ | 13/16 [02:22<00:32, 10.91s/it]\u001b[A\n",
      "5.  88%|████████▊ | 14/16 [06:20<00:54, 27.15s/it]\u001b[A\n",
      "4.  69%|██████▉   | 11/16 [03:09<01:25, 17.18s/it]\u001b[A\n",
      "6.  88%|████████▊ | 14/16 [02:32<00:21, 10.92s/it]\u001b[A\n",
      "4.  75%|███████▌  | 12/16 [03:26<01:08, 17.18s/it]\u001b[A\n",
      "6.  94%|█████████▍| 15/16 [02:43<00:10, 10.93s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  93%|█████████▎| 28/30 [4:14:48<13:30, 405.12s/it]\n",
      "6. {'loss': 0.0103, 'grad_norm': 3.2201054096221924, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:54<00:00, 10.91s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  93%|█████████▎| 28/30 [4:16:15<13:30, 405.12s/it]\n",
      "6. {'train_runtime': 174.7948, 'train_samples_per_second': 0.732, 'train_steps_per_second': 0.092, 'train_loss': 0.028875060845166445, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:54<00:00, 10.91s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:54<00:00, 10.91s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:54<00:00, 10.92s/it]\n",
      "5.  94%|█████████▍| 15/16 [06:47<00:27, 27.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  93%|█████████▎| 28/30 [4:07:44<22:45, 682.60s/it]\n",
      "5. {'loss': 0.0008, 'grad_norm': 0.27955859899520874, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  93%|█████████▎| 28/30 [4:11:21<22:45, 682.60s/it]\n",
      "5. {'train_runtime': 434.2978, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.037, 'train_loss': 0.002169808925827965, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [07:14<00:00, 27.12s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [07:14<00:00, 27.12s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [07:14<00:00, 27.12s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [07:14<00:00, 27.14s/it]\n",
      "4.  81%|████████▏ | 13/16 [03:43<00:51, 17.19s/it]\u001b[A\n",
      "4.  88%|████████▊ | 14/16 [04:00<00:34, 17.19s/it]\u001b[A\n",
      "4.  94%|█████████▍| 15/16 [04:17<00:17, 17.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  97%|█████████▋| 29/30 [4:19:19<08:43, 523.62s/it]\n",
      "4. {'loss': 0.0008, 'grad_norm': 0.07075163722038269, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "4.  97%|█████████▋| 29/30 [4:21:37<08:43, 523.62s/it]\n",
      "4. {'train_runtime': 275.0147, 'train_samples_per_second': 0.465, 'train_steps_per_second': 0.058, 'train_loss': 0.0043894307455047965, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4. 100%|██████████| 16/16 [04:35<00:00, 17.19s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:35<00:00, 17.19s/it]\u001b[A\n",
      "4. \u001b[A\n",
      "4. 100%|██████████| 16/16 [04:35<00:00, 17.19s/it]\u001b[A\n",
      "4. 100%|██████████| 16/16 [04:35<00:00, 17.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  93%|█████████▎| 28/30 [4:16:15<13:30, 405.12s/it]*** -> Training took 174.7948 seconds.\n",
      "6.  97%|█████████▋| 29/30 [4:17:10<05:53, 353.61s/it]retraining model for key 'faa9f03d' (retrain_dataset_size=10)\n",
      "6. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "6. Map: 100%|██████████| 128/128 [00:00<00:00, 1614.10 examples/s]\n",
      "6. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "6.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "6. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "6. \\        /    Total batch size = 8 | Total steps = 16\n",
      "6.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "6.   6%|▋         | 1/16 [00:10<02:36, 10.44s/it]\u001b[A\n",
      "6.  12%|█▎        | 2/16 [00:20<02:25, 10.42s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  97%|█████████▋| 29/30 [4:21:37<08:43, 523.62s/it]*** -> Training took 275.0147 seconds.\n",
      "4. 100%|██████████| 30/30 [4:22:21<00:00, 463.35s/it]\n",
      "4. 100%|██████████| 30/30 [4:22:21<00:00, 524.72s/it]\n",
      "4. *** Completed inference run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  19%|█▉        | 3/16 [00:31<02:15, 10.41s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:   0%|          | 0/23 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  25%|██▌       | 4/16 [00:41<02:04, 10.39s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:   4%|▍         | 1/23 [00:05<01:59,  5.43s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  31%|███▏      | 5/16 [00:51<01:54, 10.38s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:   9%|▊         | 2/23 [00:20<03:51, 11.05s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  38%|███▊      | 6/16 [01:02<01:43, 10.38s/it]\u001b[A\n",
      "6.  44%|████▍     | 7/16 [01:12<01:33, 10.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. {'loss': 0.0018, 'grad_norm': 0.10638858377933502, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:23<01:22, 10.37s/it]\u001b[A\n",
      "6. \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  13%|█▎        | 3/23 [00:30<03:36, 10.82s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  50%|█████     | 8/16 [01:23<01:22, 10.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  17%|█▋        | 4/23 [00:52<04:43, 14.91s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  56%|█████▋    | 9/16 [01:33<01:12, 10.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  22%|██▏       | 5/23 [01:05<04:18, 14.37s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  62%|██████▎   | 10/16 [01:43<01:02, 10.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  26%|██▌       | 6/23 [01:09<03:05, 10.91s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  69%|██████▉   | 11/16 [01:54<00:51, 10.36s/it]\u001b[A\n",
      "6.  75%|███████▌  | 12/16 [02:04<00:41, 10.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  30%|███       | 7/23 [01:25<03:18, 12.43s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  81%|████████▏ | 13/16 [02:14<00:31, 10.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  35%|███▍      | 8/23 [01:41<03:25, 13.68s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  88%|████████▊ | 14/16 [02:25<00:20, 10.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  39%|███▉      | 9/23 [01:51<02:54, 12.43s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.  94%|█████████▍| 15/16 [02:35<00:10, 10.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.  97%|█████████▋| 29/30 [4:18:37<05:53, 353.61s/it]\n",
      "6. {'loss': 0.0012, 'grad_norm': 0.48776867985725403, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "6.  97%|█████████▋| 29/30 [4:20:00<05:53, 353.61s/it]\n",
      "6. {'train_runtime': 165.9731, 'train_samples_per_second': 0.771, 'train_steps_per_second': 0.096, 'train_loss': 0.0015015621320344508, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6. 100%|██████████| 16/16 [02:45<00:00, 10.36s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:45<00:00, 10.36s/it]\u001b[A\n",
      "6. \u001b[A\n",
      "6. 100%|██████████| 16/16 [02:45<00:00, 10.36s/it]\u001b[A\n",
      "6. 100%|██████████| 16/16 [02:45<00:00, 10.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  43%|████▎     | 10/23 [02:06<02:53, 13.38s/it]\n",
      "5.  93%|█████████▎| 28/30 [4:11:21<22:45, 682.60s/it]*** -> Training took 434.2978 seconds.\n",
      "5.  97%|█████████▋| 29/30 [4:15:33<11:25, 685.18s/it]retraining model for key 'eee78d87' (retrain_dataset_size=5)\n",
      "5. *** Set model state_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. Map:   0%|          | 0/128 [00:00<?, ? examples/s]\u001b[A\n",
      "5. Map: 100%|██████████| 128/128 [00:00<00:00, 1600.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  48%|████▊     | 11/23 [02:10<02:05, 10.47s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "5.    \\\\   /|    Num examples = 128 | Num Epochs = 1\n",
      "5. O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
      "5. \\        /    Total batch size = 8 | Total steps = 16\n",
      "5.  \"-____-\"     Number of trainable parameters = 94,044,160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. *** Start training run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  52%|█████▏    | 12/23 [02:44<03:13, 17.63s/it]\n",
      "4. calculate augmented scores:  57%|█████▋    | 13/23 [02:59<02:47, 16.71s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.   6%|▋         | 1/16 [00:10<02:37, 10.50s/it]\u001b[A\n",
      "5.  12%|█▎        | 2/16 [00:20<02:26, 10.47s/it]\u001b[A\n",
      "5.  19%|█▉        | 3/16 [00:31<02:16, 10.46s/it]\u001b[A\n",
      "5.  25%|██▌       | 4/16 [00:41<02:05, 10.45s/it]\u001b[A\n",
      "5.  31%|███▏      | 5/16 [00:52<01:54, 10.45s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  61%|██████    | 14/23 [03:07<02:05, 13.99s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  38%|███▊      | 6/16 [01:02<01:44, 10.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  65%|██████▌   | 15/23 [03:51<03:04, 23.05s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  44%|████▍     | 7/16 [01:13<01:33, 10.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. {'loss': 0.0147, 'grad_norm': 0.06800442934036255, 'learning_rate': 5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  50%|█████     | 8/16 [01:23<01:23, 10.45s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5.  50%|█████     | 8/16 [01:23<01:23, 10.45s/it]\u001b[A\n",
      "5.  56%|█████▋    | 9/16 [01:34<01:13, 10.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  70%|██████▉   | 16/23 [04:07<02:27, 21.02s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  62%|██████▎   | 10/16 [01:44<01:02, 10.45s/it]\u001b[A\n",
      "5.  69%|██████▉   | 11/16 [01:54<00:52, 10.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  74%|███████▍  | 17/23 [04:34<02:16, 22.77s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  75%|███████▌  | 12/16 [02:05<00:41, 10.44s/it]\u001b[A\n",
      "5.  81%|████████▏ | 13/16 [02:15<00:31, 10.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  78%|███████▊  | 18/23 [04:59<01:56, 23.40s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  88%|████████▊ | 14/16 [02:26<00:20, 10.43s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  83%|████████▎ | 19/23 [05:16<01:27, 21.75s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5.  94%|█████████▍| 15/16 [02:36<00:10, 10.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  97%|█████████▋| 29/30 [4:17:00<11:25, 685.18s/it]\n",
      "5. {'loss': 0.0057, 'grad_norm': 0.06220773607492447, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
      "5.  97%|█████████▋| 29/30 [4:18:24<11:25, 685.18s/it]\n",
      "5. {'train_runtime': 167.1316, 'train_samples_per_second': 0.766, 'train_steps_per_second': 0.096, 'train_loss': 0.010196179384365678, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5. 100%|██████████| 16/16 [02:47<00:00, 10.44s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:47<00:00, 10.44s/it]\u001b[A\n",
      "5. \u001b[A\n",
      "5. 100%|██████████| 16/16 [02:47<00:00, 10.44s/it]\u001b[A\n",
      "5. 100%|██████████| 16/16 [02:47<00:00, 10.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. calculate augmented scores:  87%|████████▋ | 20/23 [05:28<00:55, 18.65s/it]\n",
      "4. calculate augmented scores:  91%|█████████▏| 21/23 [05:39<00:32, 16.48s/it]\n",
      "6.  97%|█████████▋| 29/30 [4:20:00<05:53, 353.61s/it]*** -> Training took 165.9731 seconds.\n",
      "6. 100%|██████████| 30/30 [4:23:45<00:00, 365.75s/it]\n",
      "6. 100%|██████████| 30/30 [4:23:45<00:00, 527.50s/it]\n",
      "6. *** Completed inference run.\n",
      "4. calculate augmented scores:  96%|█████████▌| 22/23 [05:49<00:14, 14.46s/it]\n",
      "4. calculate augmented scores: 100%|██████████| 23/23 [05:56<00:00, 12.23s/it]\n",
      "4. calculate augmented scores: 100%|██████████| 23/23 [05:56<00:00, 15.50s/it]\n",
      "4. *** GPU: NVIDIA L4, used 13.6 / 22.3 GB.\n",
      "6. calculate augmented scores:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "6. calculate augmented scores:   3%|▎         | 1/30 [00:29<14:02, 29.07s/it]\n",
      "6. calculate augmented scores:   7%|▋         | 2/30 [00:34<07:07, 15.27s/it]\n",
      "6. calculate augmented scores:  10%|█         | 3/30 [00:44<05:40, 12.62s/it]\n",
      "6. calculate augmented scores:  13%|█▎        | 4/30 [00:53<04:56, 11.40s/it]\n",
      "6. calculate augmented scores:  17%|█▋        | 5/30 [01:02<04:19, 10.37s/it]\n",
      "5.  97%|█████████▋| 29/30 [4:18:24<11:25, 685.18s/it]*** -> Training took 167.1316 seconds.\n",
      "5. 100%|██████████| 30/30 [4:20:14<00:00, 563.82s/it]\n",
      "5. 100%|██████████| 30/30 [4:20:14<00:00, 520.49s/it]\n",
      "5. *** Completed inference run.\n",
      "6. calculate augmented scores:  20%|██        | 6/30 [01:28<06:17, 15.72s/it]\n",
      "6. calculate augmented scores:  23%|██▎       | 7/30 [01:50<06:46, 17.67s/it]\n",
      "6. calculate augmented scores:  27%|██▋       | 8/30 [01:57<05:15, 14.35s/it]\n",
      "6. calculate augmented scores:  30%|███       | 9/30 [02:02<04:03, 11.62s/it]\n",
      "5. calculate augmented scores:   0%|          | 0/26 [00:00<?, ?it/s]\n",
      "6. calculate augmented scores:  33%|███▎      | 10/30 [02:15<03:59, 11.98s/it]\n",
      "5. calculate augmented scores:   4%|▍         | 1/26 [00:53<22:17, 53.51s/it]\n",
      "6. calculate augmented scores:  37%|███▋      | 11/30 [02:31<04:09, 13.15s/it]\n",
      "5. calculate augmented scores:   8%|▊         | 2/26 [01:11<13:01, 32.56s/it]\n",
      "6. calculate augmented scores:  40%|████      | 12/30 [02:58<05:14, 17.49s/it]\n",
      "5. calculate augmented scores:  12%|█▏        | 3/26 [01:29<09:55, 25.90s/it]\n",
      "5. calculate augmented scores:  15%|█▌        | 4/26 [01:41<07:30, 20.46s/it]\n",
      "5. calculate augmented scores:  19%|█▉        | 5/26 [01:58<06:43, 19.23s/it]\n",
      "6. calculate augmented scores:  43%|████▎     | 13/30 [03:11<04:34, 16.13s/it]\n",
      "6. calculate augmented scores:  47%|████▋     | 14/30 [03:41<05:24, 20.27s/it]\n",
      "5. calculate augmented scores:  23%|██▎       | 6/26 [02:08<05:23, 16.15s/it]\n",
      "6. calculate augmented scores:  50%|█████     | 15/30 [03:52<04:23, 17.56s/it]\n",
      "5. calculate augmented scores:  27%|██▋       | 7/26 [02:32<05:55, 18.71s/it]\n",
      "6. calculate augmented scores:  53%|█████▎    | 16/30 [04:10<04:05, 17.53s/it]\n",
      "5. calculate augmented scores:  31%|███       | 8/26 [02:53<05:49, 19.40s/it]\n",
      "5. calculate augmented scores:  35%|███▍      | 9/26 [02:59<04:16, 15.12s/it]\n",
      "6. calculate augmented scores:  57%|█████▋    | 17/30 [04:28<03:48, 17.54s/it]\n",
      "6. calculate augmented scores:  60%|██████    | 18/30 [04:41<03:17, 16.42s/it]\n",
      "5. calculate augmented scores:  38%|███▊      | 10/26 [03:05<03:19, 12.45s/it]\n",
      "6. calculate augmented scores:  63%|██████▎   | 19/30 [04:55<02:50, 15.46s/it]\n",
      "6. calculate augmented scores:  67%|██████▋   | 20/30 [05:01<02:08, 12.81s/it]\n",
      "5. calculate augmented scores:  42%|████▏     | 11/26 [03:28<03:55, 15.73s/it]\n",
      "6. calculate augmented scores:  70%|███████   | 21/30 [05:21<02:14, 14.90s/it]\n",
      "6. calculate augmented scores:  73%|███████▎  | 22/30 [05:43<02:15, 16.93s/it]\n",
      "5. calculate augmented scores:  46%|████▌     | 12/26 [03:51<04:09, 17.84s/it]\n",
      "6. calculate augmented scores:  77%|███████▋  | 23/30 [06:05<02:10, 18.64s/it]\n",
      "6. calculate augmented scores:  80%|████████  | 24/30 [06:16<01:38, 16.42s/it]\n",
      "5. calculate augmented scores:  50%|█████     | 13/26 [04:36<05:39, 26.15s/it]\n",
      "6. calculate augmented scores:  83%|████████▎ | 25/30 [06:33<01:21, 16.30s/it]\n",
      "6. calculate augmented scores:  87%|████████▋ | 26/30 [06:37<00:50, 12.64s/it]\n",
      "6. calculate augmented scores:  90%|█████████ | 27/30 [06:41<00:30, 10.06s/it]\n",
      "6. calculate augmented scores:  93%|█████████▎| 28/30 [06:44<00:15,  7.95s/it]\n",
      "5. calculate augmented scores:  54%|█████▍    | 14/26 [05:05<05:21, 26.77s/it]\n",
      "6. calculate augmented scores:  97%|█████████▋| 29/30 [06:56<00:09,  9.34s/it]\n",
      "6. calculate augmented scores: 100%|██████████| 30/30 [07:15<00:00, 12.17s/it]\n",
      "6. calculate augmented scores: 100%|██████████| 30/30 [07:15<00:00, 14.52s/it]\n",
      "6. *** GPU: NVIDIA L4, used 13.5 / 22.3 GB.\n",
      "5. calculate augmented scores:  58%|█████▊    | 15/26 [05:28<04:41, 25.63s/it]\n",
      "5. calculate augmented scores:  62%|██████▏   | 16/26 [05:48<03:59, 23.98s/it]\n",
      "5. calculate augmented scores:  65%|██████▌   | 17/26 [06:02<03:10, 21.14s/it]\n",
      "5. calculate augmented scores:  69%|██████▉   | 18/26 [06:18<02:36, 19.60s/it]\n",
      "5. calculate augmented scores:  73%|███████▎  | 19/26 [06:34<02:09, 18.55s/it]\n",
      "5. calculate augmented scores:  77%|███████▋  | 20/26 [06:51<01:47, 17.97s/it]\n",
      "5. calculate augmented scores:  81%|████████  | 21/26 [07:07<01:26, 17.32s/it]\n",
      "5. calculate augmented scores:  85%|████████▍ | 22/26 [07:19<01:03, 15.83s/it]\n",
      "5. calculate augmented scores:  88%|████████▊ | 23/26 [07:56<01:06, 22.08s/it]\n",
      "5. calculate augmented scores:  92%|█████████▏| 24/26 [08:19<00:45, 22.53s/it]\n",
      "5. calculate augmented scores:  96%|█████████▌| 25/26 [08:43<00:22, 22.85s/it]\n",
      "5. calculate augmented scores: 100%|██████████| 26/26 [08:57<00:00, 20.27s/it]\n",
      "5. calculate augmented scores: 100%|██████████| 26/26 [08:57<00:00, 20.68s/it]\n",
      "5. *** GPU: NVIDIA L4, used 13.5 / 22.3 GB.\n",
      "*** Subprocesses exit codes: [0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "proc_exit_codes = await wait_for_subprocesses(\n",
    "    train_proc0, train_proc1, train_proc2, train_proc3,\n",
    "    infer_proc0, infer_proc1, infer_proc2, infer_proc3,\n",
    "    print_output=True or arc_test_set.is_fake\n",
    ")\n",
    "print(f'*** Subprocesses exit codes: {proc_exit_codes}')\n",
    "assert all(x==0 for x in proc_exit_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c443fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:57:08.911649Z",
     "iopub.status.busy": "2025-11-13T11:57:08.910842Z",
     "iopub.status.idle": "2025-11-13T11:57:12.600282Z",
     "shell.execute_reply": "2025-11-13T11:57:12.599617Z"
    },
    "papermill": {
     "duration": 4.011376,
     "end_time": "2025-11-13T11:57:12.601761",
     "exception": false,
     "start_time": "2025-11-13T11:57:08.590385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate augmented scores: 100%|██████████| 107/107 [00:03<00:00, 33.50it/s]\n",
      "*** Generating submission for 107 outputs...\n"
     ]
    }
   ],
   "source": [
    "# write submission\n",
    "from common_stuff import *\n",
    "with RemapCudaOOM():\n",
    "    model, formatter, dataset = None, MyFormatter(), None\n",
    "    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, frac_score=True).from_store(infer_params['store'])\n",
    "    if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
    "    submission = arc_test_set.get_submission(decoder.run_selection_algo(submission_select_algo))\n",
    "    with open('submission.json', 'w') as f: json.dump(submission, f)\n",
    "    if arc_test_set.is_fake:\n",
    "        decoder.benchmark_selection_algos(selection_algorithms)\n",
    "        with open('submission.json') as f: reload_submission = json.load(f)\n",
    "        print('*** Reload score:', arc_test_set.validate_submission(reload_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fcf2e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T11:57:13.292018Z",
     "iopub.status.busy": "2025-11-13T11:57:13.291675Z",
     "iopub.status.idle": "2025-11-13T11:57:13.319945Z",
     "shell.execute_reply": "2025-11-13T11:57:13.319331Z"
    },
    "papermill": {
     "duration": 0.354024,
     "end_time": "2025-11-13T11:57:13.321355",
     "exception": false,
     "start_time": "2025-11-13T11:57:12.967331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping inference visualization - not in fake test mode\n"
     ]
    }
   ],
   "source": [
    "# Visualization for inference results from submission.json\n",
    "if arc_test_set.is_fake:\n",
    "    from common_stuff import *\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import colors\n",
    "    import json\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUALIZING RESULTS FROM SUBMISSION.JSON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if submission file exists\n",
    "    submission_path = 'submission.json'\n",
    "    if not os.path.exists(submission_path):\n",
    "        print(f\"Submission file not found at {submission_path}\")\n",
    "    else:\n",
    "        print(f\"Found submission file: {submission_path}\")\n",
    "        \n",
    "        # Load submission data\n",
    "        with open(submission_path, 'r') as f:\n",
    "            submission_data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded submission with {len(submission_data)} tasks\")\n",
    "        \n",
    "        # ARC color map\n",
    "        cmap = colors.ListedColormap(\n",
    "            ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "             '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "        norm = colors.Normalize(vmin=0, vmax=9)\n",
    "        \n",
    "        # Function to check if prediction is non-trivial (not just zeros)\n",
    "        def is_non_trivial_prediction(pred_array):\n",
    "            # Check if the prediction contains any non-zero values\n",
    "            return np.any(np.array(pred_array) > 0)\n",
    "        \n",
    "        # Function to visualize a single task result\n",
    "        def visualize_submission_result(task_id, task_data, submission_output, test_idx):\n",
    "            # Skip visualization if both predictions are just zeros\n",
    "            pred_1 = np.array(submission_output['attempt_1'])\n",
    "            pred_2 = np.array(submission_output['attempt_2'])\n",
    "            \n",
    "            if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n",
    "                print(f\"  Skipping visualization for Task {task_id} - Test #{test_idx+1} (all predictions are zeros)\")\n",
    "                return False\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = plt.figure(figsize=(15, 8))\n",
    "            grid_spec = plt.GridSpec(2, 3, width_ratios=[1, 1, 1])\n",
    "            \n",
    "            # Training examples (first one only for simplicity)\n",
    "            if task_data['train']:\n",
    "                # Train Input\n",
    "                ax1 = fig.add_subplot(grid_spec[0, 0])\n",
    "                ax1.imshow(task_data['train'][0]['input'], cmap=cmap, norm=norm)\n",
    "                ax1.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax1.set_title(\"Training Input\")\n",
    "                ax1.set_xticks([])\n",
    "                ax1.set_yticks([])\n",
    "                \n",
    "                # Train Output\n",
    "                ax2 = fig.add_subplot(grid_spec[1, 0])\n",
    "                ax2.imshow(task_data['train'][0]['output'], cmap=cmap, norm=norm)\n",
    "                ax2.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax2.set_title(\"Training Output\")\n",
    "                ax2.set_xticks([])\n",
    "                ax2.set_yticks([])\n",
    "            \n",
    "            # Test Input\n",
    "            if test_idx < len(task_data['test']):\n",
    "                ax3 = fig.add_subplot(grid_spec[0, 1])\n",
    "                ax3.imshow(task_data['test'][test_idx]['input'], cmap=cmap, norm=norm)\n",
    "                ax3.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax3.set_title(f\"Test Input (Test #{test_idx+1})\")\n",
    "                ax3.set_xticks([])\n",
    "                ax3.set_yticks([])\n",
    "                \n",
    "                # Ground Truth (if available)\n",
    "                if 'output' in task_data['test'][test_idx]:\n",
    "                    ax4 = fig.add_subplot(grid_spec[1, 1])\n",
    "                    ax4.imshow(task_data['test'][test_idx]['output'], cmap=cmap, norm=norm)\n",
    "                    ax4.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                    ax4.set_title(\"Ground Truth\")\n",
    "                    ax4.set_xticks([])\n",
    "                    ax4.set_yticks([])\n",
    "            \n",
    "            # Model Predictions\n",
    "            # Attempt 1\n",
    "            ax5 = fig.add_subplot(grid_spec[0, 2])\n",
    "            ax5.imshow(pred_1, cmap=cmap, norm=norm)\n",
    "            ax5.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax5.set_title(\"Model Prediction (Attempt 1)\")\n",
    "            ax5.set_xticks([])\n",
    "            ax5.set_yticks([])\n",
    "            \n",
    "            # Attempt 2\n",
    "            ax6 = fig.add_subplot(grid_spec[1, 2])\n",
    "            ax6.imshow(pred_2, cmap=cmap, norm=norm)\n",
    "            ax6.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax6.set_title(\"Model Prediction (Attempt 2)\")\n",
    "            ax6.set_xticks([])\n",
    "            ax6.set_yticks([])\n",
    "            \n",
    "            plt.suptitle(f\"Task {task_id} - Test Example #{test_idx+1}\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.9)\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate accuracy if ground truth is available\n",
    "            if 'output' in task_data['test'][test_idx]:\n",
    "                ground_truth = np.array(task_data['test'][test_idx]['output'])\n",
    "                \n",
    "                # Check accuracy of both attempts\n",
    "                results = []\n",
    "                match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                results.append(f\"Attempt 1: {'✓' if match_1 else '✗'}{' (zeros)' if not is_non_trivial_prediction(pred_1) else ''}\")\n",
    "                \n",
    "                match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                results.append(f\"Attempt 2: {'✓' if match_2 else '✗'}{' (zeros)' if not is_non_trivial_prediction(pred_2) else ''}\")\n",
    "                \n",
    "                print(f\"  Results: {', '.join(results)}\")\n",
    "                \n",
    "                # Display task statistics\n",
    "                print(f\"  Shape - Ground Truth: {ground_truth.shape}, Prediction 1: {pred_1.shape}, Prediction 2: {pred_2.shape}\")\n",
    "                print(f\"  Values - Ground Truth unique values: {np.unique(ground_truth)}\")\n",
    "                print(f\"          Prediction 1 unique values: {np.unique(pred_1)}\")\n",
    "                print(f\"          Prediction 2 unique values: {np.unique(pred_2)}\")\n",
    "            print()\n",
    "            return True\n",
    "        \n",
    "        # Process ALL results from submission (no limit)\n",
    "        visualized_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        # Get a list of tasks in the submission\n",
    "        task_ids = list(submission_data.keys())\n",
    "        \n",
    "        # Collect all task/test combinations\n",
    "        all_predictions = []\n",
    "        for task_id in task_ids:\n",
    "            if task_id in arc_test_set.queries:\n",
    "                task_data = arc_test_set.queries[task_id]\n",
    "                for test_idx, test_prediction in enumerate(submission_data[task_id]):\n",
    "                    # Check if we have ground truth available\n",
    "                    has_ground_truth = (task_id in arc_test_set.replies and \n",
    "                                        test_idx < len(arc_test_set.replies[task_id]))\n",
    "                    \n",
    "                    # Check if predictions are non-trivial\n",
    "                    pred_1 = np.array(test_prediction['attempt_1'])\n",
    "                    pred_2 = np.array(test_prediction['attempt_2'])\n",
    "                    has_non_zero_pred = is_non_trivial_prediction(pred_1) or is_non_trivial_prediction(pred_2)\n",
    "                    \n",
    "                    # Score based on correctness if ground truth is available\n",
    "                    score = 0\n",
    "                    if has_ground_truth and has_non_zero_pred:\n",
    "                        ground_truth = np.array(arc_test_set.replies[task_id][test_idx])\n",
    "                        \n",
    "                        match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                        match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                        score = match_1 + match_2\n",
    "                        \n",
    "                    all_predictions.append((task_id, test_idx, score, has_ground_truth, has_non_zero_pred))\n",
    "        \n",
    "        # Sort by whether they have ground truth first, then by score\n",
    "        all_predictions.sort(key=lambda x: (-int(x[3]), -x[2]))\n",
    "        \n",
    "        # Print summary before visualization\n",
    "        print(f\"\\nFound {len(all_predictions)} total predictions to visualize\")\n",
    "        \n",
    "        # Visualize all tasks\n",
    "        for task_id, test_idx, score, has_ground_truth, has_non_zero_pred in all_predictions:\n",
    "            # Get task data and predictions\n",
    "            task_data = arc_test_set.queries[task_id]\n",
    "            submission_output = submission_data[task_id][test_idx]\n",
    "            \n",
    "            # Visualize this task\n",
    "            score_info = f\" (Score: {score}/2)\" if has_ground_truth and has_non_zero_pred else \" (no ground truth)\" if not has_ground_truth else \" (all zeros - no score)\"\n",
    "            print(f\"\\nTask: {task_id} - Test #{test_idx+1}{score_info}\")\n",
    "            \n",
    "            # Only increment visualized_count if actually visualized\n",
    "            if visualize_submission_result(task_id, task_data, submission_output, test_idx):\n",
    "                visualized_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        \n",
    "        print(f\"\\nVisualized {visualized_count} inference results (skipped {skipped_count} with all-zero predictions)\")\n",
    "        \n",
    "        # Calculate overall accuracy statistics\n",
    "        if arc_test_set.is_fake:\n",
    "            total_tests = 0\n",
    "            total_scored_tests = 0\n",
    "            correct_attempt1 = 0\n",
    "            correct_attempt2 = 0\n",
    "            correct_any = 0\n",
    "            zero_predictions = 0\n",
    "            \n",
    "            for task_id, test_predictions in submission_data.items():\n",
    "                if task_id in arc_test_set.replies:\n",
    "                    for test_idx, test_prediction in enumerate(test_predictions):\n",
    "                        if test_idx < len(arc_test_set.replies[task_id]):\n",
    "                            total_tests += 1\n",
    "                            \n",
    "                            ground_truth = np.array(arc_test_set.replies[task_id][test_idx])\n",
    "                            pred_1 = np.array(test_prediction['attempt_1'])\n",
    "                            pred_2 = np.array(test_prediction['attempt_2'])\n",
    "                            \n",
    "                            # Check if both predictions are all zeros\n",
    "                            if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n",
    "                                zero_predictions += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Only count tests with at least one non-zero prediction\n",
    "                            total_scored_tests += 1\n",
    "                            \n",
    "                            match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                            match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                            \n",
    "                            if match_1: correct_attempt1 += 1\n",
    "                            if match_2: correct_attempt2 += 1\n",
    "                            if match_1 or match_2: correct_any += 1\n",
    "            \n",
    "            if total_tests > 0:\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"OVERALL ACCURACY STATISTICS\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"Total test examples: {total_tests}\")\n",
    "                print(f\"Test examples with zero predictions (excluded from accuracy): {zero_predictions}\")\n",
    "                print(f\"Test examples included in accuracy calculation: {total_scored_tests}\")\n",
    "                \n",
    "                if total_scored_tests > 0:\n",
    "                    print(f\"Correct on attempt 1: {correct_attempt1}/{total_scored_tests} ({correct_attempt1/total_scored_tests:.2%})\")\n",
    "                    print(f\"Correct on attempt 2: {correct_attempt2}/{total_scored_tests} ({correct_attempt2/total_scored_tests:.2%})\")\n",
    "                    print(f\"Correct on either attempt: {correct_any}/{total_scored_tests} ({correct_any/total_scored_tests:.2%})\")\n",
    "                else:\n",
    "                    print(\"No non-zero predictions to calculate accuracy\")\n",
    "                    \n",
    "                print(f\"Overall completion rate: {total_scored_tests/total_tests:.2%} of tests have non-zero predictions\")\n",
    "                print(\"=\"*80)\n",
    "else:\n",
    "    print(\"Skipping inference visualization - not in fake test mode\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 5793177,
     "sourceId": 9515958,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 488614,
     "modelInstanceId": 472738,
     "sourceId": 627781,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24661.932143,
   "end_time": "2025-11-13T11:57:14.564033",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T05:06:12.631890",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
