{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winning Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out how to import or what imports to use was extremely difficult. Mainly, because we needed a Data Collator that does a very niche thing that is not built in. So we needed to use TRL's DataCollatorForCompletionOnlyLM. I will try to explain this in more detail later. But for now I can share the import code that worked well. \n",
    "\n",
    "```\n",
    "%%capture\n",
    "import os\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
    "!pip install --upgrade -qqq uv\n",
    "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "except: get_numpy = \"numpy\"\n",
    "try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "except: is_t4 = False\n",
    "get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
    "!uv pip install -qqq --upgrade     unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
    "!uv pip install -qqq {get_triton}\n",
    "!uv pip install \"huggingface_hub>=0.34.0\" \"datasets>=3.4.1,<4.0.\n",
    "!uv pip install transformers==4.55.4\n",
    "!uv pip install \"trl==0.9.6\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might be wondering what all of this does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data creation is an interesting case. Because we need to do several things. We first need to process each task into a form that we intuitively think LLMs will have an easier time understanding. \n",
    "\n",
    "The original grind world problems are represented by JSON. So we have a bunch of unique task keys.\n",
    "\n",
    "So for each task the task data looks like this:\n",
    "```\n",
    "{test: {input: gridArray} ,\n",
    " train: [\n",
    "    {input: gridArray,\n",
    "     output: gridArray},\n",
    "    {input: gridArray,\n",
    "     output: gridArray}, ...\n",
    " ]}\n",
    "```\n",
    "and then the solution data looks like this:\n",
    "```\n",
    "[gridArray]\n",
    "```\n",
    "Note: sometimes there are multiple test inputs and solution outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to transform this into something that the ML model will have a less messy, easier time reading and have the proper format for our data loaders in training to be able to us. \n",
    "\n",
    "Note: we know what we want it to look like so now we just have to figure out what our data loader would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a very difficult part of the pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay what am I trying to do.\n",
    "I want to get the fine-tuning pipeline build using multiple gpus.\n",
    "I want to use somewhat basic configs so they are simple for me to understand.\n",
    "I want to have a clear file system that is easy to navigate\n",
    "The end goal is to have a fine-tuned model sent to my huggingface folder\n",
    "I think I will have to merge the weights together before sending them to huggingface\n",
    "I need the data to be created and then sent through the pipeline. I want to do DDP.\n",
    "I should figure out if they create the data and then send put it in the data collator or what\n",
    "So I need to set up data infra training infra and merging and sending infra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/shared/arc/\n",
    "├── data/                       # read-only datasets (everyone can read)\n",
    "│   └── ARC-Data/\n",
    "│       └── input/\n",
    "│           ├── arc-prize-2024/\n",
    "│           │   ├── arc-agi_evaluation_challenges.json\n",
    "│           │   └── arc-agi_evaluation_solutions.json\n",
    "│           ├── re_arc/\n",
    "│           │   ├── metadata.json\n",
    "│           │   └── tasks/\n",
    "│           │       ├── <task-id-1>.json\n",
    "│           │       └── ...\n",
    "│           └── arc-dataset-collection/\n",
    "│               └── dataset/\n",
    "│                   └── ConceptARC/\n",
    "│                       └── data/\n",
    "│                           ├── group-1/*.json\n",
    "│                           └── ...\n",
    "├── outputs/                    # shared, writable (checkpoints, logs, merged models)\n",
    "│   ├── runs/                   # per-run folders\n",
    "│   ├── checkpoints/\n",
    "│   ├── logs/\n",
    "│   └── models/\n",
    "│       └── Llama-3/            # where your script saves LoRA + merged\n",
    "└── cache/\n",
    "    ├── hf/                     # shared HF cache (optional)\n",
    "    └── ds/                     # optional DeepSpeed NVMe/AIO cache (if used)\n",
    "\n",
    "main/\n",
    "├── code/\n",
    "│   └── arc-trainer/            # your git repo (your Python package + scripts)\n",
    "│       ├── train_v1.py\n",
    "│       ├── ds_configs/\n",
    "│       │   ├── ds_ze[email protected]          # deepspeed jsons\n",
    "│       │   └── ds_zero2.json\n",
    "│       ├── configs/\n",
    "│       │   └── training.yaml   # hyperparams you may want in YAML\n",
    "│       ├── scripts/\n",
    "│       │   ├── run_slurm.sh    # SLURM launcher\n",
    "│       │   └── run_local.sh    # single-node launcher\n",
    "│       └── README.md\n",
    "└── .cache/\n",
    "    └── huggingface/            # per-user HF cache (if not using shared)\n",
    "\n",
    "/scratch/$USER/                 # fast temp (per-job)\n",
    "└── arc-run-<jobid>/            # created by run script at runtime\n",
    "    ├── offload/                # DS CPU/NVMe offload (if enabled)\n",
    "    └── tmp/                    # temp files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SLURM / cluster issues\n",
    "\n",
    "* **Invalid partition (`general`)**\n",
    "\n",
    "  * **Symptom:** `sbatch: error: invalid partition specified: general`\n",
    "  * **Fix:** Use the cluster’s real GPU queue: `#SBATCH -p GPU-shared` and include `#SBATCH -A cis250063p`.\n",
    "  * **Why:** You requested a queue that doesn’t exist on this system; SLURM rejects the job.\n",
    "\n",
    "# Conda / environment\n",
    "\n",
    "* **Conda activation path wrong**\n",
    "\n",
    "  * **Symptom:** `No such file: .../miniconda3/etc/profile.d/conda.sh`, `CondaError: Run 'conda init' before 'conda activate'`\n",
    "  * **Fix:** Use the right init: `eval \"$($HOME/miniconda3/bin/conda shell.bash hook)\"` → `conda activate arc-env`.\n",
    "  * **Why:** You sourced a non-existent path; using the shell hook lets `conda activate` work non-interactively.\n",
    "\n",
    "* **`torchrun: command not found`**\n",
    "\n",
    "  * **Fix:** Activate the env that has PyTorch (`arc-env`) before launching.\n",
    "  * **Why:** `torchrun` is provided by the PyTorch package in your env.\n",
    "\n",
    "# CUDA / DeepSpeed\n",
    "\n",
    "* **CUDA module not found**\n",
    "\n",
    "  * **Symptom:** `The following module(s) are unknown: \"cuda/12.1\"`\n",
    "  * **Fix:** Load an available one (e.g., `module load cuda/12.4.0`).\n",
    "  * **Why:** Only specific CUDA versions are installed as modules.\n",
    "\n",
    "* **DeepSpeed tries to compile ops (CUDA\\_HOME missing)**\n",
    "\n",
    "  * **Symptoms:**\n",
    "\n",
    "    * `MissingCUDAException: CUDA_HOME does not exist, unable to compile CUDA op(s)`\n",
    "    * `.../cpu_adam.so: cannot open shared object file`\n",
    "  * **Fix (any of these):**\n",
    "\n",
    "    * **Simplest:** uninstall DeepSpeed: `pip uninstall deepspeed`.\n",
    "    * Or keep DS inert: `export DS_BUILD_OPS=0 DS_SKIP_CUDA_CHECK=1 DS_BUILD_AIO=0`.\n",
    "    * Or fully support DS: `module load cuda/12.4.0`, set `CUDA_HOME`, and (if needed) `TORCH_EXTENSIONS_DIR` to a writable per-job scratch; avoid CPU offload or use Torch’s AdamW.\n",
    "  * **Why:** DS imports trigger a JIT build of CUDA/C++ extensions; without a matching CUDA toolkit (and build env), import fails. Disabling or uninstalling DS prevents the JIT path.\n",
    "\n",
    "# Multi-GPU / DDP specifics\n",
    "\n",
    "* **Invalid device ordinal**\n",
    "\n",
    "  * **Symptom:** `RuntimeError: CUDA error: invalid device ordinal`\n",
    "  * **Fix:** Match ranks to GPUs (`torchrun --nproc_per_node=2` when you actually have 2), and don’t constrain `CUDA_VISIBLE_DEVICES` incorrectly.\n",
    "  * **Why:** A rank tried to set `cuda:N` where `N` didn’t exist/was masked.\n",
    "\n",
    "* **4/8-bit model can’t move devices**\n",
    "\n",
    "  * **Symptom:** `You can't train a model loaded in 8-bit or 4-bit precision on a different device...`\n",
    "  * **Fix:** Load *per-rank* on its GPU and never move it:\n",
    "\n",
    "    ```python\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    model = AutoModelForCausalLM.from_pretrained(..., quantization_config=..., device_map={\"\": local_rank})\n",
    "    ```\n",
    "  * **Why:** bitsandbytes quantized weights aren’t transferrable between devices after load; each rank must load onto its own GPU.\n",
    "\n",
    "* **DDP + gradient checkpointing + LoRA crash**\n",
    "\n",
    "  * **Symptom:** `RuntimeError: Expected to mark a variable ready only once... lora_B...weight marked ready twice`\n",
    "  * **Fixes:**\n",
    "\n",
    "    * Use **non-reentrant** checkpointing:\n",
    "      `TrainingArguments(..., gradient_checkpointing=True, gradient_checkpointing_kwargs={\"use_reentrant\": False})`\n",
    "      or `model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})`\n",
    "    * Enable GC **once** (don’t also pass `use_gradient_checkpointing=True` to `prepare_model_for_kbit_training` if you call `.gradient_checkpointing_enable()` later).\n",
    "    * `ddp_find_unused_parameters=False`.\n",
    "    * Ensure LoRA wrap (`get_peft_model`) happens **once**.\n",
    "  * **Why:** Reentrant checkpointing can re-enter the same module during backward; DDP then sees the same param twice in one step.\n",
    "\n",
    "# Transformers / TRL warnings (harmless, but you cleaned them up)\n",
    "\n",
    "* **`use_cache=True` incompatible with gradient checkpointing**\n",
    "\n",
    "  * **Fix:** `model.config.use_cache = False` (or let the auto-toggle happen).\n",
    "  * **Why:** Checkpointing re-computes forward; cached KV states conflict.\n",
    "\n",
    "* **Pad/EOS token warnings**\n",
    "\n",
    "  * **Fix:**\n",
    "\n",
    "    ```python\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    ```\n",
    "  * **Why:** Ensures consistent special tokens for collation/loss masking.\n",
    "\n",
    "# Paths / outputs / caches\n",
    "\n",
    "* **Mixed paths (relative vs absolute; wrong base path)**\n",
    "\n",
    "  * **Fix:** Use absolute paths under your project (e.g., `/ocean/.../shared/arc/...`) for outputs, logs, and caches; create directories up front.\n",
    "  * **Why:** SLURM jobs start in arbitrary working dirs; absolute paths avoid surprises and ensure visibility across nodes.\n",
    "\n",
    "# Monitoring / tmux\n",
    "\n",
    "\n",
    "* **Pane “frozen” / just a viewer**\n",
    "\n",
    "  * **Symptoms:** Keys don’t appear; or stuck in copy/`watch`.\n",
    "  * **Fix:** `Ctrl-C` (stop `watch`), `q`/`Esc` (exit copy/less), or open a new tmux window (`Ctrl-b c`). If flow control, `Ctrl-Q`.\n",
    "  * **Why:** You were in a program/copy mode that captures input, not a regular shell.\n",
    "\n",
    "# Performance / utilization sanity\n",
    "\n",
    "* **Zero GPU utilization**\n",
    "\n",
    "  * **Symptom:** `nvidia-smi` shows \\~0% util, \\~1 MiB used.\n",
    "  * **Fix:** The job has crashed or ended. Relaunch and monitor logs.\n",
    "  * **Why:** No process bound to the GPUs.\n",
    "\n",
    "* **Healthy training**\n",
    "\n",
    "  * **Symptom:** `nvidia-smi` shows >80% util, tens of GB memory used on both GPUs.\n",
    "  * **Why:** Both ranks are running forward/backward as expected.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 0** (format/IO warm-up): very small set of clean, short synthetic RE-ARC samples (small grids, e.g., sizes=[3]), heavy augmentation on rotations/transpositions to force the IO format to “click”. Keep sequences short and consistent with your fmt_opts and collator templates.\n",
    "\n",
    "**Stage 1** (easy→medium structure): larger slice of RE-ARC with a mix of small/medium sizes; start to mix in ConceptARC but cap its ratio so the model still gets lots of easy pattern wins. Keep augmentation but reduce aggressiveness a bit.\n",
    "\n",
    "**Stage 2** (harder distributional match): increase ConceptARC share and introduce harder RE-ARC generators or bigger sizes=[3,4,5,6] (whatever “hard” is in your pipeline). Consider a small rehearsal buffer (5–20%) of Stage-1 data to avoid forgetting.\n",
    "\n",
    "**Stage 3** (final polish): use a dev-like mix (closest to your intended eval distribution) with minimal augmentation (preserve signal). Tight LR, short epochs, early-stop on dev solve-rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python - <<'PY'\n",
    "import os, textwrap\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, whoami\n",
    "\n",
    "token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\", None)\n",
    "api = HfApi()\n",
    "user = whoami(token=token)[\"name\"]\n",
    "\n",
    "base_model = \"'nvidia/Mistral-NeMo-Minitron-8B-Base'\"\n",
    "LORA_DIR   = os.environ[\"LORA_DIR\"]\n",
    "MERGED_DIR = os.environ[\"MERGED_DIR\"]\n",
    "\n",
    "lora_repo_id   = f\"{user}/arc-lora-mistral-8b-nemo-mix\"\n",
    "merged_repo_id = f\"{user}/arc-merged-mistral-8b-nemo-mix\"\n",
    "\n",
    "# Create private repos (flip to public later in the UI if you want)\n",
    "create_repo(lora_repo_id,   repo_type=\"model\", private=True, exist_ok=True, token=token)\n",
    "create_repo(merged_repo_id, repo_type=\"model\", private=True, exist_ok=True, token=token)\n",
    "\n",
    "# Minimal model cards\n",
    "lora_readme = f\"\"\"\\\n",
    "# ARC LoRA Adapter for Mistral 8B\n",
    "base_model: {base_model}\n",
    "library_name: peft\n",
    "pipeline_tag: text-generation\n",
    "tags:\n",
    "- lora\n",
    "- qlora\n",
    "- causal-lm\n",
    "- arc\n",
    "\n",
    "This repo contains the **LoRA adapter only**.\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "tok = AutoTokenizer.from_pretrained(\"{base_model}\")\n",
    "base = AutoModelForCausalLM.from_pretrained(\"{base_model}\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(base, \"{lora_repo_id}\")\n",
    "\"\"\"\n",
    "\n",
    "merged_readme = f\"\"\"\\\n",
    "\n",
    "ARC Fine-tuned Llama 3.2 3B (Merged)\n",
    "base_model: {base_model}\n",
    "pipeline_tag: text-generation\n",
    "tags:\n",
    "\n",
    "merged-weights\n",
    "\n",
    "causal-lm\n",
    "\n",
    "arc\n",
    "\n",
    "This repo contains the merged weights (base + LoRA fused).\n",
    "\n",
    "Usage\n",
    "python\n",
    "Copy code\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tok = AutoTokenizer.from_pretrained(\"{merged_repo_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{merged_repo_id}\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\"\"\"\n",
    "\n",
    "#Write READMEs locally so they get uploaded\n",
    "with open(os.path.join(LORA_DIR, \"README.md\"), \"w\") as f: f.write(textwrap.dedent(lora_readme))\n",
    "with open(os.path.join(MERGED_DIR, \"README.md\"), \"w\") as f: f.write(textwrap.dedent(merged_readme))\n",
    "\n",
    "\n",
    "print(\"Uploading LoRA adapter...\")\n",
    "upload_folder(\n",
    "repo_id=lora_repo_id,\n",
    "folder_path=LORA_DIR,\n",
    "repo_type=\"model\",\n",
    "token=token,\n",
    "commit_message=\"Upload LoRA adapter\",\n",
    "ignore_patterns=[\"/runs/\",\"/logs/\",\"/*.pt\",\"/tmp_*\"],\n",
    ")\n",
    "\n",
    "print(\"Uploading merged model...\")\n",
    "upload_folder(\n",
    "repo_id=merged_repo_id,\n",
    "folder_path=MERGED_DIR,\n",
    "repo_type=\"model\",\n",
    "token=token,\n",
    "commit_message=\"Upload merged model\",\n",
    "ignore_patterns=[\"/runs/\",\"/logs/\",\"**/tmp_*\"],\n",
    ")\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"LoRA repo:\", lora_repo_id)\n",
    "print(\"Merged repo:\", merged_repo_id)\n",
    "PY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
